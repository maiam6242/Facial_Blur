{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial_Blur.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nT2e7XpvVrcl",
        "MnqGEb9KWZz8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiam6242/Facial_Blur/blob/master/Facial_Blur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t20OdGNZI7l",
        "colab_type": "text"
      },
      "source": [
        "# Drawing Recognition with Convolutional Neural Networks\n",
        "\n",
        "Colin Snow and Maia Materman\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-NASiOiqVVc",
        "colab_type": "text"
      },
      "source": [
        "#### Our Application\n",
        "\n",
        "For this project, we hoped to create a machine learning model that was able to recognize drawings from a user. When we were framing this project, we thought that it would be interesting to look at it as a software we could give to kids to allow them to associate words with corresponding images. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4wRsmOBqdVK",
        "colab_type": "text"
      },
      "source": [
        "#### Our Model\n",
        "\n",
        "Our model is outlined below. Essentially, we took a dataset from google Quick Draw and trained a Convolutional Neural Network on it. We also incorporated data from actual users we asked to engage in the same exercise the dataset did. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT2e7XpvVrcl",
        "colab_type": "text"
      },
      "source": [
        "##### Data\n",
        "We began by importing required python modules and our datasets from google and then converting them to numpy arrays. We imported four datasets, each containing one type of image. We chose to use datasets of tower, bear, airplane, broccoli, dog, and broom images. This was mostly based on which ones we thought looked good or funny, but this selection also gives a good representation of living and static items and geometric and organic shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4x1Y1xtZMAi",
        "colab_type": "code",
        "outputId": "e37d25b1-1cc6-45a6-8ccb-38ac7a1a1f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "!pip install torchviz\n",
        "# !CUDA_LAUNCH_BLOCKING=1\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy\n",
        "import time\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy', 'eiffeltower.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy', 'bear.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy', 'airplane.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy', 'broccoli.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy', 'dog.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy', 'broom.npy', False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy\n",
            "To: /content/eiffeltower.npy\n",
            "100%|██████████| 106M/106M [00:00<00:00, 191MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy\n",
            "To: /content/bear.npy\n",
            "100%|██████████| 106M/106M [00:00<00:00, 187MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
            "To: /content/airplane.npy\n",
            "100%|██████████| 119M/119M [00:00<00:00, 186MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy\n",
            "To: /content/broccoli.npy\n",
            "100%|██████████| 104M/104M [00:00<00:00, 196MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy\n",
            "To: /content/dog.npy\n",
            "100%|██████████| 119M/119M [00:00<00:00, 196MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy\n",
            "To: /content/broom.npy\n",
            "100%|██████████| 91.7M/91.7M [00:00<00:00, 193MB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'broom.npy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPERGQfVgc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tower = np.load('eiffeltower.npy') #type = 1\n",
        "bear = np.load('bear.npy') # type = 0\n",
        "airplane = np.load('airplane.npy')\n",
        "broccoli = np.load('broccoli.npy')\n",
        "dog = np.load('dog.npy')\n",
        "broom = np.load('broom.npy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7USekPFvWGHP",
        "colab_type": "text"
      },
      "source": [
        "##### Visualization\n",
        "\n",
        "Because our data was an array of numpy arrays, we displayed an image as an index of a list using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICO8bUo6V2He",
        "colab_type": "code",
        "outputId": "75ee590b-5a89-4237-8245-a91aa8774bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X = airplane[750]\n",
        "X = np.resize(X,(28,28))\n",
        "X = np.invert(X)\n",
        "plt.imshow(X, cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnoRPCJ8wWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnqGEb9KWZz8",
        "colab_type": "text"
      },
      "source": [
        "##### Our Dataset\n",
        "\n",
        "In addition to the google dataset, we decided to create our own data to see how changing the method of collection affected the result. We collected a full set of 6 images from 14 different people around Olin for a total of 84 new test images. We recorded the data in photoshop by giving each participant 20 seconds to draw the required image on a 28 x 28 pixel grid. We saved each of these images named by type and then imported them below to create a numpy array for each of these datasets as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgR_pbWwS4R",
        "colab_type": "code",
        "outputId": "2783a518-0682-4360-c861-e148bb3a4887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
        "\n",
        "\n",
        "airplane_list = []\n",
        "for filename in glob.glob('airplane*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    airplane_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "broom_list = []\n",
        "for filename in glob.glob('broom*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    broom_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "bear_list = []\n",
        "for filename in glob.glob('bear*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    bear_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "broccoli_list = []\n",
        "for filename in glob.glob('broccoli*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    broccoli_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "tower_list = []\n",
        "for filename in glob.glob('tower*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    tower_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "dog_list = []\n",
        "for filename in glob.glob('dog*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    dog_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "dog_list = np.array(dog_list)\n",
        "broccoli_list = np.array(broccoli_list)\n",
        "bear_list = np.array(bear_list)\n",
        "broom_list = np.array(broom_list)\n",
        "airplane_list = np.array(airplane_list)\n",
        "print(len(tower_list))\n",
        "tower_list = np.array(tower_list)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzTf6Uf4ZRcd",
        "colab_type": "text"
      },
      "source": [
        "##### Tensor Creation\n",
        "\n",
        "In order to use our data, we first needed to load it into pytorch and assign keys to each image. This is so that the model knows the correct identity of each picture. We did this by creating a QuickDrawData class that took an arbitry number of numpy arrays and stacked them all together while giving each a unique key identifier. We used this same class to load both the google data and our own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yhVEn8Ql_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import torch\n",
        "\n",
        "class QuickDrawData(Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super(QuickDrawData, self).__init__()\n",
        "        count = 0\n",
        "        # self.data = np.empty(args[0].shape, dtype=int)\n",
        "        # self.targets = np.empty(args[0].shape, dtype=int)\n",
        "        self.classes = []\n",
        "        for arg in args:\n",
        "          # print(str(arg))\n",
        "          if type(arg) == str:\n",
        "            self.classes += arg\n",
        "          else:\n",
        "            if count == 0:\n",
        "              self.data = np.array(arg)\n",
        "              self.targets = np.array(0*np.ones(arg.shape[0], dtype = int))\n",
        "            else:\n",
        "              self.data = np.vstack((self.data, arg))\n",
        "              self.targets = np.hstack((self.targets, int(count)*np.ones(arg.shape[0], dtype = int)))\n",
        "            count+=1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.targets.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # print(self.data[index, :])\n",
        "        # print(type(self.data[index, :]))\n",
        "        # print(np.size(self.data[index, :]))\n",
        "        return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "\n",
        "quick_draw_data = QuickDrawData(tower, bear, airplane, broccoli, dog, broom, 'tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "testdata = QuickDrawData(np.array(tower_list), np.array(bear_list), np.array(airplane_list), np.array(broccoli_list), np.array(dog_list), np.array(broom_list), 'tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJbA5bHYcN-i",
        "colab_type": "text"
      },
      "source": [
        "##### Train/Test split\n",
        "Because our own data is very limited, we used a portion of the QuickDraw data as our testing set. We split the dataset into 90% training and 10% test, assigning each image randomly to the sets to avoid any ordering bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbaebNQ7RwG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = int(len(quick_draw_data)*.9) \n",
        "train, test = random_split(quick_draw_data, [x,(len(quick_draw_data) - x)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89ThqJ5zd35r",
        "colab_type": "text"
      },
      "source": [
        "##### Assigning samplers\n",
        "\n",
        "In order to run batches through the model, pytorch needed a sampler class which takes images from the sets and runs them through the model. We created a sampler class for the training and testing sets as these needed to be used by the model while it was being trained. We also created another test set, test_set_2, which represented the data we collected ourselves. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMrsY0PERwJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data set information\n",
        "image_dims = 1, 28, 28\n",
        "n_training_samples = len(train) # How many training images to use\n",
        "n_test_samples = len(test) # How many test images to use\n",
        "classes = ('tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "# Load the training set\n",
        "train_set = train\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "\n",
        "#Load the test set\n",
        "test_set = test\n",
        "test_set2 = testdata\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
        "test_sampler2 = SubsetRandomSampler(np.arange(84, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsALCxqnibid",
        "colab_type": "text"
      },
      "source": [
        "##### Defining CNN\n",
        "We then needed to define our convolutional neural network. We needed to specify layers of our model, both in the type of layer and the size. We utilized two 2D convolutions, two maxpool layers, three fully connected layers, a batch norm per convolution and a ReLu activation function for every layer except the last, which used a soft max. Those generally took a number of kernels and kernel size or a layer size as arguments. \n",
        "\n",
        "Once we defined all of the layers, we ordered them in a forward function. That fucntion is actually what determined the structure of our model, and was also visualized in the code below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QoJj27WRwO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyCNN, self).__init__()\n",
        "    \n",
        "    # Defining the number of kernels and the size of the fully convolutional layers\n",
        "    num_kernels = 16\n",
        "    fcl_size = 256\n",
        "    fcl_size2 = 128\n",
        "    fcl_size3 = 64\n",
        "    \n",
        "    # Defining the layers of our model\n",
        "    self.conv1 = nn.Conv2d(1, num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.conv2 = nn.Conv2d(num_kernels, num_kernels*2, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.maxpool_output_size1 = int(num_kernels*(image_dims[1]/2) * (image_dims[2]/2))\n",
        "    self.maxpool_output_size2 = int(num_kernels*(image_dims[1]/4) * (image_dims[2]/4)*2)\n",
        "    self.batchnorm1 = nn.BatchNorm2d(num_kernels)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(num_kernels*2)\n",
        "    self.batchnorm3 = nn.BatchNorm1d(fcl_size)\n",
        "    self.batchnorm4 = nn.BatchNorm1d(fcl_size2)\n",
        "    self.batchnorm5 = nn.BatchNorm1d(fcl_size3)\n",
        "    self.fc1 = nn.Linear(1568, fcl_size)\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "    self.fc2 = nn.Linear(fcl_size, fcl_size2)\n",
        "    self.fc4 = nn.Linear(fcl_size2, fcl_size3)\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "    fc3_size = len(classes)\n",
        "    self.fc3 = nn.Linear(fcl_size3, fc3_size)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = x.view(-1, self.maxpool_output_size2)\n",
        "    x = self.fc1(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = self.fc4(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = self.fc3(x)\n",
        "    x = torch.nn.functional.log_softmax(x)\n",
        "    return x\n",
        "\n",
        "  def get_loss(self, learning_rate):\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    return loss, optimizer\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHaayoLKRwaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = MyCNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SbsGk9qjpmY",
        "colab_type": "code",
        "outputId": "a7974295-7c13-44bf-aa36-b0f0718afb53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1210
        }
      },
      "source": [
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    fake_input = Variable(torch.zeros((1, image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "# This plots a nice visualization of our network as seen below\n",
        "visualize_network(net)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f7927b8ad68>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"463pt\" height=\"864pt\"\n viewBox=\"0.00 0.00 463.04 864.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.7135 .7135) rotate(0) translate(4 1207)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1207 645,-1207 645,4 -4,4\"/>\n<!-- 140158334669320 -->\n<g id=\"node1\" class=\"node\">\n<title>140158334669320</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"539,-21 414,-21 414,0 539,0 539,-21\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogSoftmaxBackward</text>\n</g>\n<!-- 140158334670832 -->\n<g id=\"node2\" class=\"node\">\n<title>140158334670832</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"528.5,-78 424.5,-78 424.5,-57 528.5,-57 528.5,-78\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140158334670832&#45;&gt;140158334669320 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140158334670832&#45;&gt;140158334669320</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M476.5,-56.7787C476.5,-49.6134 476.5,-39.9517 476.5,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"480.0001,-31.1732 476.5,-21.1732 473.0001,-31.1732 480.0001,-31.1732\"/>\n</g>\n<!-- 140158334213928 -->\n<g id=\"node3\" class=\"node\">\n<title>140158334213928</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"411.5,-148 357.5,-148 357.5,-114 411.5,-114 411.5,-148\"/>\n<text text-anchor=\"middle\" x=\"384.5\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.bias</text>\n<text text-anchor=\"middle\" x=\"384.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6)</text>\n</g>\n<!-- 140158334213928&#45;&gt;140158334670832 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140158334213928&#45;&gt;140158334670832</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M409.1543,-113.9832C422.6894,-104.641 439.3926,-93.1122 452.778,-83.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.7865,-86.7398 461.0283,-78.1788 450.8102,-80.9788 454.7865,-86.7398\"/>\n</g>\n<!-- 140158334211576 -->\n<g id=\"node4\" class=\"node\">\n<title>140158334211576</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"523.5,-141.5 429.5,-141.5 429.5,-120.5 523.5,-120.5 523.5,-141.5\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140158334211576&#45;&gt;140158334670832 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140158334211576&#45;&gt;140158334670832</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M476.5,-120.2281C476.5,-111.5091 476.5,-98.9699 476.5,-88.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"480.0001,-88.1128 476.5,-78.1128 473.0001,-88.1129 480.0001,-88.1128\"/>\n</g>\n<!-- 140158334213816 -->\n<g id=\"node5\" class=\"node\">\n<title>140158334213816</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"527.5,-211.5 423.5,-211.5 423.5,-190.5 527.5,-190.5 527.5,-211.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-197.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140158334213816&#45;&gt;140158334211576 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140158334213816&#45;&gt;140158334211576</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.6519,-190.3685C475.7972,-180.1925 476.0206,-164.5606 476.2016,-151.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.7034,-151.7806 476.3467,-141.7315 472.7041,-151.6805 479.7034,-151.7806\"/>\n</g>\n<!-- 140158334211240 -->\n<g id=\"node6\" class=\"node\">\n<title>140158334211240</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-288 356.5,-288 356.5,-254 410.5,-254 410.5,-288\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc4.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-261.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 140158334211240&#45;&gt;140158334213816 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140158334211240&#45;&gt;140158334213816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M406.2416,-253.6966C420.6068,-242.7666 439.0787,-228.7119 453.3325,-217.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.7491,-220.4258 461.5881,-211.5852 451.5104,-214.855 455.7491,-220.4258\"/>\n</g>\n<!-- 140158334210232 -->\n<g id=\"node7\" class=\"node\">\n<title>140158334210232</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"522.5,-281.5 428.5,-281.5 428.5,-260.5 522.5,-260.5 522.5,-281.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140158334210232&#45;&gt;140158334213816 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140158334210232&#45;&gt;140158334213816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-260.3685C475.5,-250.1925 475.5,-234.5606 475.5,-221.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-221.7315 475.5,-211.7315 472.0001,-221.7316 479.0001,-221.7315\"/>\n</g>\n<!-- 140158334284856 -->\n<g id=\"node8\" class=\"node\">\n<title>140158334284856</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"527.5,-351.5 423.5,-351.5 423.5,-330.5 527.5,-330.5 527.5,-351.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-337.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140158334284856&#45;&gt;140158334210232 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140158334284856&#45;&gt;140158334210232</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-330.3685C475.5,-320.1925 475.5,-304.5606 475.5,-291.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-291.7315 475.5,-281.7315 472.0001,-291.7316 479.0001,-291.7315\"/>\n</g>\n<!-- 140158334287040 -->\n<g id=\"node9\" class=\"node\">\n<title>140158334287040</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-428 356.5,-428 356.5,-394 410.5,-394 410.5,-428\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-414.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 140158334287040&#45;&gt;140158334284856 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140158334287040&#45;&gt;140158334284856</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M406.2416,-393.6966C420.6068,-382.7666 439.0787,-368.7119 453.3325,-357.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.7491,-360.4258 461.5881,-351.5852 451.5104,-354.855 455.7491,-360.4258\"/>\n</g>\n<!-- 140158334287600 -->\n<g id=\"node10\" class=\"node\">\n<title>140158334287600</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"522.5,-421.5 428.5,-421.5 428.5,-400.5 522.5,-400.5 522.5,-421.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140158334287600&#45;&gt;140158334284856 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140158334287600&#45;&gt;140158334284856</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-400.3685C475.5,-390.1925 475.5,-374.5606 475.5,-361.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-361.7315 475.5,-351.7315 472.0001,-361.7316 479.0001,-361.7315\"/>\n</g>\n<!-- 140158334284968 -->\n<g id=\"node11\" class=\"node\">\n<title>140158334284968</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"526.5,-491.5 422.5,-491.5 422.5,-470.5 526.5,-470.5 526.5,-491.5\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-477.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140158334284968&#45;&gt;140158334287600 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140158334284968&#45;&gt;140158334287600</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M474.6519,-470.3685C474.7972,-460.1925 475.0206,-444.5606 475.2016,-431.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"478.7034,-431.7806 475.3467,-421.7315 471.7041,-431.6805 478.7034,-431.7806\"/>\n</g>\n<!-- 140158334287768 -->\n<g id=\"node12\" class=\"node\">\n<title>140158334287768</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-568 356.5,-568 356.5,-534 410.5,-534 410.5,-568\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-554.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-541.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 140158334287768&#45;&gt;140158334284968 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140158334287768&#45;&gt;140158334284968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M405.9944,-533.6966C420.2034,-522.7666 438.4745,-508.7119 452.5735,-497.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.947,-500.4565 460.7393,-491.5852 450.679,-494.9081 454.947,-500.4565\"/>\n</g>\n<!-- 140158334287824 -->\n<g id=\"node13\" class=\"node\">\n<title>140158334287824</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"520,-561.5 429,-561.5 429,-540.5 520,-540.5 520,-561.5\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-547.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 140158334287824&#45;&gt;140158334284968 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140158334287824&#45;&gt;140158334284968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M474.5,-540.3685C474.5,-530.1925 474.5,-514.5606 474.5,-501.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"478.0001,-501.7315 474.5,-491.7315 471.0001,-501.7316 478.0001,-501.7315\"/>\n</g>\n<!-- 140158334810712 -->\n<g id=\"node14\" class=\"node\">\n<title>140158334810712</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"520.5,-631.5 426.5,-631.5 426.5,-610.5 520.5,-610.5 520.5,-631.5\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-617.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140158334810712&#45;&gt;140158334287824 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140158334810712&#45;&gt;140158334287824</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.6519,-610.3685C473.7972,-600.1925 474.0206,-584.5606 474.2016,-571.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.7034,-571.7806 474.3467,-561.7315 470.7041,-571.6805 477.7034,-571.7806\"/>\n</g>\n<!-- 140158334810376 -->\n<g id=\"node15\" class=\"node\">\n<title>140158334810376</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"563.5,-695 383.5,-695 383.5,-674 563.5,-674 563.5,-695\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-681.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140158334810376&#45;&gt;140158334810712 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140158334810376&#45;&gt;140158334810712</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-673.7281C473.5,-665.0091 473.5,-652.4699 473.5,-641.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-641.6128 473.5,-631.6128 470.0001,-641.6129 477.0001,-641.6128\"/>\n</g>\n<!-- 140158334817952 -->\n<g id=\"node16\" class=\"node\">\n<title>140158334817952</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"550,-752 397,-752 397,-731 550,-731 550,-752\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-738.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnBatchNormBackward</text>\n</g>\n<!-- 140158334817952&#45;&gt;140158334810376 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140158334817952&#45;&gt;140158334810376</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-730.7787C473.5,-723.6134 473.5,-713.9517 473.5,-705.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-705.1732 473.5,-695.1732 470.0001,-705.1732 477.0001,-705.1732\"/>\n</g>\n<!-- 140158334816328 -->\n<g id=\"node17\" class=\"node\">\n<title>140158334816328</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"401.5,-815.5 307.5,-815.5 307.5,-794.5 401.5,-794.5 401.5,-815.5\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-801.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140158334816328&#45;&gt;140158334817952 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140158334816328&#45;&gt;140158334817952</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M374.4179,-794.3715C393.553,-784.1608 422.6839,-768.6162 444.4131,-757.0212\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"446.3423,-759.9589 453.5171,-752.1631 443.0468,-753.7831 446.3423,-759.9589\"/>\n</g>\n<!-- 140158334017720 -->\n<g id=\"node18\" class=\"node\">\n<title>140158334017720</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"433,-879 276,-879 276,-858 433,-858 433,-879\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-865.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140158334017720&#45;&gt;140158334816328 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140158334017720&#45;&gt;140158334816328</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-857.7281C354.5,-849.0091 354.5,-836.4699 354.5,-825.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-825.6128 354.5,-815.6128 351.0001,-825.6129 358.0001,-825.6128\"/>\n</g>\n<!-- 140158334017888 -->\n<g id=\"node19\" class=\"node\">\n<title>140158334017888</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"295.5,-942.5 115.5,-942.5 115.5,-921.5 295.5,-921.5 295.5,-942.5\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-928.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140158334017888&#45;&gt;140158334017720 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140158334017888&#45;&gt;140158334017720</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M230.4392,-921.3715C255.0429,-910.8861 292.8449,-894.7758 320.2558,-883.094\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"321.6522,-886.3036 329.4794,-879.1631 318.9077,-879.864 321.6522,-886.3036\"/>\n</g>\n<!-- 140158334017944 -->\n<g id=\"node20\" class=\"node\">\n<title>140158334017944</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"282,-1006 129,-1006 129,-985 282,-985 282,-1006\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-992.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnBatchNormBackward</text>\n</g>\n<!-- 140158334017944&#45;&gt;140158334017888 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140158334017944&#45;&gt;140158334017888</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M205.5,-984.7281C205.5,-976.0091 205.5,-963.4699 205.5,-952.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0001,-952.6128 205.5,-942.6128 202.0001,-952.6129 209.0001,-952.6128\"/>\n</g>\n<!-- 140158334018112 -->\n<g id=\"node21\" class=\"node\">\n<title>140158334018112</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"133.5,-1069.5 39.5,-1069.5 39.5,-1048.5 133.5,-1048.5 133.5,-1069.5\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-1055.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140158334018112&#45;&gt;140158334017944 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140158334018112&#45;&gt;140158334017944</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.4179,-1048.3715C125.553,-1038.1608 154.6839,-1022.6162 176.4131,-1011.0212\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"178.3423,-1013.9589 185.5171,-1006.1631 175.0468,-1007.7831 178.3423,-1013.9589\"/>\n</g>\n<!-- 140158334018280 -->\n<g id=\"node22\" class=\"node\">\n<title>140158334018280</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"165,-1133 8,-1133 8,-1112 165,-1112 165,-1133\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-1119.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140158334018280&#45;&gt;140158334018112 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140158334018280&#45;&gt;140158334018112</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M86.5,-1111.7281C86.5,-1103.0091 86.5,-1090.4699 86.5,-1079.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"90.0001,-1079.6128 86.5,-1069.6128 83.0001,-1079.6129 90.0001,-1079.6128\"/>\n</g>\n<!-- 140158334018560 -->\n<g id=\"node23\" class=\"node\">\n<title>140158334018560</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"81,-1203 0,-1203 0,-1169 81,-1169 81,-1203\"/>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-1189.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-1176.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16, 1, 3, 3)</text>\n</g>\n<!-- 140158334018560&#45;&gt;140158334018280 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140158334018560&#45;&gt;140158334018280</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M52.8272,-1168.9832C58.9107,-1160.5853 66.2742,-1150.4204 72.5621,-1141.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5945,-1143.5204 78.6266,-1133.3687 69.9256,-1139.4138 75.5945,-1143.5204\"/>\n</g>\n<!-- 140158334018504 -->\n<g id=\"node24\" class=\"node\">\n<title>140158334018504</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"167.5,-1203 99.5,-1203 99.5,-1169 167.5,-1169 167.5,-1203\"/>\n<text text-anchor=\"middle\" x=\"133.5\" y=\"-1189.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"133.5\" y=\"-1176.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140158334018504&#45;&gt;140158334018280 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140158334018504&#45;&gt;140158334018280</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M120.9049,-1168.9832C114.6237,-1160.4969 107.0069,-1150.2062 100.5384,-1141.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.3071,-1139.3243 94.5445,-1133.3687 97.6806,-1143.4888 103.3071,-1139.3243\"/>\n</g>\n<!-- 140158334018168 -->\n<g id=\"node25\" class=\"node\">\n<title>140158334018168</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"259.5,-1076 151.5,-1076 151.5,-1042 259.5,-1042 259.5,-1076\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-1062.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm1.weight</text>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-1049.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140158334018168&#45;&gt;140158334017944 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140158334018168&#45;&gt;140158334017944</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M205.5,-1041.9832C205.5,-1034.1157 205.5,-1024.6973 205.5,-1016.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0001,-1016.3686 205.5,-1006.3687 202.0001,-1016.3687 209.0001,-1016.3686\"/>\n</g>\n<!-- 140158334018392 -->\n<g id=\"node26\" class=\"node\">\n<title>140158334018392</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"373,-1076 278,-1076 278,-1042 373,-1042 373,-1076\"/>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-1062.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm1.bias</text>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-1049.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140158334018392&#45;&gt;140158334017944 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140158334018392&#45;&gt;140158334017944</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M293.3422,-1041.9832C275.1833,-1032.3741 252.6526,-1020.4516 234.9561,-1011.0872\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"236.1563,-1007.7625 225.6804,-1006.1788 232.8822,-1013.9496 236.1563,-1007.7625\"/>\n</g>\n<!-- 140158334017832 -->\n<g id=\"node27\" class=\"node\">\n<title>140158334017832</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"395,-949 314,-949 314,-915 395,-915 395,-949\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-935.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.weight</text>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-922.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32, 16, 3, 3)</text>\n</g>\n<!-- 140158334017832&#45;&gt;140158334017720 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140158334017832&#45;&gt;140158334017720</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-914.9832C354.5,-907.1157 354.5,-897.6973 354.5,-889.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-889.3686 354.5,-879.3687 351.0001,-889.3687 358.0001,-889.3686\"/>\n</g>\n<!-- 140158334018000 -->\n<g id=\"node28\" class=\"node\">\n<title>140158334018000</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"481.5,-949 413.5,-949 413.5,-915 481.5,-915 481.5,-949\"/>\n<text text-anchor=\"middle\" x=\"447.5\" y=\"-935.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.bias</text>\n<text text-anchor=\"middle\" x=\"447.5\" y=\"-922.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140158334018000&#45;&gt;140158334017720 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140158334018000&#45;&gt;140158334017720</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M422.5777,-914.9832C408.8955,-905.641 392.0107,-894.1122 378.4799,-884.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.372,-881.9273 370.1398,-879.1788 376.4248,-887.7082 380.372,-881.9273\"/>\n</g>\n<!-- 140158334816496 -->\n<g id=\"node29\" class=\"node\">\n<title>140158334816496</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"527.5,-822 419.5,-822 419.5,-788 527.5,-788 527.5,-822\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-808.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm2.weight</text>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-795.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140158334816496&#45;&gt;140158334817952 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140158334816496&#45;&gt;140158334817952</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-787.9832C473.5,-780.1157 473.5,-770.6973 473.5,-762.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-762.3686 473.5,-752.3687 470.0001,-762.3687 477.0001,-762.3686\"/>\n</g>\n<!-- 140158334817896 -->\n<g id=\"node30\" class=\"node\">\n<title>140158334817896</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"641,-822 546,-822 546,-788 641,-788 641,-822\"/>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-808.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm2.bias</text>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-795.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140158334817896&#45;&gt;140158334817952 -->\n<g id=\"edge29\" class=\"edge\">\n<title>140158334817896&#45;&gt;140158334817952</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M561.3422,-787.9832C543.1833,-778.3741 520.6526,-766.4516 502.9561,-757.0872\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"504.1563,-753.7625 493.6804,-752.1788 500.8822,-759.9496 504.1563,-753.7625\"/>\n</g>\n<!-- 140158388316200 -->\n<g id=\"node31\" class=\"node\">\n<title>140158388316200</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"612,-561.5 539,-561.5 539,-540.5 612,-540.5 612,-561.5\"/>\n<text text-anchor=\"middle\" x=\"575.5\" y=\"-547.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140158388316200&#45;&gt;140158334284968 -->\n<g id=\"edge30\" class=\"edge\">\n<title>140158388316200&#45;&gt;140158334284968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M560.1603,-540.3685C543.6024,-528.8927 517.033,-510.4783 497.8726,-497.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"499.8659,-494.3219 489.6532,-491.5022 495.8784,-500.0752 499.8659,-494.3219\"/>\n</g>\n<!-- 140158334809704 -->\n<g id=\"node32\" class=\"node\">\n<title>140158334809704</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"614,-638 539,-638 539,-604 614,-604 614,-638\"/>\n<text text-anchor=\"middle\" x=\"576.5\" y=\"-624.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"576.5\" y=\"-611.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256, 1568)</text>\n</g>\n<!-- 140158334809704&#45;&gt;140158388316200 -->\n<g id=\"edge31\" class=\"edge\">\n<title>140158334809704&#45;&gt;140158388316200</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M576.2528,-603.6966C576.1152,-594.0634 575.9429,-582.003 575.7979,-571.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"579.2967,-571.7402 575.6542,-561.7913 572.2975,-571.8403 579.2967,-571.7402\"/>\n</g>\n<!-- 140158334284296 -->\n<g id=\"node33\" class=\"node\">\n<title>140158334284296</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"615,-421.5 542,-421.5 542,-400.5 615,-400.5 615,-421.5\"/>\n<text text-anchor=\"middle\" x=\"578.5\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140158334284296&#45;&gt;140158334284856 -->\n<g id=\"edge32\" class=\"edge\">\n<title>140158334284296&#45;&gt;140158334284856</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.8565,-400.3685C545.9707,-388.8927 518.8752,-370.4783 499.3354,-357.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"501.1913,-354.2284 490.9532,-351.5022 497.2567,-360.0179 501.1913,-354.2284\"/>\n</g>\n<!-- 140158334283904 -->\n<g id=\"node34\" class=\"node\">\n<title>140158334283904</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"614,-498 545,-498 545,-464 614,-464 614,-498\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-484.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128, 256)</text>\n</g>\n<!-- 140158334283904&#45;&gt;140158334284296 -->\n<g id=\"edge33\" class=\"edge\">\n<title>140158334283904&#45;&gt;140158334284296</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.2528,-463.6966C579.1152,-454.0634 578.9429,-442.003 578.7979,-431.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"582.2967,-431.7402 578.6542,-421.7913 575.2975,-431.8403 582.2967,-431.7402\"/>\n</g>\n<!-- 140158334213368 -->\n<g id=\"node35\" class=\"node\">\n<title>140158334213368</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"615,-281.5 542,-281.5 542,-260.5 615,-260.5 615,-281.5\"/>\n<text text-anchor=\"middle\" x=\"578.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140158334213368&#45;&gt;140158334213816 -->\n<g id=\"edge34\" class=\"edge\">\n<title>140158334213368&#45;&gt;140158334213816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.8565,-260.3685C545.9707,-248.8927 518.8752,-230.4783 499.3354,-217.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"501.1913,-214.2284 490.9532,-211.5022 497.2567,-220.0179 501.1913,-214.2284\"/>\n</g>\n<!-- 140158334286760 -->\n<g id=\"node36\" class=\"node\">\n<title>140158334286760</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"613,-358 546,-358 546,-324 613,-324 613,-358\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc4.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-331.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 128)</text>\n</g>\n<!-- 140158334286760&#45;&gt;140158334213368 -->\n<g id=\"edge35\" class=\"edge\">\n<title>140158334286760&#45;&gt;140158334213368</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.2528,-323.6966C579.1152,-314.0634 578.9429,-302.003 578.7979,-291.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"582.2967,-291.7402 578.6542,-281.7913 575.2975,-291.8403 582.2967,-291.7402\"/>\n</g>\n<!-- 140158334214040 -->\n<g id=\"node37\" class=\"node\">\n<title>140158334214040</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"616,-141.5 543,-141.5 543,-120.5 616,-120.5 616,-141.5\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140158334214040&#45;&gt;140158334670832 -->\n<g id=\"edge36\" class=\"edge\">\n<title>140158334214040&#45;&gt;140158334670832</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.0275,-120.2281C545.6519,-110.1325 520.9682,-94.9149 502.3209,-83.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"504.0636,-80.3814 493.7145,-78.1128 500.3901,-86.3401 504.0636,-80.3814\"/>\n</g>\n<!-- 140158334211352 -->\n<g id=\"node38\" class=\"node\">\n<title>140158334211352</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"613,-218 546,-218 546,-184 613,-184 613,-218\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6, 64)</text>\n</g>\n<!-- 140158334211352&#45;&gt;140158334214040 -->\n<g id=\"edge37\" class=\"edge\">\n<title>140158334211352&#45;&gt;140158334214040</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.5,-183.6966C579.5,-174.0634 579.5,-162.003 579.5,-151.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"583.0001,-151.7912 579.5,-141.7913 576.0001,-151.7913 583.0001,-151.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o6bwN8MnPan",
        "colab_type": "text"
      },
      "source": [
        "##### Training Our Model\n",
        "In order to actually use our model, we needed to train it. Training a model turned out to be a time and computationally intensive project. To begin, we needed to define some parameters which determined the way the model runs like the number of epochs and the batch size. Using the batch sizes we defined, we sorted our data into different batches which we utilized to train and test the model. We then created variables to track certain things about the way that our model was performing like the training and test losses and history. After that, we looped through the number of epochs that we specified earlier. In this loop, we ran the training data through our model, calculating the inaccuracies (loss) at each point in the process. Once that happened, we ran our test set through the model that was trained on the previous data. That allowed us to determine an accuracy percentage on the test data at that point. This process was repeated a number of times (as per the number of epochs we defined), and then we summed the total loss in our model and time our code took to execute. \n",
        "\n",
        "After getting those metrics, we were able to plot the loss of our function over the number of training and test images which we had, giving us a way to see how our loss was changing with more data. That visualization is displayed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_9CTlb4Yx7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training parameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2\n",
        "n_epochs = 5\n",
        "\n",
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, sampler=train_sampler, num_workers = 2)\n",
        "test_loader2 = torch.utils.data.DataLoader(testdata, batch_size = 84, sampler=test_sampler2, num_workers = 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, sampler=test_sampler, num_workers = 2)\n",
        "\n",
        "def train_model(net):\n",
        "    \"\"\" Train a the specified network.\n",
        "\n",
        "        Outputs a tuple with the following four elements\n",
        "        train_hist_x: the x-values (batch number) that the training set was \n",
        "            evaluated on.\n",
        "        train_loss_hist: the loss values for the training set corresponding to\n",
        "            the batch numbers returned in train_hist_x\n",
        "        test_hist_x: the x-values (batch number) that the test set was \n",
        "            evaluated on.\n",
        "        test_loss_hist: the loss values for the test set corresponding to\n",
        "            the batch numbers returned in test_hist_x\n",
        "    \"\"\" \n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "    \n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 200\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    test_hist_x = []\n",
        "    test_loss_hist = []\n",
        "    training_start_time = time.time()\n",
        "\n",
        "    # Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # Get inputs in right form\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "            # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Compute the loss and find the loss with respect to each parameter of the model\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            \n",
        "            # Change each parameter with respect to the recently computed loss.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print every 20th batch of an epoch\n",
        "            if (i % print_every) == print_every-1:\n",
        "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "                \n",
        "                # Reset running loss and time\n",
        "                train_loss_hist.append(running_loss / print_every)\n",
        "                train_hist_x.append(idx)\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            idx += 1\n",
        "\n",
        "        # At the end of the epoch, do a pass on the test set\n",
        "        total_test_loss = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            # Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            test_outputs = net(inputs)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.data.item()\n",
        "        # Calculating the total loss over the whole system\n",
        "        test_loss_hist.append(total_test_loss / len(test_loader))\n",
        "        test_hist_x.append(idx)\n",
        "        print(\"Validation loss = {:.2f}\".format(\n",
        "            total_test_loss / len(test_loader)))\n",
        "    # Calculating the time that it took to train our model\n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "    return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGyZdjgNjf_h",
        "colab_type": "code",
        "outputId": "a9fed6ea-c008-4043-b745-322bca4049ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 200\t train_loss: 0.73 took: 2.23s\n",
            "Epoch 1, Iteration 400\t train_loss: 0.50 took: 1.94s\n",
            "Epoch 1, Iteration 600\t train_loss: 0.44 took: 2.02s\n",
            "Epoch 1, Iteration 800\t train_loss: 0.43 took: 1.98s\n",
            "Epoch 1, Iteration 1000\t train_loss: 0.39 took: 1.94s\n",
            "Epoch 1, Iteration 1200\t train_loss: 0.38 took: 2.01s\n",
            "Epoch 1, Iteration 1400\t train_loss: 0.36 took: 1.92s\n",
            "Epoch 1, Iteration 1600\t train_loss: 0.37 took: 1.97s\n",
            "Epoch 1, Iteration 1800\t train_loss: 0.37 took: 1.95s\n",
            "Epoch 1, Iteration 2000\t train_loss: 0.37 took: 1.91s\n",
            "Epoch 1, Iteration 2200\t train_loss: 0.35 took: 1.92s\n",
            "Epoch 1, Iteration 2400\t train_loss: 0.34 took: 2.00s\n",
            "Epoch 1, Iteration 2600\t train_loss: 0.35 took: 1.90s\n",
            "Epoch 1, Iteration 2800\t train_loss: 0.32 took: 1.99s\n",
            "Epoch 1, Iteration 3000\t train_loss: 0.34 took: 2.15s\n",
            "Epoch 1, Iteration 3200\t train_loss: 0.33 took: 2.20s\n",
            "Epoch 1, Iteration 3400\t train_loss: 0.34 took: 2.28s\n",
            "Epoch 1, Iteration 3600\t train_loss: 0.34 took: 2.22s\n",
            "Epoch 1, Iteration 3800\t train_loss: 0.33 took: 2.33s\n",
            "Epoch 1, Iteration 4000\t train_loss: 0.32 took: 2.17s\n",
            "Epoch 1, Iteration 4200\t train_loss: 0.32 took: 2.25s\n",
            "Epoch 1, Iteration 4400\t train_loss: 0.31 took: 2.15s\n",
            "Epoch 1, Iteration 4600\t train_loss: 0.32 took: 2.19s\n",
            "Epoch 1, Iteration 4800\t train_loss: 0.33 took: 2.21s\n",
            "Epoch 1, Iteration 5000\t train_loss: 0.31 took: 2.18s\n",
            "Epoch 1, Iteration 5200\t train_loss: 0.32 took: 2.26s\n",
            "Epoch 1, Iteration 5400\t train_loss: 0.34 took: 2.18s\n",
            "Epoch 1, Iteration 5600\t train_loss: 0.31 took: 2.23s\n",
            "Epoch 1, Iteration 5800\t train_loss: 0.32 took: 2.17s\n",
            "Epoch 1, Iteration 6000\t train_loss: 0.32 took: 2.27s\n",
            "Epoch 1, Iteration 6200\t train_loss: 0.30 took: 2.24s\n",
            "Epoch 1, Iteration 6400\t train_loss: 0.30 took: 2.20s\n",
            "Epoch 1, Iteration 6600\t train_loss: 0.31 took: 2.20s\n",
            "Epoch 1, Iteration 6800\t train_loss: 0.31 took: 2.18s\n",
            "Epoch 1, Iteration 7000\t train_loss: 0.32 took: 2.19s\n",
            "Epoch 1, Iteration 7200\t train_loss: 0.32 took: 2.28s\n",
            "Epoch 1, Iteration 7400\t train_loss: 0.30 took: 2.14s\n",
            "Epoch 1, Iteration 7600\t train_loss: 0.30 took: 2.21s\n",
            "Epoch 1, Iteration 7800\t train_loss: 0.32 took: 2.18s\n",
            "Epoch 1, Iteration 8000\t train_loss: 0.32 took: 2.25s\n",
            "Epoch 1, Iteration 8200\t train_loss: 0.34 took: 2.17s\n",
            "Epoch 1, Iteration 8400\t train_loss: 0.30 took: 2.17s\n",
            "Epoch 1, Iteration 8600\t train_loss: 0.30 took: 2.23s\n",
            "Epoch 1, Iteration 8800\t train_loss: 0.32 took: 2.12s\n",
            "Epoch 1, Iteration 9000\t train_loss: 0.30 took: 2.21s\n",
            "Epoch 1, Iteration 9200\t train_loss: 0.31 took: 2.19s\n",
            "Epoch 1, Iteration 9400\t train_loss: 0.30 took: 2.16s\n",
            "Epoch 1, Iteration 9600\t train_loss: 0.31 took: 2.16s\n",
            "Epoch 1, Iteration 9800\t train_loss: 0.29 took: 2.14s\n",
            "Epoch 1, Iteration 10000\t train_loss: 0.31 took: 2.21s\n",
            "Epoch 1, Iteration 10200\t train_loss: 0.29 took: 2.18s\n",
            "Epoch 1, Iteration 10400\t train_loss: 0.30 took: 2.21s\n",
            "Epoch 1, Iteration 10600\t train_loss: 0.33 took: 2.16s\n",
            "Epoch 1, Iteration 10800\t train_loss: 0.29 took: 2.18s\n",
            "Epoch 1, Iteration 11000\t train_loss: 0.31 took: 2.16s\n",
            "Epoch 1, Iteration 11200\t train_loss: 0.29 took: 2.09s\n",
            "Epoch 1, Iteration 11400\t train_loss: 0.29 took: 2.14s\n",
            "Epoch 1, Iteration 11600\t train_loss: 0.31 took: 2.17s\n",
            "Epoch 1, Iteration 11800\t train_loss: 0.31 took: 2.12s\n",
            "Epoch 1, Iteration 12000\t train_loss: 0.28 took: 2.24s\n",
            "Epoch 1, Iteration 12200\t train_loss: 0.29 took: 2.24s\n",
            "Epoch 1, Iteration 12400\t train_loss: 0.28 took: 2.27s\n",
            "Epoch 1, Iteration 12600\t train_loss: 0.31 took: 2.15s\n",
            "Epoch 1, Iteration 12800\t train_loss: 0.27 took: 2.18s\n",
            "Epoch 1, Iteration 13000\t train_loss: 0.29 took: 2.19s\n",
            "Epoch 1, Iteration 13200\t train_loss: 0.29 took: 2.12s\n",
            "Epoch 1, Iteration 13400\t train_loss: 0.30 took: 2.21s\n",
            "Epoch 1, Iteration 13600\t train_loss: 0.29 took: 2.13s\n",
            "Epoch 1, Iteration 13800\t train_loss: 0.32 took: 2.24s\n",
            "Epoch 1, Iteration 14000\t train_loss: 0.31 took: 2.13s\n",
            "Epoch 1, Iteration 14200\t train_loss: 0.28 took: 2.16s\n",
            "Epoch 1, Iteration 14400\t train_loss: 0.29 took: 2.19s\n",
            "Epoch 1, Iteration 14600\t train_loss: 0.28 took: 2.16s\n",
            "Epoch 1, Iteration 14800\t train_loss: 0.29 took: 2.25s\n",
            "Epoch 1, Iteration 15000\t train_loss: 0.29 took: 2.13s\n",
            "Epoch 1, Iteration 15200\t train_loss: 0.31 took: 2.17s\n",
            "Epoch 1, Iteration 15400\t train_loss: 0.27 took: 2.15s\n",
            "Epoch 1, Iteration 15600\t train_loss: 0.29 took: 2.12s\n",
            "Epoch 1, Iteration 15800\t train_loss: 0.29 took: 2.18s\n",
            "Epoch 1, Iteration 16000\t train_loss: 0.27 took: 1.99s\n",
            "Epoch 1, Iteration 16200\t train_loss: 0.28 took: 2.00s\n",
            "Epoch 1, Iteration 16400\t train_loss: 0.27 took: 1.96s\n",
            "Epoch 1, Iteration 16600\t train_loss: 0.29 took: 1.95s\n",
            "Epoch 1, Iteration 16800\t train_loss: 0.31 took: 1.99s\n",
            "Epoch 1, Iteration 17000\t train_loss: 0.30 took: 1.96s\n",
            "Epoch 1, Iteration 17200\t train_loss: 0.28 took: 1.90s\n",
            "Epoch 1, Iteration 17400\t train_loss: 0.27 took: 2.06s\n",
            "Epoch 1, Iteration 17600\t train_loss: 0.29 took: 2.00s\n",
            "Epoch 1, Iteration 17800\t train_loss: 0.29 took: 2.15s\n",
            "Epoch 1, Iteration 18000\t train_loss: 0.32 took: 2.07s\n",
            "Epoch 1, Iteration 18200\t train_loss: 0.28 took: 2.01s\n",
            "Epoch 1, Iteration 18400\t train_loss: 0.29 took: 2.12s\n",
            "Epoch 1, Iteration 18600\t train_loss: 0.28 took: 1.95s\n",
            "Epoch 1, Iteration 18800\t train_loss: 0.29 took: 2.01s\n",
            "Epoch 1, Iteration 19000\t train_loss: 0.28 took: 1.89s\n",
            "Epoch 1, Iteration 19200\t train_loss: 0.26 took: 1.90s\n",
            "Epoch 1, Iteration 19400\t train_loss: 0.27 took: 2.01s\n",
            "Epoch 1, Iteration 19600\t train_loss: 0.29 took: 1.94s\n",
            "Epoch 1, Iteration 19800\t train_loss: 0.29 took: 2.00s\n",
            "Epoch 1, Iteration 20000\t train_loss: 0.29 took: 1.91s\n",
            "Epoch 1, Iteration 20200\t train_loss: 0.28 took: 1.94s\n",
            "Epoch 1, Iteration 20400\t train_loss: 0.28 took: 2.01s\n",
            "Epoch 1, Iteration 20600\t train_loss: 0.26 took: 1.95s\n",
            "Epoch 1, Iteration 20800\t train_loss: 0.28 took: 2.08s\n",
            "Epoch 1, Iteration 21000\t train_loss: 0.28 took: 1.94s\n",
            "Epoch 1, Iteration 21200\t train_loss: 0.29 took: 1.98s\n",
            "Epoch 1, Iteration 21400\t train_loss: 0.29 took: 1.98s\n",
            "Epoch 1, Iteration 21600\t train_loss: 0.28 took: 1.94s\n",
            "Epoch 1, Iteration 21800\t train_loss: 0.28 took: 1.94s\n",
            "Epoch 1, Iteration 22000\t train_loss: 0.26 took: 1.99s\n",
            "Epoch 1, Iteration 22200\t train_loss: 0.27 took: 1.88s\n",
            "Epoch 1, Iteration 22400\t train_loss: 0.28 took: 1.98s\n",
            "Epoch 1, Iteration 22600\t train_loss: 0.28 took: 1.87s\n",
            "Epoch 1, Iteration 22800\t train_loss: 0.28 took: 1.94s\n",
            "Epoch 1, Iteration 23000\t train_loss: 0.26 took: 2.00s\n",
            "Validation loss = 0.30\n",
            "Epoch 2, Iteration 200\t train_loss: 0.27 took: 2.14s\n",
            "Epoch 2, Iteration 400\t train_loss: 0.27 took: 2.00s\n",
            "Epoch 2, Iteration 600\t train_loss: 0.26 took: 2.08s\n",
            "Epoch 2, Iteration 800\t train_loss: 0.28 took: 1.99s\n",
            "Epoch 2, Iteration 1000\t train_loss: 0.27 took: 1.99s\n",
            "Epoch 2, Iteration 1200\t train_loss: 0.27 took: 2.09s\n",
            "Epoch 2, Iteration 1400\t train_loss: 0.27 took: 2.05s\n",
            "Epoch 2, Iteration 1600\t train_loss: 0.26 took: 2.16s\n",
            "Epoch 2, Iteration 1800\t train_loss: 0.27 took: 2.05s\n",
            "Epoch 2, Iteration 2000\t train_loss: 0.27 took: 2.10s\n",
            "Epoch 2, Iteration 2200\t train_loss: 0.25 took: 2.14s\n",
            "Epoch 2, Iteration 2400\t train_loss: 0.26 took: 2.00s\n",
            "Epoch 2, Iteration 2600\t train_loss: 0.28 took: 2.12s\n",
            "Epoch 2, Iteration 2800\t train_loss: 0.27 took: 2.09s\n",
            "Epoch 2, Iteration 3000\t train_loss: 0.27 took: 2.09s\n",
            "Epoch 2, Iteration 3200\t train_loss: 0.28 took: 2.16s\n",
            "Epoch 2, Iteration 3400\t train_loss: 0.27 took: 2.01s\n",
            "Epoch 2, Iteration 3600\t train_loss: 0.28 took: 2.12s\n",
            "Epoch 2, Iteration 3800\t train_loss: 0.27 took: 2.09s\n",
            "Epoch 2, Iteration 4000\t train_loss: 0.26 took: 2.12s\n",
            "Epoch 2, Iteration 4200\t train_loss: 0.29 took: 2.07s\n",
            "Epoch 2, Iteration 4400\t train_loss: 0.27 took: 2.04s\n",
            "Epoch 2, Iteration 4600\t train_loss: 0.32 took: 2.10s\n",
            "Epoch 2, Iteration 4800\t train_loss: 0.30 took: 2.07s\n",
            "Epoch 2, Iteration 5000\t train_loss: 0.28 took: 2.10s\n",
            "Epoch 2, Iteration 5200\t train_loss: 0.27 took: 2.10s\n",
            "Epoch 2, Iteration 5400\t train_loss: 0.27 took: 2.12s\n",
            "Epoch 2, Iteration 5600\t train_loss: 0.25 took: 2.12s\n",
            "Epoch 2, Iteration 5800\t train_loss: 0.28 took: 2.12s\n",
            "Epoch 2, Iteration 6000\t train_loss: 0.26 took: 2.16s\n",
            "Epoch 2, Iteration 6200\t train_loss: 0.25 took: 2.04s\n",
            "Epoch 2, Iteration 6400\t train_loss: 0.28 took: 2.12s\n",
            "Epoch 2, Iteration 6600\t train_loss: 0.26 took: 1.96s\n",
            "Epoch 2, Iteration 6800\t train_loss: 0.26 took: 1.95s\n",
            "Epoch 2, Iteration 7000\t train_loss: 0.25 took: 2.01s\n",
            "Epoch 2, Iteration 7200\t train_loss: 0.27 took: 2.04s\n",
            "Epoch 2, Iteration 7400\t train_loss: 0.28 took: 2.02s\n",
            "Epoch 2, Iteration 7600\t train_loss: 0.26 took: 1.97s\n",
            "Epoch 2, Iteration 7800\t train_loss: 0.27 took: 1.97s\n",
            "Epoch 2, Iteration 8000\t train_loss: 0.26 took: 2.03s\n",
            "Epoch 2, Iteration 8200\t train_loss: 0.28 took: 1.93s\n",
            "Epoch 2, Iteration 8400\t train_loss: 0.28 took: 2.02s\n",
            "Epoch 2, Iteration 8600\t train_loss: 0.27 took: 2.13s\n",
            "Epoch 2, Iteration 8800\t train_loss: 0.26 took: 2.05s\n",
            "Epoch 2, Iteration 9000\t train_loss: 0.28 took: 2.09s\n",
            "Epoch 2, Iteration 9200\t train_loss: 0.26 took: 1.93s\n",
            "Epoch 2, Iteration 9400\t train_loss: 0.25 took: 2.10s\n",
            "Epoch 2, Iteration 9600\t train_loss: 0.32 took: 2.02s\n",
            "Epoch 2, Iteration 9800\t train_loss: 0.31 took: 2.07s\n",
            "Epoch 2, Iteration 10000\t train_loss: 0.26 took: 2.14s\n",
            "Epoch 2, Iteration 10200\t train_loss: 0.27 took: 2.08s\n",
            "Epoch 2, Iteration 10400\t train_loss: 0.25 took: 2.12s\n",
            "Epoch 2, Iteration 10600\t train_loss: 0.31 took: 2.04s\n",
            "Epoch 2, Iteration 10800\t train_loss: 0.26 took: 2.13s\n",
            "Epoch 2, Iteration 11000\t train_loss: 0.29 took: 2.14s\n",
            "Epoch 2, Iteration 11200\t train_loss: 0.29 took: 2.03s\n",
            "Epoch 2, Iteration 11400\t train_loss: 0.27 took: 2.17s\n",
            "Epoch 2, Iteration 11600\t train_loss: 0.26 took: 2.10s\n",
            "Epoch 2, Iteration 11800\t train_loss: 0.25 took: 2.12s\n",
            "Epoch 2, Iteration 12000\t train_loss: 0.28 took: 2.07s\n",
            "Epoch 2, Iteration 12200\t train_loss: 0.26 took: 2.06s\n",
            "Epoch 2, Iteration 12400\t train_loss: 0.27 took: 2.09s\n",
            "Epoch 2, Iteration 12600\t train_loss: 0.28 took: 2.12s\n",
            "Epoch 2, Iteration 12800\t train_loss: 0.27 took: 2.08s\n",
            "Epoch 2, Iteration 13000\t train_loss: 0.26 took: 2.18s\n",
            "Epoch 2, Iteration 13200\t train_loss: 0.27 took: 2.06s\n",
            "Epoch 2, Iteration 13400\t train_loss: 0.27 took: 2.20s\n",
            "Epoch 2, Iteration 13600\t train_loss: 0.28 took: 2.10s\n",
            "Epoch 2, Iteration 13800\t train_loss: 0.30 took: 2.06s\n",
            "Epoch 2, Iteration 14000\t train_loss: 0.29 took: 2.17s\n",
            "Epoch 2, Iteration 14200\t train_loss: 0.24 took: 2.07s\n",
            "Epoch 2, Iteration 14400\t train_loss: 0.27 took: 2.16s\n",
            "Epoch 2, Iteration 14600\t train_loss: 0.27 took: 2.04s\n",
            "Epoch 2, Iteration 14800\t train_loss: 0.26 took: 2.10s\n",
            "Epoch 2, Iteration 15000\t train_loss: 0.28 took: 2.17s\n",
            "Epoch 2, Iteration 15200\t train_loss: 0.26 took: 2.05s\n",
            "Epoch 2, Iteration 15400\t train_loss: 0.29 took: 2.14s\n",
            "Epoch 2, Iteration 15600\t train_loss: 0.29 took: 2.02s\n",
            "Epoch 2, Iteration 15800\t train_loss: 0.26 took: 2.09s\n",
            "Epoch 2, Iteration 16000\t train_loss: 0.29 took: 2.11s\n",
            "Epoch 2, Iteration 16200\t train_loss: 0.26 took: 2.07s\n",
            "Epoch 2, Iteration 16400\t train_loss: 0.26 took: 2.17s\n",
            "Epoch 2, Iteration 16600\t train_loss: 0.26 took: 2.09s\n",
            "Epoch 2, Iteration 16800\t train_loss: 0.30 took: 2.15s\n",
            "Epoch 2, Iteration 17000\t train_loss: 0.29 took: 2.07s\n",
            "Epoch 2, Iteration 17200\t train_loss: 0.26 took: 2.10s\n",
            "Epoch 2, Iteration 17400\t train_loss: 0.26 took: 2.08s\n",
            "Epoch 2, Iteration 17600\t train_loss: 0.27 took: 2.11s\n",
            "Epoch 2, Iteration 17800\t train_loss: 0.29 took: 2.08s\n",
            "Epoch 2, Iteration 18000\t train_loss: 0.27 took: 2.26s\n",
            "Epoch 2, Iteration 18200\t train_loss: 0.27 took: 2.11s\n",
            "Epoch 2, Iteration 18400\t train_loss: 0.27 took: 2.21s\n",
            "Epoch 2, Iteration 18600\t train_loss: 0.26 took: 2.04s\n",
            "Epoch 2, Iteration 18800\t train_loss: 0.25 took: 2.06s\n",
            "Epoch 2, Iteration 19000\t train_loss: 0.26 took: 2.12s\n",
            "Epoch 2, Iteration 19200\t train_loss: 0.24 took: 1.94s\n",
            "Epoch 2, Iteration 19400\t train_loss: 0.25 took: 2.02s\n",
            "Epoch 2, Iteration 19600\t train_loss: 0.27 took: 1.90s\n",
            "Epoch 2, Iteration 19800\t train_loss: 0.26 took: 1.91s\n",
            "Epoch 2, Iteration 20000\t train_loss: 0.27 took: 1.99s\n",
            "Epoch 2, Iteration 20200\t train_loss: 0.27 took: 1.93s\n",
            "Epoch 2, Iteration 20400\t train_loss: 0.27 took: 1.97s\n",
            "Epoch 2, Iteration 20600\t train_loss: 0.28 took: 1.92s\n",
            "Epoch 2, Iteration 20800\t train_loss: 0.26 took: 1.94s\n",
            "Epoch 2, Iteration 21000\t train_loss: 0.30 took: 1.99s\n",
            "Epoch 2, Iteration 21200\t train_loss: 0.27 took: 1.86s\n",
            "Epoch 2, Iteration 21400\t train_loss: 0.26 took: 2.02s\n",
            "Epoch 2, Iteration 21600\t train_loss: 0.26 took: 1.98s\n",
            "Epoch 2, Iteration 21800\t train_loss: 0.26 took: 1.99s\n",
            "Epoch 2, Iteration 22000\t train_loss: 0.27 took: 2.04s\n",
            "Epoch 2, Iteration 22200\t train_loss: 0.30 took: 1.88s\n",
            "Epoch 2, Iteration 22400\t train_loss: 0.35 took: 2.00s\n",
            "Epoch 2, Iteration 22600\t train_loss: 0.28 took: 1.94s\n",
            "Epoch 2, Iteration 22800\t train_loss: 0.29 took: 1.90s\n",
            "Epoch 2, Iteration 23000\t train_loss: 0.27 took: 1.98s\n",
            "Validation loss = 0.27\n",
            "Epoch 3, Iteration 200\t train_loss: 0.27 took: 2.08s\n",
            "Epoch 3, Iteration 400\t train_loss: 0.27 took: 2.10s\n",
            "Epoch 3, Iteration 600\t train_loss: 0.25 took: 1.96s\n",
            "Epoch 3, Iteration 800\t train_loss: 0.28 took: 2.14s\n",
            "Epoch 3, Iteration 1000\t train_loss: 0.27 took: 1.99s\n",
            "Epoch 3, Iteration 1200\t train_loss: 0.28 took: 2.06s\n",
            "Epoch 3, Iteration 1400\t train_loss: 0.24 took: 2.10s\n",
            "Epoch 3, Iteration 1600\t train_loss: 0.26 took: 1.97s\n",
            "Epoch 3, Iteration 1800\t train_loss: 0.30 took: 2.02s\n",
            "Epoch 3, Iteration 2000\t train_loss: 0.29 took: 1.97s\n",
            "Epoch 3, Iteration 2200\t train_loss: 0.26 took: 2.28s\n",
            "Epoch 3, Iteration 2400\t train_loss: 0.27 took: 2.33s\n",
            "Epoch 3, Iteration 2600\t train_loss: 0.27 took: 2.34s\n",
            "Epoch 3, Iteration 2800\t train_loss: 0.28 took: 2.36s\n",
            "Epoch 3, Iteration 3000\t train_loss: 0.27 took: 2.31s\n",
            "Epoch 3, Iteration 3200\t train_loss: 0.26 took: 2.43s\n",
            "Epoch 3, Iteration 3400\t train_loss: 0.26 took: 2.30s\n",
            "Epoch 3, Iteration 3600\t train_loss: 0.22 took: 2.36s\n",
            "Epoch 3, Iteration 3800\t train_loss: 0.28 took: 2.28s\n",
            "Epoch 3, Iteration 4000\t train_loss: 0.25 took: 2.26s\n",
            "Epoch 3, Iteration 4200\t train_loss: 0.28 took: 2.38s\n",
            "Epoch 3, Iteration 4400\t train_loss: 0.27 took: 2.29s\n",
            "Epoch 3, Iteration 4600\t train_loss: 0.25 took: 2.27s\n",
            "Epoch 3, Iteration 4800\t train_loss: 0.26 took: 2.35s\n",
            "Epoch 3, Iteration 5000\t train_loss: 0.30 took: 2.29s\n",
            "Epoch 3, Iteration 5200\t train_loss: 0.29 took: 2.40s\n",
            "Epoch 3, Iteration 5400\t train_loss: 0.33 took: 2.24s\n",
            "Epoch 3, Iteration 5600\t train_loss: 0.32 took: 2.42s\n",
            "Epoch 3, Iteration 5800\t train_loss: 0.28 took: 2.35s\n",
            "Epoch 3, Iteration 6000\t train_loss: 0.26 took: 2.34s\n",
            "Epoch 3, Iteration 6200\t train_loss: 0.24 took: 2.33s\n",
            "Epoch 3, Iteration 6400\t train_loss: 0.27 took: 2.35s\n",
            "Epoch 3, Iteration 6600\t train_loss: 0.26 took: 2.34s\n",
            "Epoch 3, Iteration 6800\t train_loss: 0.24 took: 2.26s\n",
            "Epoch 3, Iteration 7000\t train_loss: 0.25 took: 2.41s\n",
            "Epoch 3, Iteration 7200\t train_loss: 0.25 took: 2.22s\n",
            "Epoch 3, Iteration 7400\t train_loss: 0.25 took: 2.35s\n",
            "Epoch 3, Iteration 7600\t train_loss: 0.25 took: 2.22s\n",
            "Epoch 3, Iteration 7800\t train_loss: 0.26 took: 2.26s\n",
            "Epoch 3, Iteration 8000\t train_loss: 0.28 took: 2.36s\n",
            "Epoch 3, Iteration 8200\t train_loss: 0.27 took: 2.27s\n",
            "Epoch 3, Iteration 8400\t train_loss: 0.27 took: 2.32s\n",
            "Epoch 3, Iteration 8600\t train_loss: 0.25 took: 2.26s\n",
            "Epoch 3, Iteration 8800\t train_loss: 0.25 took: 2.18s\n",
            "Epoch 3, Iteration 9000\t train_loss: 0.26 took: 2.37s\n",
            "Epoch 3, Iteration 9200\t train_loss: 0.27 took: 2.23s\n",
            "Epoch 3, Iteration 9400\t train_loss: 0.28 took: 2.32s\n",
            "Epoch 3, Iteration 9600\t train_loss: 0.24 took: 2.20s\n",
            "Epoch 3, Iteration 9800\t train_loss: 0.26 took: 2.29s\n",
            "Epoch 3, Iteration 10000\t train_loss: 0.29 took: 2.25s\n",
            "Epoch 3, Iteration 10200\t train_loss: 0.27 took: 2.25s\n",
            "Epoch 3, Iteration 10400\t train_loss: 0.33 took: 2.29s\n",
            "Epoch 3, Iteration 10600\t train_loss: 0.30 took: 2.21s\n",
            "Epoch 3, Iteration 10800\t train_loss: 0.26 took: 2.23s\n",
            "Epoch 3, Iteration 11000\t train_loss: 0.27 took: 2.23s\n",
            "Epoch 3, Iteration 11200\t train_loss: 0.26 took: 2.30s\n",
            "Epoch 3, Iteration 11400\t train_loss: 0.26 took: 2.20s\n",
            "Epoch 3, Iteration 11600\t train_loss: 0.26 took: 2.24s\n",
            "Epoch 3, Iteration 11800\t train_loss: 0.27 took: 2.25s\n",
            "Epoch 3, Iteration 12000\t train_loss: 0.25 took: 2.18s\n",
            "Epoch 3, Iteration 12200\t train_loss: 0.25 took: 2.31s\n",
            "Epoch 3, Iteration 12400\t train_loss: 0.25 took: 2.22s\n",
            "Epoch 3, Iteration 12600\t train_loss: 0.26 took: 2.31s\n",
            "Epoch 3, Iteration 12800\t train_loss: 0.29 took: 2.16s\n",
            "Epoch 3, Iteration 13000\t train_loss: 0.27 took: 2.22s\n",
            "Epoch 3, Iteration 13200\t train_loss: 0.25 took: 2.29s\n",
            "Epoch 3, Iteration 13400\t train_loss: 0.27 took: 2.20s\n",
            "Epoch 3, Iteration 13600\t train_loss: 0.30 took: 2.21s\n",
            "Epoch 3, Iteration 13800\t train_loss: 0.24 took: 2.28s\n",
            "Epoch 3, Iteration 14000\t train_loss: 0.24 took: 2.29s\n",
            "Epoch 3, Iteration 14200\t train_loss: 0.25 took: 2.29s\n",
            "Epoch 3, Iteration 14400\t train_loss: 0.27 took: 2.28s\n",
            "Epoch 3, Iteration 14600\t train_loss: 0.25 took: 2.32s\n",
            "Epoch 3, Iteration 14800\t train_loss: 0.27 took: 2.14s\n",
            "Epoch 3, Iteration 15000\t train_loss: 0.26 took: 2.27s\n",
            "Epoch 3, Iteration 15200\t train_loss: 0.26 took: 2.15s\n",
            "Epoch 3, Iteration 15400\t train_loss: 0.25 took: 2.21s\n",
            "Epoch 3, Iteration 15600\t train_loss: 0.25 took: 2.02s\n",
            "Epoch 3, Iteration 15800\t train_loss: 0.27 took: 2.00s\n",
            "Epoch 3, Iteration 16000\t train_loss: 0.26 took: 2.06s\n",
            "Epoch 3, Iteration 16200\t train_loss: 0.25 took: 1.92s\n",
            "Epoch 3, Iteration 16400\t train_loss: 0.26 took: 2.16s\n",
            "Epoch 3, Iteration 16600\t train_loss: 0.25 took: 1.95s\n",
            "Epoch 3, Iteration 16800\t train_loss: 0.26 took: 1.99s\n",
            "Epoch 3, Iteration 17000\t train_loss: 0.29 took: 2.11s\n",
            "Epoch 3, Iteration 17200\t train_loss: 0.27 took: 2.00s\n",
            "Epoch 3, Iteration 17400\t train_loss: 0.26 took: 2.07s\n",
            "Epoch 3, Iteration 17600\t train_loss: 0.27 took: 1.94s\n",
            "Epoch 3, Iteration 17800\t train_loss: 0.25 took: 2.01s\n",
            "Epoch 3, Iteration 18000\t train_loss: 0.26 took: 1.95s\n",
            "Epoch 3, Iteration 18200\t train_loss: 0.26 took: 1.97s\n",
            "Epoch 3, Iteration 18400\t train_loss: 0.27 took: 1.97s\n",
            "Epoch 3, Iteration 18600\t train_loss: 0.27 took: 2.05s\n",
            "Epoch 3, Iteration 18800\t train_loss: 0.25 took: 1.95s\n",
            "Epoch 3, Iteration 19000\t train_loss: 0.26 took: 2.08s\n",
            "Epoch 3, Iteration 19200\t train_loss: 0.26 took: 1.96s\n",
            "Epoch 3, Iteration 19400\t train_loss: 0.26 took: 1.98s\n",
            "Epoch 3, Iteration 19600\t train_loss: 0.24 took: 2.01s\n",
            "Epoch 3, Iteration 19800\t train_loss: 0.27 took: 1.95s\n",
            "Epoch 3, Iteration 20000\t train_loss: 0.27 took: 2.07s\n",
            "Epoch 3, Iteration 20200\t train_loss: 0.27 took: 1.90s\n",
            "Epoch 3, Iteration 20400\t train_loss: 0.29 took: 1.98s\n",
            "Epoch 3, Iteration 20600\t train_loss: 0.25 took: 2.05s\n",
            "Epoch 3, Iteration 20800\t train_loss: 0.26 took: 1.92s\n",
            "Epoch 3, Iteration 21000\t train_loss: 0.28 took: 2.03s\n",
            "Epoch 3, Iteration 21200\t train_loss: 0.24 took: 1.91s\n",
            "Epoch 3, Iteration 21400\t train_loss: 0.29 took: 1.99s\n",
            "Epoch 3, Iteration 21600\t train_loss: 0.25 took: 2.05s\n",
            "Epoch 3, Iteration 21800\t train_loss: 0.26 took: 1.96s\n",
            "Epoch 3, Iteration 22000\t train_loss: 0.27 took: 2.09s\n",
            "Epoch 3, Iteration 22200\t train_loss: 0.27 took: 1.93s\n",
            "Epoch 3, Iteration 22400\t train_loss: 0.26 took: 2.00s\n",
            "Epoch 3, Iteration 22600\t train_loss: 0.26 took: 2.06s\n",
            "Epoch 3, Iteration 22800\t train_loss: 0.27 took: 2.05s\n",
            "Epoch 3, Iteration 23000\t train_loss: 0.26 took: 2.11s\n",
            "Validation loss = 0.27\n",
            "Epoch 4, Iteration 200\t train_loss: 0.27 took: 2.25s\n",
            "Epoch 4, Iteration 400\t train_loss: 0.25 took: 2.19s\n",
            "Epoch 4, Iteration 600\t train_loss: 0.27 took: 2.09s\n",
            "Epoch 4, Iteration 800\t train_loss: 0.25 took: 2.14s\n",
            "Epoch 4, Iteration 1000\t train_loss: 0.22 took: 2.07s\n",
            "Epoch 4, Iteration 1200\t train_loss: 0.25 took: 2.13s\n",
            "Epoch 4, Iteration 1400\t train_loss: 0.26 took: 2.10s\n",
            "Epoch 4, Iteration 1600\t train_loss: 0.26 took: 2.06s\n",
            "Epoch 4, Iteration 1800\t train_loss: 0.25 took: 2.19s\n",
            "Epoch 4, Iteration 2000\t train_loss: 0.25 took: 2.06s\n",
            "Epoch 4, Iteration 2200\t train_loss: 0.26 took: 2.13s\n",
            "Epoch 4, Iteration 2400\t train_loss: 0.24 took: 2.06s\n",
            "Epoch 4, Iteration 2600\t train_loss: 0.29 took: 2.16s\n",
            "Epoch 4, Iteration 2800\t train_loss: 0.25 took: 2.10s\n",
            "Epoch 4, Iteration 3000\t train_loss: 0.26 took: 2.11s\n",
            "Epoch 4, Iteration 3200\t train_loss: 0.25 took: 2.11s\n",
            "Epoch 4, Iteration 3400\t train_loss: 0.25 took: 2.07s\n",
            "Epoch 4, Iteration 3600\t train_loss: 0.27 took: 2.17s\n",
            "Epoch 4, Iteration 3800\t train_loss: 0.27 took: 2.06s\n",
            "Epoch 4, Iteration 4000\t train_loss: 0.25 took: 2.10s\n",
            "Epoch 4, Iteration 4200\t train_loss: 0.26 took: 2.10s\n",
            "Epoch 4, Iteration 4400\t train_loss: 0.25 took: 2.06s\n",
            "Epoch 4, Iteration 4600\t train_loss: 0.25 took: 2.10s\n",
            "Epoch 4, Iteration 4800\t train_loss: 0.27 took: 2.12s\n",
            "Epoch 4, Iteration 5000\t train_loss: 0.31 took: 2.10s\n",
            "Epoch 4, Iteration 5200\t train_loss: 0.27 took: 2.16s\n",
            "Epoch 4, Iteration 5400\t train_loss: 0.26 took: 1.98s\n",
            "Epoch 4, Iteration 5600\t train_loss: 0.26 took: 2.06s\n",
            "Epoch 4, Iteration 5800\t train_loss: 0.25 took: 2.08s\n",
            "Epoch 4, Iteration 6000\t train_loss: 0.24 took: 2.00s\n",
            "Epoch 4, Iteration 6200\t train_loss: 0.26 took: 2.10s\n",
            "Epoch 4, Iteration 6400\t train_loss: 0.27 took: 1.98s\n",
            "Epoch 4, Iteration 6600\t train_loss: 0.25 took: 2.10s\n",
            "Epoch 4, Iteration 6800\t train_loss: 0.30 took: 1.92s\n",
            "Epoch 4, Iteration 7000\t train_loss: 0.28 took: 1.99s\n",
            "Epoch 4, Iteration 7200\t train_loss: 0.27 took: 2.18s\n",
            "Epoch 4, Iteration 7400\t train_loss: 0.26 took: 2.03s\n",
            "Epoch 4, Iteration 7600\t train_loss: 0.26 took: 2.07s\n",
            "Epoch 4, Iteration 7800\t train_loss: 0.26 took: 2.02s\n",
            "Epoch 4, Iteration 8000\t train_loss: 0.29 took: 2.16s\n",
            "Epoch 4, Iteration 8200\t train_loss: 0.29 took: 2.10s\n",
            "Epoch 4, Iteration 8400\t train_loss: 0.25 took: 2.06s\n",
            "Epoch 4, Iteration 8600\t train_loss: 0.27 took: 2.15s\n",
            "Epoch 4, Iteration 8800\t train_loss: 0.25 took: 2.05s\n",
            "Epoch 4, Iteration 9000\t train_loss: 0.25 took: 2.19s\n",
            "Epoch 4, Iteration 9200\t train_loss: 0.28 took: 2.12s\n",
            "Epoch 4, Iteration 9400\t train_loss: 0.27 took: 2.07s\n",
            "Epoch 4, Iteration 9600\t train_loss: 0.27 took: 2.20s\n",
            "Epoch 4, Iteration 9800\t train_loss: 0.24 took: 2.10s\n",
            "Epoch 4, Iteration 10000\t train_loss: 0.24 took: 2.10s\n",
            "Epoch 4, Iteration 10200\t train_loss: 0.25 took: 2.20s\n",
            "Epoch 4, Iteration 10400\t train_loss: 0.23 took: 2.12s\n",
            "Epoch 4, Iteration 10600\t train_loss: 0.27 took: 2.22s\n",
            "Epoch 4, Iteration 10800\t train_loss: 0.24 took: 2.10s\n",
            "Epoch 4, Iteration 11000\t train_loss: 0.28 took: 2.21s\n",
            "Epoch 4, Iteration 11200\t train_loss: 0.24 took: 2.15s\n",
            "Epoch 4, Iteration 11400\t train_loss: 0.27 took: 2.17s\n",
            "Epoch 4, Iteration 11600\t train_loss: 0.26 took: 2.20s\n",
            "Epoch 4, Iteration 11800\t train_loss: 0.27 took: 2.07s\n",
            "Epoch 4, Iteration 12000\t train_loss: 0.27 took: 2.29s\n",
            "Epoch 4, Iteration 12200\t train_loss: 0.26 took: 2.03s\n",
            "Epoch 4, Iteration 12400\t train_loss: 0.25 took: 2.16s\n",
            "Epoch 4, Iteration 12600\t train_loss: 0.26 took: 2.19s\n",
            "Epoch 4, Iteration 12800\t train_loss: 0.29 took: 2.11s\n",
            "Epoch 4, Iteration 13000\t train_loss: 0.27 took: 2.26s\n",
            "Epoch 4, Iteration 13200\t train_loss: 0.27 took: 2.05s\n",
            "Epoch 4, Iteration 13400\t train_loss: 0.26 took: 2.20s\n",
            "Epoch 4, Iteration 13600\t train_loss: 0.28 took: 2.13s\n",
            "Epoch 4, Iteration 13800\t train_loss: 0.29 took: 2.11s\n",
            "Epoch 4, Iteration 14000\t train_loss: 0.27 took: 2.15s\n",
            "Epoch 4, Iteration 14200\t train_loss: 0.28 took: 2.15s\n",
            "Epoch 4, Iteration 14400\t train_loss: 0.26 took: 2.12s\n",
            "Epoch 4, Iteration 14600\t train_loss: 0.35 took: 2.18s\n",
            "Epoch 4, Iteration 14800\t train_loss: 0.26 took: 2.10s\n",
            "Epoch 4, Iteration 15000\t train_loss: 0.27 took: 2.02s\n",
            "Epoch 4, Iteration 15200\t train_loss: 0.28 took: 1.96s\n",
            "Epoch 4, Iteration 15400\t train_loss: 0.27 took: 1.99s\n",
            "Epoch 4, Iteration 15600\t train_loss: 0.26 took: 1.97s\n",
            "Epoch 4, Iteration 15800\t train_loss: 0.26 took: 1.97s\n",
            "Epoch 4, Iteration 16000\t train_loss: 0.25 took: 1.96s\n",
            "Epoch 4, Iteration 16200\t train_loss: 0.26 took: 1.93s\n",
            "Epoch 4, Iteration 16400\t train_loss: 0.24 took: 2.05s\n",
            "Epoch 4, Iteration 16600\t train_loss: 0.25 took: 1.95s\n",
            "Epoch 4, Iteration 16800\t train_loss: 0.26 took: 1.94s\n",
            "Epoch 4, Iteration 17000\t train_loss: 0.26 took: 2.02s\n",
            "Epoch 4, Iteration 17200\t train_loss: 0.24 took: 1.94s\n",
            "Epoch 4, Iteration 17400\t train_loss: 0.24 took: 2.03s\n",
            "Epoch 4, Iteration 17600\t train_loss: 0.25 took: 1.93s\n",
            "Epoch 4, Iteration 17800\t train_loss: 0.27 took: 1.93s\n",
            "Epoch 4, Iteration 18000\t train_loss: 0.26 took: 2.06s\n",
            "Epoch 4, Iteration 18200\t train_loss: 0.25 took: 1.92s\n",
            "Epoch 4, Iteration 18400\t train_loss: 0.24 took: 2.01s\n",
            "Epoch 4, Iteration 18600\t train_loss: 0.25 took: 1.99s\n",
            "Epoch 4, Iteration 18800\t train_loss: 0.24 took: 1.98s\n",
            "Epoch 4, Iteration 19000\t train_loss: 0.25 took: 2.06s\n",
            "Epoch 4, Iteration 19200\t train_loss: 0.25 took: 1.93s\n",
            "Epoch 4, Iteration 19400\t train_loss: 0.25 took: 1.99s\n",
            "Epoch 4, Iteration 19600\t train_loss: 0.27 took: 1.98s\n",
            "Epoch 4, Iteration 19800\t train_loss: 0.25 took: 1.97s\n",
            "Epoch 4, Iteration 20000\t train_loss: 0.27 took: 2.03s\n",
            "Epoch 4, Iteration 20200\t train_loss: 0.31 took: 1.98s\n",
            "Epoch 4, Iteration 20400\t train_loss: 0.28 took: 1.92s\n",
            "Epoch 4, Iteration 20600\t train_loss: 0.26 took: 2.03s\n",
            "Epoch 4, Iteration 20800\t train_loss: 0.28 took: 1.88s\n",
            "Epoch 4, Iteration 21000\t train_loss: 0.30 took: 2.02s\n",
            "Epoch 4, Iteration 21200\t train_loss: 0.27 took: 2.01s\n",
            "Epoch 4, Iteration 21400\t train_loss: 0.26 took: 1.91s\n",
            "Epoch 4, Iteration 21600\t train_loss: 0.24 took: 2.07s\n",
            "Epoch 4, Iteration 21800\t train_loss: 0.26 took: 1.96s\n",
            "Epoch 4, Iteration 22000\t train_loss: 0.29 took: 2.20s\n",
            "Epoch 4, Iteration 22200\t train_loss: 0.33 took: 2.26s\n",
            "Epoch 4, Iteration 22400\t train_loss: 0.28 took: 2.27s\n",
            "Epoch 4, Iteration 22600\t train_loss: 0.26 took: 2.27s\n",
            "Epoch 4, Iteration 22800\t train_loss: 0.27 took: 2.24s\n",
            "Epoch 4, Iteration 23000\t train_loss: 0.28 took: 2.37s\n",
            "Validation loss = 0.28\n",
            "Epoch 5, Iteration 200\t train_loss: 0.24 took: 2.42s\n",
            "Epoch 5, Iteration 400\t train_loss: 0.25 took: 2.26s\n",
            "Epoch 5, Iteration 600\t train_loss: 0.26 took: 2.29s\n",
            "Epoch 5, Iteration 800\t train_loss: 0.24 took: 2.24s\n",
            "Epoch 5, Iteration 1000\t train_loss: 0.25 took: 2.25s\n",
            "Epoch 5, Iteration 1200\t train_loss: 0.27 took: 2.32s\n",
            "Epoch 5, Iteration 1400\t train_loss: 0.28 took: 2.25s\n",
            "Epoch 5, Iteration 1600\t train_loss: 0.25 took: 2.32s\n",
            "Epoch 5, Iteration 1800\t train_loss: 0.26 took: 2.19s\n",
            "Epoch 5, Iteration 2000\t train_loss: 0.24 took: 2.28s\n",
            "Epoch 5, Iteration 2200\t train_loss: 0.24 took: 2.28s\n",
            "Epoch 5, Iteration 2400\t train_loss: 0.26 took: 2.25s\n",
            "Epoch 5, Iteration 2600\t train_loss: 0.28 took: 2.30s\n",
            "Epoch 5, Iteration 2800\t train_loss: 0.25 took: 2.19s\n",
            "Epoch 5, Iteration 3000\t train_loss: 0.25 took: 2.25s\n",
            "Epoch 5, Iteration 3200\t train_loss: 0.26 took: 2.24s\n",
            "Epoch 5, Iteration 3400\t train_loss: 0.24 took: 2.29s\n",
            "Epoch 5, Iteration 3600\t train_loss: 0.26 took: 2.15s\n",
            "Epoch 5, Iteration 3800\t train_loss: 0.26 took: 2.11s\n",
            "Epoch 5, Iteration 4000\t train_loss: 0.24 took: 2.22s\n",
            "Epoch 5, Iteration 4200\t train_loss: 0.26 took: 2.14s\n",
            "Epoch 5, Iteration 4400\t train_loss: 0.27 took: 2.28s\n",
            "Epoch 5, Iteration 4600\t train_loss: 0.25 took: 2.28s\n",
            "Epoch 5, Iteration 4800\t train_loss: 0.26 took: 2.32s\n",
            "Epoch 5, Iteration 5000\t train_loss: 0.27 took: 2.14s\n",
            "Epoch 5, Iteration 5200\t train_loss: 0.27 took: 2.16s\n",
            "Epoch 5, Iteration 5400\t train_loss: 0.25 took: 2.16s\n",
            "Epoch 5, Iteration 5600\t train_loss: 0.26 took: 2.17s\n",
            "Epoch 5, Iteration 5800\t train_loss: 0.25 took: 2.24s\n",
            "Epoch 5, Iteration 6000\t train_loss: 0.26 took: 2.27s\n",
            "Epoch 5, Iteration 6200\t train_loss: 0.26 took: 2.16s\n",
            "Epoch 5, Iteration 6400\t train_loss: 0.25 took: 2.21s\n",
            "Epoch 5, Iteration 6600\t train_loss: 0.25 took: 2.20s\n",
            "Epoch 5, Iteration 6800\t train_loss: 0.23 took: 2.31s\n",
            "Epoch 5, Iteration 7000\t train_loss: 0.26 took: 2.19s\n",
            "Epoch 5, Iteration 7200\t train_loss: 0.27 took: 2.17s\n",
            "Epoch 5, Iteration 7400\t train_loss: 0.26 took: 2.11s\n",
            "Epoch 5, Iteration 7600\t train_loss: 0.27 took: 2.03s\n",
            "Epoch 5, Iteration 7800\t train_loss: 0.24 took: 2.14s\n",
            "Epoch 5, Iteration 8000\t train_loss: 0.25 took: 1.99s\n",
            "Epoch 5, Iteration 8200\t train_loss: 0.29 took: 2.00s\n",
            "Epoch 5, Iteration 8400\t train_loss: 0.26 took: 2.02s\n",
            "Epoch 5, Iteration 8600\t train_loss: 0.24 took: 2.00s\n",
            "Epoch 5, Iteration 8800\t train_loss: 0.24 took: 2.09s\n",
            "Epoch 5, Iteration 9000\t train_loss: 0.25 took: 1.93s\n",
            "Epoch 5, Iteration 9200\t train_loss: 0.27 took: 2.06s\n",
            "Epoch 5, Iteration 9400\t train_loss: 0.27 took: 1.90s\n",
            "Epoch 5, Iteration 9600\t train_loss: 0.25 took: 1.95s\n",
            "Epoch 5, Iteration 9800\t train_loss: 0.26 took: 2.03s\n",
            "Epoch 5, Iteration 10000\t train_loss: 0.24 took: 1.91s\n",
            "Epoch 5, Iteration 10200\t train_loss: 0.26 took: 1.97s\n",
            "Epoch 5, Iteration 10400\t train_loss: 0.25 took: 1.95s\n",
            "Epoch 5, Iteration 10600\t train_loss: 0.24 took: 1.88s\n",
            "Epoch 5, Iteration 10800\t train_loss: 0.26 took: 1.96s\n",
            "Epoch 5, Iteration 11000\t train_loss: 0.26 took: 1.99s\n",
            "Epoch 5, Iteration 11200\t train_loss: 0.26 took: 1.88s\n",
            "Epoch 5, Iteration 11400\t train_loss: 0.26 took: 2.04s\n",
            "Epoch 5, Iteration 11600\t train_loss: 0.24 took: 1.92s\n",
            "Epoch 5, Iteration 11800\t train_loss: 0.25 took: 2.01s\n",
            "Epoch 5, Iteration 12000\t train_loss: 0.26 took: 2.03s\n",
            "Epoch 5, Iteration 12200\t train_loss: 0.24 took: 1.98s\n",
            "Epoch 5, Iteration 12400\t train_loss: 0.28 took: 2.11s\n",
            "Epoch 5, Iteration 12600\t train_loss: 0.25 took: 1.94s\n",
            "Epoch 5, Iteration 12800\t train_loss: 0.27 took: 1.96s\n",
            "Epoch 5, Iteration 13000\t train_loss: 0.25 took: 1.96s\n",
            "Epoch 5, Iteration 13200\t train_loss: 0.28 took: 1.96s\n",
            "Epoch 5, Iteration 13400\t train_loss: 0.36 took: 2.01s\n",
            "Epoch 5, Iteration 13600\t train_loss: 0.30 took: 1.94s\n",
            "Epoch 5, Iteration 13800\t train_loss: 0.27 took: 2.04s\n",
            "Epoch 5, Iteration 14000\t train_loss: 0.28 took: 1.92s\n",
            "Epoch 5, Iteration 14200\t train_loss: 0.27 took: 1.99s\n",
            "Epoch 5, Iteration 14400\t train_loss: 0.26 took: 2.03s\n",
            "Epoch 5, Iteration 14600\t train_loss: 0.25 took: 1.91s\n",
            "Epoch 5, Iteration 14800\t train_loss: 0.26 took: 2.12s\n",
            "Epoch 5, Iteration 15000\t train_loss: 0.26 took: 2.08s\n",
            "Epoch 5, Iteration 15200\t train_loss: 0.27 took: 2.08s\n",
            "Epoch 5, Iteration 15400\t train_loss: 0.26 took: 2.04s\n",
            "Epoch 5, Iteration 15600\t train_loss: 0.25 took: 2.10s\n",
            "Epoch 5, Iteration 15800\t train_loss: 0.24 took: 2.19s\n",
            "Epoch 5, Iteration 16000\t train_loss: 0.23 took: 2.06s\n",
            "Epoch 5, Iteration 16200\t train_loss: 0.27 took: 2.07s\n",
            "Epoch 5, Iteration 16400\t train_loss: 0.26 took: 2.14s\n",
            "Epoch 5, Iteration 16600\t train_loss: 0.26 took: 2.09s\n",
            "Epoch 5, Iteration 16800\t train_loss: 0.26 took: 2.17s\n",
            "Epoch 5, Iteration 17000\t train_loss: 0.26 took: 2.02s\n",
            "Epoch 5, Iteration 17200\t train_loss: 0.25 took: 2.12s\n",
            "Epoch 5, Iteration 17400\t train_loss: 0.24 took: 2.08s\n",
            "Epoch 5, Iteration 17600\t train_loss: 0.25 took: 2.08s\n",
            "Epoch 5, Iteration 17800\t train_loss: 0.24 took: 2.12s\n",
            "Epoch 5, Iteration 18000\t train_loss: 0.27 took: 2.04s\n",
            "Epoch 5, Iteration 18200\t train_loss: 0.27 took: 2.15s\n",
            "Epoch 5, Iteration 18400\t train_loss: 0.28 took: 2.07s\n",
            "Epoch 5, Iteration 18600\t train_loss: 0.33 took: 2.14s\n",
            "Epoch 5, Iteration 18800\t train_loss: 0.28 took: 2.12s\n",
            "Epoch 5, Iteration 19000\t train_loss: 0.28 took: 2.10s\n",
            "Epoch 5, Iteration 19200\t train_loss: 0.26 took: 2.13s\n",
            "Epoch 5, Iteration 19400\t train_loss: 0.25 took: 2.08s\n",
            "Epoch 5, Iteration 19600\t train_loss: 0.25 took: 2.13s\n",
            "Epoch 5, Iteration 19800\t train_loss: 0.25 took: 2.08s\n",
            "Epoch 5, Iteration 20000\t train_loss: 0.23 took: 2.12s\n",
            "Epoch 5, Iteration 20200\t train_loss: 0.26 took: 2.03s\n",
            "Epoch 5, Iteration 20400\t train_loss: 0.25 took: 1.98s\n",
            "Epoch 5, Iteration 20600\t train_loss: 0.26 took: 2.06s\n",
            "Epoch 5, Iteration 20800\t train_loss: 0.27 took: 2.02s\n",
            "Epoch 5, Iteration 21000\t train_loss: 0.26 took: 1.95s\n",
            "Epoch 5, Iteration 21200\t train_loss: 0.25 took: 2.04s\n",
            "Epoch 5, Iteration 21400\t train_loss: 0.25 took: 1.91s\n",
            "Epoch 5, Iteration 21600\t train_loss: 0.26 took: 1.98s\n",
            "Epoch 5, Iteration 21800\t train_loss: 0.26 took: 2.01s\n",
            "Epoch 5, Iteration 22000\t train_loss: 0.25 took: 2.02s\n",
            "Epoch 5, Iteration 22200\t train_loss: 0.26 took: 2.08s\n",
            "Epoch 5, Iteration 22400\t train_loss: 0.28 took: 1.94s\n",
            "Epoch 5, Iteration 22600\t train_loss: 0.26 took: 2.08s\n",
            "Epoch 5, Iteration 22800\t train_loss: 0.25 took: 2.05s\n",
            "Epoch 5, Iteration 23000\t train_loss: 0.27 took: 2.02s\n",
            "Validation loss = 0.27\n",
            "Training finished, took 1243.73s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNSsFzVji6W",
        "colab_type": "code",
        "outputId": "571cf46d-b552-4b10-ad84-c1aa3d7fcddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# Plot the loss from above\n",
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEKCAYAAAA8QgPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4XMXV/z+zq12tumRJlotc5IJ7\nF7bB2MZgwMaJ6WBKAiSEQCCE8HshDmmEhAQILy0vCSEEQ+jEhBaKaXZsio1777Zky72pl9VK8/vj\nFu1Ku6q7luQ9n+fRo9175947d+/ufOecOXNGaa0RBEEQhJbiaO8KCIIgCJ0TERBBEAShVYiACIIg\nCK1CBEQQBEFoFSIggiAIQqsQAREEQRBahQiIIAiC0CpEQARBEIRWIQIiCIIgtIqY9q5AS8nIyNB9\n+/Zt72oIgiB0KlauXHlUa50ZznN2OgHp27cvK1asaO9qCIIgdCqUUvnhPqe4sARBEIRWIQIiCIIg\ntAoREEEQBKFVdLoxEEEQTj7V1dUUFBRQWVnZ3lURmsDj8ZCdnY3L5Yr4tURABEFokoKCApKSkujb\nty9KqfaujhACrTXHjh2joKCAnJyciF9PXFiCIDRJZWUl6enpIh4dHKUU6enpJ81SFAERBKFZiHh0\nDk7mc4oaAVmed5z//XgrXl9te1dFEAThlCBqBGRV/gn+/PkOfLUiIILQ2SgsLOQvf/lLq4698MIL\nKSwsbHb5++67j0ceeaRV14o2okZALKuuVrdvPQRBaDmNCYjP52v02A8++IDU1NRIVCvqiRoBcZgK\norUoiCB0NubOncvOnTsZPXo0d999N4sWLWLy5MnMnj2boUOHAnDxxRczbtw4hg0bxjPPPGMf27dv\nX44ePUpeXh5DhgzhBz/4AcOGDeP888+noqKi0euuWbOGiRMnMnLkSC655BJOnDgBwJNPPsnQoUMZ\nOXIkc+bMAeC///0vo0ePZvTo0YwZM4aSkpIIfRodh6gL4xULRBDaxm/f28im/cVhPefQHsn85tvD\nQu5/8MEH2bBhA2vWrAFg0aJFrFq1ig0bNtjhqs899xxdunShoqKC008/ncsuu4z09PSA82zfvp1X\nX32Vv//971x55ZW8+eabXHfddSGv+93vfpc///nPTJ06lV//+tf89re/5fHHH+fBBx9k9+7dxMbG\n2u6xRx55hKeeeopJkyZRWlqKx+Np68fS4Yk6CwQREEE4JRg/fnzAXIcnn3ySUaNGMXHiRPbu3cv2\n7dsbHJOTk8Po0aMBGDduHHl5eSHPX1RURGFhIVOnTgXg+uuvZ/HixQCMHDmSa6+9lpdeeomYGKMf\nPmnSJO666y6efPJJCgsL7e2nMqf+HZrUjYGIgghCW2jMUjiZJCQk2K8XLVrEp59+ytdff018fDxn\nn3120LkQsbGx9mun09mkCysU77//PosXL+a9997jgQceYP369cydO5dZs2bxwQcfMGnSJBYsWMDg\nwYNbdf7OQtRZICIfgtD5SEpKanRMoaioiLS0NOLj49myZQtLly5t8zVTUlJIS0tjyZIlALz44otM\nnTqV2tpa9u7dy7Rp03jooYcoKiqitLSUnTt3MmLECH72s59x+umns2XLljbXoaMjFoggCB2e9PR0\nJk2axPDhw5k5cyazZs0K2D9jxgyefvpphgwZwqBBg5g4cWJYrvvCCy9wyy23UF5eTr9+/Zg3bx41\nNTVcd911FBUVobXmjjvuIDU1lV/96lcsXLgQh8PBsGHDmDlzZljq0JFRnS0qKTc3V7dmQakXl+bz\nq7c3sPwX08lMim36AEEQbDZv3syQIUPauxpCMwn2vJRSK7XWueG8TtS4sKzJ/Z1NMAVBEDoqERUQ\npdQMpdRWpdQOpdTcIPsfU0qtMf+2KaWaP120hcgYiCAIQniJ2BiIUsoJPAWcBxQAy5VS72qtN1ll\ntNY/9Sv/Y2BM5Opj/JcxEEEQhPAQSQtkPLBDa71La+0FXgMuaqT81cCrkaqMw5oGIvohCIIQFiIp\nID2BvX7vC8xtDVBK9QFygM8jVRlljoKIBSIIghAeOsog+hxgvta6JthOpdTNSqkVSqkVR44cadUF\nlFgggiAIYSWSArIP6OX3PtvcFow5NOK+0lo/o7XO1VrnZmZmtqoyyk6m2KrDBUHoZCQmJgKwf/9+\nLr/88qBlzj77bJqaFvD4449TXl5uv29pevhQnApp4yMpIMuBgUqpHKWUG0Mk3q1fSCk1GEgDvo5g\nXerGQCQOSxCiih49ejB//vxWH19fQCQ9fB0RExCttQ+4HVgAbAbe0FpvVErdr5Sa7Vd0DvCajvAE\nDVkPRBA6L3PnzuWpp56y31u999LSUs4991zGjh3LiBEjeOeddxocm5eXx/DhwwGoqKhgzpw5DBky\nhEsuuSQgF9att95Kbm4uw4YN4ze/+Q1gJGjcv38/06ZNY9q0aUBdeniARx99lOHDhzN8+HAef/xx\n+3rRkjY+oqlMtNYfAB/U2/breu/vi2QdLKxBdJlIKAht5MO5cHB9eM/ZbQTMfDDk7quuuoo777yT\n2267DYA33niDBQsW4PF4eOutt0hOTubo0aNMnDiR2bNnh1wX/K9//Svx8fFs3ryZdevWMXbsWHvf\nAw88QJcuXaipqeHcc89l3bp13HHHHTz66KMsXLiQjIyMgHOtXLmSefPmsWzZMrTWTJgwgalTp5KW\nlhY1aeM7yiB6xBELRBA6L2PGjOHw4cPs37+ftWvXkpaWRq9evdBac++99zJy5EimT5/Ovn37OHTo\nUMjzLF682G7IR44cyciRI+19b7zxBmPHjmXMmDFs3LiRTZs2hToNAF988QWXXHIJCQkJJCYmcuml\nl9qJF6MlbXwUJVO0k5m0az0EodPTiKUQSa644grmz5/PwYMHueqqqwB4+eWXOXLkCCtXrsTlctG3\nb9+gadybYvfu3TzyyCMsX76ctLQ0brjhhladxyJa0sZHjQUiEwkFoXNz1VVX8dprrzF//nyuuOIK\nwOi9d+3aFZfLxcKFC8nPz2/0HFOmTOGVV14BYMOGDaxbtw6A4uJiEhISSElJ4dChQ3z44Yf2MaFS\nyU+ePJm3336b8vJyysrKeOutt5g8eXKL76szp42PHgvEnkjYzhURBKFVDBs2jJKSEnr27En37t0B\nuPbaa/n2t7/NiBEjyM3NbbInfuutt3LjjTcyZMgQhgwZwrhx4wAYNWoUY8aMYfDgwfTq1YtJkybZ\nx9x8883MmDGDHj16sHDhQnv72LFjueGGGxg/fjwAN910E2PGjGnUXRWKzpo2PmrSuX+4/gC3vryK\nj+6czOBuyRGomSCcukg6986FpHMPM/Ygem371kMQBOFUIYoExErn3rksLkEQhI5K9AiI+b+TeewE\nocPQ2dzd0crJfE5RIyAOyYUlCK3G4/Fw7NgxEZEOjtaaY8eOnbTJhdEThSULSglCq8nOzqagoIDW\nZsMWTh4ej4fs7OyTcq2oERBZ0lYQWo/L5SInJ6e9qyF0MKLGhYVYIIIgCGElagRExkAEQRDCS9QI\nSF0UliiIIAhCOIgaAZExEEEQhPASNQJSNxNdJEQQBCEcRJ2AiHwIgiCEh+gREDsbr0iIIAhCOIga\nAXHIelKCIAhhJWoExEqmKEMggiAI4SFqBMRekVBMEEEQhLAQNQJSlwurfeshCIJwqhBFAmLNRBcF\nEQRBCAfRIyDmf9EPQRCE8BA1AuKQFQkFQRDCStQIiKyJLgiCEF6iRkAkF5YgCEJ4iRoBsZCZ6IIg\nCOEhagRE1gMRBEEILxEVEKXUDKXUVqXUDqXU3BBlrlRKbVJKbVRKvRK5uhj/JYxXEAQhPERsTXSl\nlBN4CjgPKACWK6Xe1Vpv8iszEPg5MElrfUIp1TVS9ZExEEEQhPASSQtkPLBDa71La+0FXgMuqlfm\nB8BTWusTAFrrw5GqjJI10QVBEMJKJAWkJ7DX732Buc2f04DTlFJfKqWWKqVmRKoydi4s0Q9BEISw\nEDEXVguuPxA4G8gGFiulRmitC/0LKaVuBm4G6N27dysvJeuBCIIghJNIWiD7gF5+77PNbf4UAO9q\nrau11ruBbRiCEoDW+hmtda7WOjczM7NVlbFcWIIgCEJ4iKSALAcGKqVylFJuYA7wbr0yb2NYHyil\nMjBcWrsiURmHEgtEEAQhnERMQLTWPuB2YAGwGXhDa71RKXW/Umq2WWwBcEwptQlYCNyttT4WifpI\nMkVBEITwEtExEK31B8AH9bb92u+1Bu4y/yKKTCQUBEEIL1EzE13CeAVBEMJL1AmIyIcgCEJ4iCIB\nkRUJBUEQwknUCIhMJBQEQQgvUSMgyp5I2M4VEQRBOEWIGgGxLRAZBREEQQgLUSMg2FFY7VsNQRCE\nU4WoERCHkkEQQRCEcBI1AmLNRBcLRBAEITxEjYA4JIxXEAQhrESNgCgZAxEEQQgrUSQgsqStIAhC\nOIkiATH+iwtLEAQhPESNgEg2XkEQhPASNQJSF4UlCiIIghAOokZAHDIGIgiCEFaiRkBkPRBBEITw\nEnUCIvohCIIQHqJHQJCJhIIgCOEkagRE1gMRBEEIL1EjINZEQpmJLgiCEB6iRkBkPRBBEITwEjUC\nIhaIIAhCeIkaAQEzEksGQQRBEMJCdAkIYoEIgiCEi6gSEIdSMgYiCIIQJqJKQJQSC0QQBCFcRJmA\nKBkCEQRBCBPRJSDITHRBEIRwEVUCYoyBCIIgCOEgogKilJqhlNqqlNqhlJobZP8NSqkjSqk15t9N\nka0P1MogiCAIQliIidSJlVJO4CngPKAAWK6Ueldrvale0de11rdHqh7+iAUiCIIQPiJpgYwHdmit\nd2mtvcBrwEURvF6TGPNAREIEQRDCQbMERCnVXykVa74+Wyl1h1IqtYnDegJ7/d4XmNvqc5lSap1S\nar5SqleI69+slFqhlFpx5MiR5lQ5OEomoguCIISL5logbwI1SqkBwDNAL+CVMFz/PaCv1nok8Anw\nQrBCWutntNa5WuvczMzMVl8sxqGokTEQQRCEsNBcAanVWvuAS4A/a63vBro3ccw+DKGxyDa32Wit\nj2mtq8y3zwLjmlmfVuFyOqiuqY3kJQRBEKKG5gpItVLqauB64D/mNlcTxywHBiqlcpRSbmAO8K5/\nAaWUvwjNBjY3sz6twuV04BUBEQRBCAvNjcK6EbgFeEBrvVsplQO82NgBWmufUup2YAHgBJ7TWm9U\nSt0PrNBavwvcoZSaDfiA48ANrbyPZuGOcVBdIy4sQRCEcNAsATFDb+8AUEqlAUla64eacdwHwAf1\ntv3a7/XPgZ+3pMJtweVUVPvEAhEEQQgHzY3CWqSUSlZKdQFWAX9XSj0a2aqFH8MCEQERBEEIB80d\nA0nRWhcDlwL/1FpPAKZHrlqRQcZABEEQwkdzBSTGHPC+krpB9E6HRGEJgiCEj+YKyP0Yg+E7tdbL\nlVL9gO2Rq1ZkcDtlEF0QBCFcNHcQ/V/Av/ze7wIui1SlIoXLqSiuFAtEEAQhHDR3ED1bKfWWUuqw\n+femUio70pULNy6nA69EYQmCIISF5rqw5mFMAuxh/r1nbutUuCQKSxAEIWw0V0AytdbztNY+8+95\noPVJqdoJGQMRBEEIH80VkGNKqeuUUk7z7zrgWCQrFglcTiUWiCAIQphoroB8DyOE9yBwALicCKcd\niQQSxisIghA+miUgWut8rfVsrXWm1rqr1vpiOmUUlgyiC4IghIu2rEh4V9hqcZKQZIqCIAjhoy0C\nosJWi5OEjIEIgiCEj7YISKfryrucDny1mlpZlVAQBKHNNDoTXSlVQnChUEBcRGoUQVxOQy+ra2uJ\ndTjbuTaCIAidm0YFRGuddLIqcjJwWwJSo4lt7lJagiAIQlDa4sLqdLicxrCNLColCILQdqJKQDwu\nw21VXl3TzjURBEHo/ESVgGQlewA4WFTRzjURBEHo/ESVgPRINcb99xdWtnNNBEEQOj9RJSDdUw0L\nZH+hWCCCIAhtJaoEJNnjIik2hgNFYoEIgiC0lagSEIAuiW5OlHvbuxqCIAidnqgTkDiXk3KvRGEJ\ngiC0legTELeTSgnjFQRBaDNRJyDxbrFABEEQwkHUCUicK0YERBAEIQxEnYDEu51UeH3tXQ1BEIRO\nT9QJSJzLSYWMgQiCILSZiAqIUmqGUmqrUmqHUmpuI+UuU0pppVRuJOsDxiC6uLAEQRDaTsQERCnl\nBJ4CZgJDgauVUkODlEsCfgIsi1Rd/DFcWCIggiAIbSWSFsh4YIfWepfW2gu8BlwUpNzvgIeAkzI9\nPN7txFerZWlbQRCENhJJAekJ7PV7X2Bus1FKjQV6aa3fb+xESqmblVIrlFIrjhw50qZK2SndxQoR\nBEFoE+02iK6UcgCPAv+vqbJa62e01rla69zMzMw2XTfebSxFKG4sQRCEthFJAdkH9PJ7n21us0gC\nhgOLlFJ5wETg3UgPpCfEGhZImYTyCoIgtIlICshyYKBSKkcp5QbmAO9aO7XWRVrrDK11X611X2Ap\nMFtrvSKCdSIt3g3A8TJJqCgIgtAWIiYgWmsfcDuwANgMvKG13qiUul8pNTtS122K9ERDQI6VVrVX\nFQRBEE4JYiJ5cq31B8AH9bb9OkTZsyNZF4vMxFgAjpSKBSIIgtAWom4melqCWCCCIAjhIOoExOV0\nkBbv4qgIiCAIQpuIOgEBSE+M5Zi4sARBENpEVApIsieG4srq9q6GIAhCpyYqBSTJ46Kk0seDH27h\n4Y+2tHd1BEEQOiURjcLqqCR5YvjvtiOsKygC4J4Zg9u5RoIgCJ2PqLVABEEQhLYRlQKS7IlKw0sQ\nBCGsRKWAWBl5AdwxUfkRCIIgtJmobD295logsTEOvL5aWRtEEAShFUSlgFSaa6JnmGlNyqokM68g\nCK1j84Fidhwuae9qtAtRKSBDuiUDMK5PGgClIiCCEHYOl1SycX9Re1cj4sx8YgnTH13c3tVoF6JS\nQK7IzWbBnVM4f1gWAGVVsrhUZ2fVnhMUlcvk0I7EuY/8l1lPftHe1RAiSFQKiFKKQd2SSIg1orFk\ncanOTU2t5tK/fMWNz3/T3lUR/CgRy/6UJyoFxCLRFJDv/uMbdh0pbefaCK2lymdYkBv2FbdzTQQh\nuohqAemVFg8YYyCfbDrEzCeW8NLS/HauldBSqqqNKDpHVH+bBeHkE9U/uW4pHubdcDpgLHG7+UAx\nv3x7A1rrdq6Z0BIqTQvEqVQ710QQoouoFhCAKadlAvC3xbvsbUUVdYOxD7y/iT8tkISLHRnLAnE6\nREAE4WQS9QISrNE5UFRpv/77kt08tXBn0GMPl1TyxKfbxWJpZ6p8IiAdGfl9nLpEvYAE46CfgDTG\nT19fw2OfbpPB23bGGkR3yiBIh6Q2TPrx1Y6jPPnZ9vCcTAgL8osLwoFmCoi1qmGwduuVZXs4XNy8\n8whto9J2YbX+HLOeXMKN81oWBvzljqO8smxP6y8aJfhqw5Mq6Jpnl/HoJ9vCci4hPIiA1EMpOFhU\nAdT1bEPhNV0n9X8f+woruPet9dzy0sqI1FEIxHpOMW2wQDbuL2bh1iMtOubaZ5dx71vrW33NaKEm\nXCZIB6elrrpjpVXUdvLPRgTEj5un9KNHShz5x8s5XFLJ/e9tarS85XuvrCc0VWaurWNlsu76yUDC\neDs2vk7eSDaXltznsdIqxv3+U/73k60RrFHkkZ+cH/dcMIicjATeWbOf8Q98xst+7gnL2li87Qj3\nvbsRqOv5VngDBcR2qUhY6UnBHkQPw+cdLb3lk0lNTXg/0470jPytDquNaA5W53LBxkNhr9PJRATE\nj5hGnOhf7TxKTa3mphdW8PxXeazdW8hRcwzEyu5rUW6mRnE0EhX0ze7j/GvF3mbVa8fhEj5Yf6BZ\nZaORukH0tgvIsdKqFh/TkoYjGgm3BdKRll/wv7WWfA9OlcA0ERDgxe+P5+czjXXRJw3ICFrmhnnL\neW/tfpLjjOVwL3rqS3tfZb0vjpXdt7Ee8ZV/+5q7569rlt90+qOL+dHLq5os11bG/e4TfjZ/XcSv\nE24si68tYyAWh4pbLiDlkkutUcJtMXQkAfGvi7cF9epIVlRbEAEBJg/M5IdT+wPGOEj3FE/QcsWV\n1cQE6eXe8erqgIbXyu5bv0e8cOthvtl9PGCbZcV0BI6VeXm9mVaRxb9W7OWOV1e3quceLiwLpDGL\nrzH8Rbw1iTXLvJLNuTFqwtzdrg6zS6wt+AtBSyyQlohNR0YEpB5Oh6JLgjvovv2FlRwqCR6a69/w\nWo2Q06E4XFzJi0vz8dXUcuO85Vz5t68DjtvZgiSOHTFi4/FPt/Pu2v0szzvedOEIUTeRsG3HQ0N3\nZHOoEAukUcI9BtKRLBCf3721RBSqWvE964iIgARhqpne5O3bJvHu7ZPs7U//d2eAW6pbckNL5cev\nruYe0xqp8tXwxoq9/OrtDTxRbwJUuilS+cfKgtZh6a5jfLH9aMC2jthrsRrcqnYcB6hqY9CCdTzU\nucNagqwn0zjhmgdi0ZHGnPzvrSX1sn4vnT3MJqICopSaoZTaqpTaoZSaG2T/LUqp9UqpNUqpL5RS\nQyNZn+by/84fxOf/byqje6UyMjuVvAdn0TXJWP52YFYSkwakA9A1OTbguFeW7eG9tfvt99sOlfLI\nx8bEp73HywPKxjiNr06p2fj89PU1LNh40N4/55mlXPePZQHHVLWicWsurbVurDa7PX/UVhh1aw00\n/zDspub+BKMzriezIu84e46VN10wDJzKYyC+Vrqw2rPDFU4iJiBKKSfwFDATGApcHUQgXtFaj9Ba\njwYeBh6NVH1agtOh6JeZGLAtxRw8z0qOJS3esB7S67m6GptUFqpnW17lo7ZW89bqffzwxcYnHram\ncWsu/tZNSyZEKVNBiiqq6Tv3ff6zbn8TR4QfS1hb27D4C3NrRLq8E1oglz/9NVP+tPCkXKs1UVhD\nf/0R330ueGaAjjQGEiAgLXFhRfC3fDKJpAUyHtihtd6ltfYCrwEX+RfQWvsnkUoAOs43ox6p8YaA\ndEv22GMklqg0h/J6Pk+rt1LmraG4MvRSrP6NeWvcK83Fv+GsaIF/1jLBdx4xXHFPfBo6V5GvprZV\nYwxNYf0YWxsu6v/5158U2hxCWSBaa/69qqBD9Zjbg9ZYIOXeGhZvC54ZoCN9nr6a1o2fRdKbcDKJ\npID0BPxDegrMbQEopW5TSu3EsEDuiGB92oS1/G3XZA/dU+IAcLTA5+6fIr7KV8MJc/3ucq/Pfh2M\n4sq6ximSvZaqmrpzN1af+lgfgfVDqm3Eern22WUM/tVHratgI1juAF8rG5Zv/blu3e6WNALWvZeH\niMJ6d+1+7npjLX/7b/BsztFCuOeBdKSxQP97C/U9CIb1ne2wPeZm0u6D6Frrp7TW/YGfAb8MVkYp\ndbNSaoVSasWRIy3LVxQucvukAdAl3sV3zujD5eOyuXRstr3//KFZ9AgR/guBE9RmPr7Efl1WVUNh\nuRHK644xHod/j83/uBb5TbWG52bAv2+GL5+AHZ9ByaGQM5j8e0QnWpCCxRJR60fdmPdrmRnCHKoH\nuTzveLOjuTYfKGZlvlHWFpAgDVVJZTV/WrCl2cLQkp6hNWgf6txHSoxn19FS2oT6/COVdr2mDYPo\nh4orOVhUGdA5qO5A4wf+UVj1M1I0xqniwoqJ4Ln3Ab383meb20LxGvDXYDu01s8AzwDk5ua2i2j/\n6OwB9M9MZPJpmSTGxvDIFaMAeO6GXDITPYzITmH+ygL+519rgx5fcKLCfr3raF3kVUW1j0Kzxx/n\ncgJQ6md1+E9sC/WlKyz3cv5ji7n7gkFckWt+5N4ycCfC7iWw7vW6wvEZkDUMuo0w/mcNg4xBAeJU\n2AILxGpzrN5Xcx5O/rEyBnRNarD9iqeNEOe8B2c1eY6ZTyyxy1oNeMGJCnYcLgk49+vL9/LUwp3E\nxji549yBTZ63JS4sh0NBrQ7pWrR89a62pAnGEKLfv7+JP1wywraE20IwF+X+wgrOfPBzHr1yVEDH\nKBy0xWCY8IfPANj42wvsbR1rDKTu5loSTHGqDKJHUkCWAwOVUjkYwjEHuMa/gFJqoNbacprPAjps\nsn+HQzFzRPcG288ZnGW/Pm9oFuP6pDGxXxfKqmp4/qu8Js9bVlXDCdMCsQeif3yWvX/P8TqxOVrq\n5Zvdxxmf0wUweoxKKbYfLuVwSRV3z19XJyCxiXDdfON1+XE4tNH8W2/8X/4s+Mw5LcpJdmp/nnSl\ns7m2D65dxdB1CiT3qPPThMD6AZWZs+8bc2FZ7DhcGtDI3zjvG8b0TrPfl1RWk+Rp/viS/49x5hNL\n2P7AhfZ7K+Bh9Z4TzTpXc8eZFm49bI9jhbJArF5zsMmnLeF/P97KO2v2M7FfOt1TPBwqruSq03u3\n+nzBesp5Zjj5a8v3cunYbJbnHeflpfk8euXoZk3QLPf6eGv1Pq4Z39sOrLBoaRhvMFek/zOO1BiI\n11fL0dIqeqTGNfuYVlsg1ZbF3nHEsDVETEC01j6l1O3AAsAJPKe13qiUuh9YobV+F7hdKTUdqAZO\nANdHqj4ng5Q4F2/eeqb9vrqmNiAhYzD+u+1Ig0HGdQVF9uuHPqrL1mlFab13+1lc9czXlHtr+PSu\nKQELYPlqahvm9IrvAjmTjT+L2ho4vgsOGoJSvGMlYx3rme38Gr5+Db4G4tIga3idpZI1DDKHgDve\nPo3ViFrpW/KPlfPRhoPMGN4t5D1bFk5ldQ17j5ezcOuRgFTqm/YXM6FfemMfWwD+k7Lq904tQfO3\n+vyp/wNurqvrxnnL665frze59WAJ33t+OVMHGfOJGsux5qupZf7KAi4flx2ynNdPiG4wr9scAdl7\nvJzJDy/k3dsnMTI71d4ezFcfG2NYv9ZnaVmDv71oeLOCRR78cAv//DqfHqlxTBvUNWBfSwfR66cG\ngkCrKVJjID97cx1vrd7H1t/PsD+PpvB3m7ZkPlBbAz86CpG0QNBafwB8UG/br/1e/ySS129vQvWi\nzx+aRXpiLP9eVUCVr5YvdgROGDzktxDV8SD+8yc+2243Ags2HsLt1/AcLfXSrZGxGBuHEzIGGn/D\nL2X8J+8DkEwZvx4Pl2cXwaHwOwDyAAAgAElEQVQNhrWy6kWoNhtg5YAu/U1BGc5ZNVWsU9mUVtaJ\nyi0vrWTnHy4MSOUSkLXUbABeWprP79/f3KBqJ8qrWbDxIJMHZhDvbvor2pg7wBIEXwi3h3XsBcOy\n2LCvuFWuhfqi87fFO9lXWMHHZqZVtzN0D/615Xv55dsbKPfW8L2zcoKWaa0rbJEZxfTa8r0BAhKs\np2wJbWV1bcCcIKOha1pArHGe4opqNh8oDmjwW9pIBhNxy8KFyFkgn2wynlelt7b5AuJXl/Lqlruw\nOns0VkQFJNqZkNOFhVsOU1RRzUE/UUjyuPjjpSN4fXlw66R+epOuSbEcLqkbC/EPO122+zgb9tVZ\nLK8t38PU0zJ5edkeHrpsZKMZaoOZz8UksMmVA+P9puzU1uI9uouKgrXUHtjAwW0rGbhvDTGb3uYv\nTsAJpaXxbHL3Ykttb7bo3hzfkswabw/e2lhIdlo8P51+mn06y2oJlQfsm93Hee7L3VyZm83Dl48K\nWX+jarpRq8FySYVyrVk/4PE56Ww/XNqqMOP6Y1N2YIG5vbGG37LcDjayeqU1aBzTiBC1hIogDZ11\n3xXVNYETK5vZwFluOl+NtsenLFqayiTYMyjxGxd8a9U+HlmwlU/vmtrAaqup1byyLJ+rTu9tB6U0\nFzuqrtpHSjNE07qeRWsG0Tv7YLoISASZNrgr0wZ35Qf/XMHBTXUNhDvG+KY6lLIbtm7JHrsR+c86\nI3V7ZlIsR0qq6JbiCRCQTfuL6ZHiYeqgTF79JjD54eOfbufJz7ZTq+GWqf2CDlZb3PvWel79Zm/A\nACVgj8nYOBzc/lEhH2/yMGvEt3n/UC4ZibFUVBYySO1lsGMvI2L20p98LnZ+QbKqgDf+wXnAwNos\nDnj6UeOYxAUOH5t1H6qqB3KgqIJ31wSPqdhjztrfdqjpPGEV1TVBZwAfKakiyRNjN0ah3CjWDzg2\nxkFsjLPJMZAKbw1P1wvLrX+MpdnNcbV4zEauscbHGkMIV8/b34VVU6txOpQtFBXVNQH7m2uRWR2V\nYIkTm2WB+KqgqhS8JdQeOMw4tZVEVUkCFSSqCpLXbODHzm0kqEpSd1cxq7aCmrffICY+GdwJ4IoH\ndyIr9lWxbOURuuwfwKyx/Y197kTD7epOAFcCxATPdWcJf0vCcatb6cI6UWZ0Ak+UV7P3eDm9usSH\nLFtdU0uMQzUYW+oIiICcRCb268LSXcftHulbP5rEt//PmIOQkeRu0Avtl5HAkZIqspI9QJ2VUVrl\n47SsREb3SrUF5MFLRzD338ZMeOs7vXF/MQO6JnGwqJJETwyJ9SJ4rGOP+IlTWryLPcfLOVpaRa3W\ndE0y3GEfm+a9dY6jpVVAHKv0aayqOQ3s344mWx1lmGMPA9nDEEc+w7x5JCxdyt/cRsW8X8Sx/Yve\n3O7tyWZnbzbX9mar7k0Jxo9o99FS8z6CNzz+glHurQnayJ3+wKeM6JnClNOM9PzBGvPnv9xNN3NO\nj8flxONyBO0RFpZ7SYiNweV08Phn2/jbf3cF7K/fY7YaIqtejTX8Vg+6McvHa/bgdx+pG8exAiga\nQ9llA7f7i1VFdQ2JsTF1q2tW1wTsD9lDrq0Fb6nxV1VKTuVWznDkk5x3kIsdO0hUFSRSQYKq5PAb\nL3JkTSKZbq8pEsYxVJWAt8R4XVtnVfcG3oytd73V8P9cUKVdlBBHhYrFuWsn+MrR3lKUNuo5AZjg\nBtabf8FwuExh8ftzJfBnXU6xy02XT96BtDRTlBqW838fU1JBCqWU4wlwPTfFrqNlDO6WxJaDJXy0\n4SA/mNIvaLmiimpG/fZj7r1wMDdP6d/s858sREBOAvdfNIycjATS4t0BAjIiO8UuE2yg0ooGsZI2\nju/bhW/MeRLdUjycllVnXUzol87PZgzmoY+22Ns2HSjmotE9mfjHzxiUlcSCn06x9/mnlX9xab79\nekR2KqvzT5D7+08Bw3321+vG2fubntOgKNCZFNRksoBxhrBUw2OXnMa8tz9ksGMPV2YX4juwgQud\ny7hGfW4fWaAz2Fzbmy0nerPZ0Rtf9VBjsN8R6I+e/HDdMeVeX4NGzvLhr99XRHKc8RUvr6rhqx1H\nOVBUyWXjsnn+y93c57dkscflIDE2JmDipsXo+z9h8sAMXvz+BAqOVzTYX7/xtxp2q+H2NuLCsY4N\nNnBsYbmwnvx8R922Gm1bss1Ca6OX7y1FFeYxROWTQAW+rYCjim479/J95zYSqiuJX/QZD8TsIEFV\nkP3hcxRXl+LylRGnK6CqBF1ViqoODEr4MfBjN7ABZvh18Gu0ogwPZdvjIDPTiA50J0JCpvHfeh+b\nCLHJ4E5kZzHct2APZdpDKXGUaQ83TBvOQwv34fNrsv5zzVkM75nCD19YzqLN+1h0Ry4frd7NK19s\n4oZxmVw3LsMIZ/f/q/Z/X26IWXU5eMvIpJCeqoKEgjzIrzD26catiknAWnPI0bvXSfUDCbjikvzE\nx8/6MQVIuxI4/9g+hvbpzuLj5WTm74Stp/mVS7SPP1xofHdeWbZHBCRa6Z4Sx70XDuGphUYD4O8T\n/+f3xvPhhgO228af3qZZGx/r5I0fnsHQHsmc9dDnFJZX0zXJQ2ZSXTctNc5FRmKgaX6gsNJ23Ww9\nVGJv37CvKCCt/D++2G2/Hto9OSCFxOGSKh5ZUBcJVnCidQn4PtpWxHr6s1EP4I18DVwCaLpxnCGO\nPQxRexjs2MNgtYdpjjXExNQaRtcf7oSuQyBrGEXJg3klP4nK4jjAyFVW7q2hqrqWayf0JsaheOHr\nfPYV1jXyX+44BhgWyDXPGskpz+ifHiAeAJ4YJ5mJsew6EtgwWr3xJduPUlrlMy2vQOq7sOonpgxp\ngWhNVWUFiZQTU3EUigrAV8XRomJSXbXE6Gq27z9KbN4aLnD4iKUat6omlmpqv94Nqhp8XiMc21tm\n9uxL7B7+rMLjnB1bSJeNVbChAmoNcTwPOM/66vzb+DcOGGf2YarXu7nA6aFMe4gpzWDDsVrKtIcp\nw0cQE5dEoS+Wf646RmxCMrecNwpik3l+xRE+3FZGmfJQqj2U6ThK8VBBLKCIcznZfPuM4J9DPQ7u\nOMqS2sBEonsqPAHiAXXjIh9vPgy4KHYkU+TOYocu5nDKQMg5jZbwnd9/ytHSKl64eLyRkVtrqPEG\nFaCH3llJweGjXDAwiZU7CoinigRVyUCPg/P6JdrC5C0vofDYXjJjfSi/43/q8MFeuMAB7DT/gjAQ\n2B7rpLLMA/+bAtN/A6PmtOi+IokIyEnEakj8o3KmnJbJlNMyufQvXwaUfeCS4bbQnCjz2nM/kj0u\nCsur6ZbiISOxTkCS41wBUV+DspJYV1DIiPsW2Nv+s24/3xrZg+KK0BMFpwzMaODj93et1c8q3FwW\nbDxE3/R4iit9fpFlioOkc7A2nYWMsd1gsXgZoPYxxLGH34zRJBVuha0fklL+ErcCt3rggO7Cltpe\npH61hOk1TvrpSZw94Qxe+Dq/ydDpTzbsw0OV0SBTTazykVq+m2HOw+wu3s2zL+3j8lGZpLo15UXF\nXOxYg1tVs+z11Zx9/BjjnKXEKp99vHtPNXueS2bT3iMMSnfxnYpyZrtK7ca+21oF25RhAdRUGf/N\n17cBt3mAPcBjRv3818QcCDwfzGX/Wd1LrZyoBr35JEriE1le1J0eqZmcMaSPuS+JRXnlvLa2kFLi\nuPfi0+nTvSvnPbWKUjyU42FsTlfbQj0/LYuPDxjuy5+mn8ZPpg9kxaZDPPbNCiiCiVmTGN0rldUb\nV7NM7w85kzTYXBivr5Y/friZH509wO4MfbnjKNc+u6xB2WDuoZJ6OeRKK312hylYav/Xl++hT3oC\nE0OEiFuHVHh9+Gpq+WrnMSYPzEDFxxqh8H4scSo21BazvSiJLTUl/PXasdwzfx0pPhfnXXyOXe7O\nl1fywc6DvH7zRDs0/YlPt/GXzzaz8I5c5r76NTnJivtn9g1qFe0oOMSCNTvpHlfDpf3TILlBNqh2\nRQTkJGILSJDoEGvg7pezhnDWwAwGd0umwmu4XX58Tt0MamuwsluyB4/LGbDdctcADOiayPv11lG/\n/ZXVPLN4F+cMrovTf/H74/nOP4ysp3+9dixnDshgQNdEdhyuG8De5zeLvsxbQ7+MhJDzKhpj8sBM\n/r2qoMlyVbipyBjO/CM5zP8adv/xYXw1tfz6lc/Zt2U5g9UeBjv2MkTtIWvjszwWUw3r/g82ulmU\nmM2Jr5182+3Fjc8UCFMoqMaND9enNVxfP9L5P5ALfN8N7DD/gHTgcasB3wXngh3VWqVjqMKFFxfe\nfBeDdAxVh90keDxUKIVXuygljhh3Cl17ZFLrjEXFxOLFzb6SGvp1T+ez7YV8nV9qn+f7Uwfz+KJ8\nuqWn8OuLxnDTKxs4WgFejGtV4aJKu0hKSKAKNwfKapk5oidPXTu2wef48ZJd/H7PZq7u2YuhZw5h\n74lyhvVI5psTW/mo1ugkbHcPZvV+H/tN2YpzOSnw6yRYY18AX+48yrdHdecH/1xhb7v4qS9Zcs+0\nJgeeg01G3HqwhHlf5tE/M5HrJvYB4Fdvbwh6/KGSKlxOFTDPp7TKFxBJWFLls12CXr/cbgUnylm6\n6zg/e9MYFFn7m/MDXMab9hczMCvRDn4o99bw+/c38/xXeQ3m0FhYyydvOWhY9ucNzeKaCb2Z91Ue\nWms+2XSIcX3S7HDgvGNltoBs2l9M366p9OzeE29CT7bUAD0NN/He4+Xc9cYanr5uHOmJsaxesZc/\nrVgHJZA6OJcpvTM7VKPdkepyyjPWnG09tk9ag333XjiEe99az3UT+9jCEOd28vicMQHlrJ9hVpDF\nrJL9LJD0xOCRJusKiuyJipeM6cmZ/ev6uwOzDLdQny7xAQJSfwC6tcEgl47tycvL8psuCFwwrBsL\ntxxmy8ESdh4p4501+3h1kxcYxWJG2dbK4xcN4en5C7h7dDXnph0hOW8te/Yex0ui3ShX1Rr/u6Qk\nsb+0lp7pqaw/WGnsw4WXGH58/ggOlNby1y8L8OKid2YqD155OssLyrj77a14tYtxA7qxaEexfUyw\n5YBiHIoxXVJZnlc38/3SHj1Zt6eIHYdL+Z/zTyPvWDnz1xXw/tln8dHRPP61q05U+7oH80HtFmZ1\n7w4DxrLY6+W6M/vw0tL8gOdQ52lz8P76Ayz93ScUV1az6f4ZuJwOamo1b66qi3J78vPt/OOL3fzf\nNWMorKgmNsZBla+Wg0WVxLnrOiI90+ICnr0/Fd6agGhAi8kPL7RzxVlcNLoH76ypS+1fWuWjuqY2\nwH1rhaPv9uuMhJr1nn+sjITYmIA0OyWVvoAAitJKn+3W8o+IOuuhwLT1+05U2AKSf6yMC58MDDsu\n99bY39O/L9nNY1eOahAuXN+iinE6yEyKxeszJobePX8d43O6cHpf43NZvafQnvxZ5vXZ3oLUeBd5\nR+sE+5nFu1ied4K3Vu/jpsn9AiIiv/f8CnY8MDPo59NeiICcRM4dksXKX04nPbF+iInhyvriZ+cE\nOSoQ68eeltBw0D3JU/c4609AvHFSX+Z9mRew7cHLRgTME7HyLCU3MfN49qiePPbptpD7Z43szvtm\nKHJ6gtseeB/dK7XJRZ9S4w0XXb+MBC67ZgzTH13MyvzjbD1YErT8nf/aDPSmetg4GN6NLsB35xqT\nIpUKjECa1bM76wuKyHC6WVVTGHCemwdPwVdSxaIlhvskvyoOeo5l76EC8nQx2WlxbC52UkLdIPu8\nG08PmJFu1N9tLxJmseVgid0oP/LxNs4baqS/mfXkF9Rn7V6jXusLijhWWoXXV0tqvKvJkGDrMz5e\n5iUr2cM/v85j8wFjtYTa2rqVL9fvK6KovJrstDh2Hinjjx9u4e4LBtnn6ZoUG1pAqmtCpmVZkX+C\nSQPSWbu3iMTYGM4dkhUgIDW1mq0HSxjesy5wxHKl5vkJSDDXk9OhKCyvpmdqXICAlFb57Hk0YEw2\nLKmqy3Ltq6nll0EsmsMllWxaWcwlY3oGjJfZ9+mtsS2d99buZ2K/LpzZP4O8Y2UM7JpIdlp80Dk5\nXc1O3d3miqSr8k8wzhTW11fs5abJOQzomkRZVY39W02Nc1NYYTzzcb/7xH6O1vWPl9Xdb2q8q9Gs\nBu1Bx6pNFBBMPFrCk1eP4TsT+zDQnN/x0Z2T7dxZVoity6n40dkDmDWiO/+4PpfRvVK5dWpgBEdi\nbEyD2bbWrO/YJiZg3XHuAO46r26A8oFLhgfsP29IXX6wzKRYPrpzMp/8dErIsNOxvetcBB/cMZlf\nzhrCrJHd6ZeRSLzbyZaDJQGf26L/ObvBOaYP6dpgW0a9zzo7NY70RDer9xY2KBsb47TzZgH2YLkl\nxDkZCQ0a1tQ4F8N7Jgdui3cFhEUDDdZ7qR/s4M/S3cag/57j5bbAtCSB4sGiSpbtOtbAgrRS9B8p\nqaKwwktqvNu2JD/fctivbqG/n8HmqjxyxSh7rZw4VwzLfzGdRXefbScGBbj3wsGAkTa/uLKaCm8N\nG/cX2Usc7PZb1jnYV6RfRgJgfLajehnfFbfTwZGSKltwwRCU4grTAvHWsGpPIa8t39vgfM8s3sX/\n/GstLy3NZ39hw7GV+kkRf/HWBmb/3xfcOG85Zz20kG2HSmwXlj9dkwI/O1+ttj8zreGjDcaKo+Ve\nn/1bTY13caK8Gq11QITjvsJy7n9vU8B4ZJf40N+b9kIskE5G/8xEfndxXYM9uFtdA5YW7+bSsT25\nZnxvhvZItv3i5/o16BbWjx7g79/N5YWv8kgyv9SNzeAdlJWEUoo7zh3IY59uQ2vISqpzp31052SK\n/HqJXZM9AXWszzu3TSIzKZYzH/yc0b1S6ZEax02T62Li+6QnkHe0jO5+Ce76ZiSQ9+As+pqWxpJ7\npgXtmaUnuDlSUsXkgRkM7ZHMj88ZyLRHFgVNOZ+R5Mbp16u0evwHiirxuBz0DJJgLzXezcs3TaSw\n3MvCLYf56393cqy0qsF6KvXdPqF6+BCYCdkKXkiMbTytRnZanJ3ted6Xu3l7TeCqkG+trnNlHS31\nGkEYyR7+feuZXPKXrwIyGWSnBU8kGO92Uu71NZi0mRrnon9mIivzTxDvdtoWsiWSl4/L5geT+/GH\nD4zw8pH3fUya2WheNLoHAHuOlds53IJlThjQNZHth0tJjXfx1DVj2XO8nO89v5znv8oLSFhaUumz\nB9ZLKn0hE2h+tdMQ6XUFRXakoz/BB+zrROXNVQWszG947u5BUghtPlBMRqKbnmnxfLLpELefM5Cy\nqhq7s5aRaLi96oePbztUGhBqD+HLRBBOxAI5hXA4FI9eOZrcvl2C7h/n56f2j9g6b2gWL900wfY/\nu0OYyW/eeiZv/PAM+/3AronmdevKDO6WHJDNdEj30DPhzxnclVGmaCy5Zxp//25ugzJZybEs3HqE\nV8zIqn/dckaDMsEad8CO7OmREsfPZw4hMTaGMb0aDoiCYX35D6xqDfsKK3h9+V76pieQYgqufw85\nNc5FSpyLPukJ3DAph/OHdrPFo296w4STFv7jIwDTh2Rx69l1FmJSPYsjmAWSkehmUFYSD146ggcu\nGWFvX7a78fVUFm87wsb9xaTEu+x5RP7jCHecO5C0+IYuzIzEWMq9NQ3S3ce5nfYKnTmmpQAwpnca\nT8wZzX2zh6GUYqGf1Wh9Rpv2Gy42X61mX2EFX2w/GnR1Tus5psa5SY13MzI7tcGkWDByxFnh8Iu3\nHeGPH25pUMaf/YUV7A/iwtoeIgNCarwLl1Pxt//uCpoS31+M3rltEmA8j9gYJ+cPzWJtQRH7Ciso\n8/pIMDsF3VMN0TlQFFiP+uIBLZshf7IQAYki3rz1TB681GhsUhsZ5xjQNXA9+J+cO5A+6fGM65Nm\nN6QAF5rp7evnS+qaXGfK33PB4KDX+Mf1ufz56roAgV5d4gPmtVj4Wzdjeqdyup84/v27udx74eAG\nA6+WBWW5pDKS6kz/x64abb+2JmhaveUEd2BPf9KDn1Na5SPe7bQDFLr5BS/UHyvyF6A/XDqC8Tld\nGohb/8wE6tMnPd4O0+6aFNsgHUgwAUlPiGXBT6cwZ3zvgAb/QFHzZkP37hJPQmxMA7HyuJwsvfdc\nPr1rasD2zKRYqny1DVLneFxOeyxjaI9AS/Oi0T3thj5YT98/km/LwRKu+8cy9vpN1Jx34+m8fvNE\nMk23mv9gf7CUKdCy1TT3nijnWJmX7LS4AOth04HioOXP7J8e1MVnPVN/F+3wnil2RyzO7WT2qB44\nFLzwVR7lfhaI1dnyv+/0hEBXlfWMIrEcdFsRF1aUYUV4pQbpZVpcdXovstPi6ZkWR2mljxHZKfz0\nvIaTsn58zkByMhI4f1hg6vbYGCcZiW6undAnZDLH8TldmuXbnztzMK+vMBqt+rP1jcHohu652BhH\nQK/f39ryv+ajV40iIzHWbhRCjdF4XE772gO6JtqNdP178w+jPqNfOmf+MIPL//qVPVC7/BfT+Wjj\nQX719gb6ZyYwMjuVt1bvo0uC224k0uLdOB0qoLeZ4I7hy7nn8PLSfP6yyPCJ+w9GpzXTNz62dyqr\n9hRy+7QB3GKOiWWleCg5XErXpFjbCoqNcTboRFgi+4lfWC8YYb+3TRvA+n0rGR/C8gXjs/r5zMEB\nVkFNrSYxNobSKl9Ql5CVFt4SGv98ZqX1XD7dUzz2c7EizJqi4EQFBScqGJ/TBY/LaR8fqqefYlqc\n9UX6/TvqlklYcs80jpRW4XQoUuNdHC6pwuNy0KtLPJMGZPDMYiMNjtVZ6WGm0rGsMYCpp2Xybz+3\nY/dUDyWHSjtkLiyxQKIMq7feN6NhT9hCKcVZAzPIyUgISLdSH6dDcdHonjgdiu+flUM/v971il+e\nF1R0LBKakaYdIC3BzTRzXY3mrEsBcM34wLUygrk7wGhoTstKsl0wwfjT5SN5+PKRXDCsG7dM7W+v\nRBkMa37N9ybl2D92K2w1KzmWzKRYrsrtxYieKfxwan/bvedvlfTNiOflmybwx0tH2PebEOukZ2oc\nPc3xibR4V0DgQkojnQGLF78/ntd/eAarf3Ue/3PBILsjYVlUN03O4cZJwdPJQ+jB9Ti3kymnZbLp\n/hmkNfI5Alw2rm6lQ8tq6peZQFJsTED2A4An5tRZivFmY+sfieYffQXw/I3j7de3TO1PVnLw+i7/\nxfQG21LiXOwzMyykBXFVWiQHWZ7hq7nnBMzH6tUl3g7XtzppHjNYxT/1kNWRyUyKxeNyBEQ1fmtU\nd+bdeDp/unwkAAO7JvHDqf2Yd8PpQe+pPRELJMqYNaI7R0uquGZC61e0C8avvjWUX31raNMFTZqz\nyp2FZUE0a50T4GczBnP7OQO4960N5vHBv+ZNra8R53LWrfCIYQ01xoCuSWz53YwAy8Qa+LQCCdwx\nDt4zo+Z8NbXkZCQyfUhXtIa7LxjENeN7k5bgpl9mIn9dtJMic84G1IW4njc0K6DRSjJdUSVVwdej\nSIt3MXmgIcL1G3lr0NwZJKrouom9iXfHoBT0z0gMOsPfP9qqKfzdpumJsZwor6Z7ioeDRZX2hDww\nLKWLRtfNuLYingLWRa+XX6xHat13IyMplt9dNJybzQXY7GsmuMlMimVUdgpHS722ZZjsqYucu3BE\nd15etoe0eHeDUPjkOJcdOVZ33dCrF6aalqH1rPxdtFaiUKdDMe+G8Vz996WA4aadNqgrSim01pwo\n93LFuF5NinN7IQISZTgdKuTCRSeDET1TWO8X9dMcrIHO3D6hXST+OByKJI+Ley4YRE1trT3voj5N\nCUhCiOinf1yfG3I+i6deg1q33kjDusc4HXbdlILbpg0I2P+zGYO57ZVV9qTRM/obM5kvGRO4ZrlS\nivW/vYBv//kL1u8r4rZp/RnbO42SSh/emlqu9BPB+vxk+kDyj5Xz7ZENl2v+/cV1g/Mf1stqYNES\nAfGPlMtKNuabdE+JYxV1obi/+fZQpteLGpw2OJPpQ7py74VD7G0Jbidlfq4mfzdlsieGxHqdhn9+\nb7w9Ufad2w0Bv/+9TTz35W5iHIpnvpvLxxsPmZNd9zB7VA/uOHcgY3/3Sd1541wBUXJNYQmmx2Xc\n91i/ZZv9I72s5wqB1qtSqkMmUPRHBEQ4qfzrljNatPAOwLdGdmeF36Ss5tKrSzx/uXZcyP3BBGTZ\nvefy5Y6j3PXG2pCrIQYLiw7F+cOyqKqp5bKx2U0Xrseskd2ZNXKW/b5PuhG+HArL7ZIW7252Hbun\nxPHqzRObLOdxBxcKj7vlXvABXRPplRYPHCMzKZZnv5vLRU8ZueCuHt+7gQjHu2N49vpA9807t09i\nRd4JewkDMKLx9hVWkBgbYw+8XzuhN3dfMMi2BvwZ1M0QlOraWib2S7dzZK39zfnExjjwuJz8dvYw\ndhwu5eVl+WSnxpEa76KiqHnfX2tsKta8n/E5XVhyzzQeeH9zg06c2+nAW1PbIkHuCIiACCcVY92N\nlv1Irj+zL3OCNCxtJVi4clayxx5Ajg/RaLaEmyb3C5jXEkkst0gol11b8I+G8ydUyHco1v7mfNxO\nB899aWSArq3VjOqVyh8uGcHbq/c1+xkP6JrEgK5JzP33ei4cYQRxXDa2J09+vgOPy8nArCRev3ki\no3unhlye1upA1F/u2H+s7foz+wJw85R+ZKfF8dJNE1iRZ+TVOmtABo1RfwwEjE7N099p2KmJdZkC\nEobv3MlEBETo8Cilwi4eEHrCpGV5jOndMounvTEXLgxw54SLoT2S+ebecxn/ByMNsLU4Wksjg6zG\n+foz+3KgqILvnGEkUbxmQu9Wjcutv+98u9d+5/TTGNMnjTNNl9CEEFl3LazjmjNBz1oxsH9mIv0z\nEzl3SFbI4AwLK7VJc8JvPS4nJZU+293VWRABEaIWV4iGY0DXRF75wYQWu8zaG6sfHQkLBOoaRDCi\nnhpbFqApEmNjAsZYWokCcrgAAAptSURBVIu/WDocyg79bQ7Th2bx/bNyAiZxNpfGUr5YXJmbzcb9\nRVw4vOH4Un2sQIlIdJQiiQiIELW4GknZ4p+luLOgbRdW+C2Q+rTGFdnRcDkdLYocbClJHhePXjm6\n6YLUWUOh5k11VDqXvSQIYaSl/vuOjjUG0tncIELdLP7O9p0UC0SIWpoK4+1sDO6WzLZDpRG1QB68\ndETQ5ZeFtvHHS0cwe1QP+mUmNl24AyECIkQtnc1d0BQPXjaCOeN7hUwuGQ7mjA/vBFTBIN4d06Lw\n8I7CqdUFE4QoJt4d0ynHboTOi1ggQtTxnx+fFTR5nyAILUMERIg6hvdMCchmKwhC6xAXliAIgtAq\nIiogSqkZSqmtSqkdSqm5QfbfpZTapJRap5T6TCnVJ5L1EQRBEMJHxAREKeUEngJmAkOBq5VS9Wft\nrAZytdYjgfnAw5GqjyAIghBeImmBjAd2aK13aa29wGvARf4FtNYLtdZWUPlSoOUpSwVBEIR2IZIC\n0hPwX0C5wNwWiu8DH0awPoIgCEIY6RBRWEqp64BcYGqI/TcDNwP07i0TmQRBEDoCkbRA9gH+S6Fl\nm9sCUEpNB34BzNZaVwU7kdb6Ga11rtY6NzMzMyKVFQRBEFpGJAVkOTBQKZWjlHIDc4B3/QsopcYA\nf8MQj8MRrIsgCIIQZpSVAjoiJ1fqQuBxwAk8p7V+QCl1P7BCa/2uUupTYARgLbi8R2s9u4lzHgHy\nW1mlDOBoK4/tiJxq9wNyT52BU+1+IDruqY/WOqwunIgKSEdDKbVCa53b3vUIF6fa/YDcU2fgVLsf\nkHtqLTITXRAEQWgVIiCCIAhCq4g2AXmmvSsQZk61+wG5p87AqXY/IPfUKqJqDEQQBEEIH9FmgQiC\nIAhhIioEpKmswO2NUqqXUmqhmZl4o1LqJ+b2LkqpT5RS283/aeZ2pZR60ryfdUqpsX7nut4sv10p\ndb3f9nFKqfXmMU8qpSK+nqtSyqmUWq2U+o/5Pkcptcysw+vm/CCUUrHm+x3m/r5+5/i5uX2rUuoC\nv+0n/ZkqpVKVUvOVUluUUpuVUmd05meklPqp+X3boJR6VSnl6WzPSCn1nFLqsFJqg9+2iD+TUNeI\n4D39yfzerVNKvaWUSvXb16LPvzXPOCRa61P6D2MOyk6gH+AG1gJD27te9erYHRhrvk4CtmFkMH4Y\nmGtunws8ZL6+ECNvmAImAsvM7V2AXeb/NPN1mrnvG7OsMo+deRLu6y7gFeA/5vs3gDnm66eBW83X\nPwKeNl/PAV43Xw81n1cskGM+R2d7PVPgBeAm87UbSO2szwgjL91uIM7v2dzQ2Z4RMAUYC2zw2xbx\nZxLqGhG8p/OBGPP1Q3731OLPv6XPuNG6RvpH195/wBnAAr/3Pwd+3t71aqLO7wDnAVuB7ua27sBW\n8/XfgKv9ym81918N/M1v+9/Mbd2BLX7bA8pF6B6ygc+Ac4D/mD/Ao34/Avu5AAuAM8zXMWY5Vf9Z\nWeXa45kCKRgNrqq3vVM+I+qSnXYxP/P/ABd0xmcE9CWwsY34Mwl1jUjdU719lwAvB/tcm/r8W/M7\nbKye0eDCamlW4HbFNBvHAMuALK21NUv/IJBlvg51T41tLwiyPZI8DtwD1Jrv04FCrbUvSB3sepv7\ni8zyLb3PSJIDHAHmKcMt96xSKoFO+oy01vuAR4A9GJkgioCVdO5nZHEynkmoa5wMvkdd5vKW3lNr\nfochiQYB6TQopRKBN4E7tdbF/vu00S3oFCFzSqlvAYe11ivbuy5hJAbDrfBXrfUYoAzDdWHTyZ5R\nGsb6PDlADyABmNGulYoAJ+OZnMznrpT6BeADXj4Z12uKaBCQZmUFbm+UUi4M8XhZa/1vc/MhpVR3\nc393wEo4GeqeGtueHWR7pJgEzFZK5WEsJHYO8ASQqpSylhDwr4Ndb3N/CnCMlt9nJCkACrTWy8z3\n8zEEpbM+o+nAbq31Ea11NfBvjOfWmZ+Rxcl4JqGuETGUUjcA3wKuNUULWn5Px2j5Mw5NJHySHekP\no+e4C6OnZQ0mDWvvetWrowL+CTxeb/ufCByoe9h8PYvAwcBvzO1dMPz0aebfbqCLua/+YOCFJ+ne\nzqZuEP1fBA7e/ch8fRuBg3dvmK+HEThAuAtjcLBdnimwBBhkvr7PfD6d8hkBE4CNQLx5vReAH3fG\nZ0TDMZCIP5NQ14jgPc0ANgGZ9cq1+PNv6TNutJ6R/tF1hD+M6IttGFEJv2jv+gSp31kYJvA6YI35\ndyGG//EzYDvwqd+XWmGsN78TWI+xrrx1ru8BO8y/G/225wIbzGP+jyYGx8J4b2dTJyD9zB/kDvNL\nHGtu95jvd5j7+/kd/wuzzlvxi0pqj2cKjAZWmM/pbbOx6bTPCPgtsMW85otmI9SpnhHwKsYYTjWG\nlfj9k/FMQl0jgve0A2N8wmofnm7t59+aZxzqT2aiC4IgCK0iGsZABEEQhAggAiIIgiC0ChEQQRAE\noVWIgAiCIAitQgREEARBaBUiIEKnRylVo5Rao5Raq5RapZQ6s4nyqUqpHzXjvIuUUu22TrZSKk8p\nldFe1xeEphABEU4FKrTWo7XWozASxv2xifKpGJlHT1n8ZhoLQsQQARFONZKBE2DkFlNKfWZaJeuV\nUheZZR4E+ptWy5/Msj8zy6xVSj3od74rlFLfKKW2KaUm17+YUups01Kx1gl52W/NCNuCUErlKqUW\nma/vU0q9oJRaopTKV0pdqpR62Lz+R2ZaG4t7zO3fKKUGmMdnKqXeVEotN/8m+Z33RaXUlxgTAwUh\nokgvRTgViFNKrcGYSdsdI/cWQCVwida62GzIlyql3sVIPTFcaz0aQCk1EyOx4AStdblSqovfuWO0\n1uOVUhcCv8HIIVWfMRgpJfYDX2LklPqiiTr3B6ZhrOfwNXCZ1voepdRbGCk33jbLFWmtRyilvouR\n4fhbGHnFHtNaf6GU6o2RhnuIWX4ocJbWuqKJ6wtCmxEBEU4FKvzE4Azgn0qp4RipK/6glJqCkVa+\nJ8HTbk8H5mmtywG01sf99lmJLVdi5CcKxjda6wLz+mvMck0JyIda62ql1HqM3EUfmdvX17vOq37/\nH/Or71BVt2BhspnJGeBdEQ/hZCECIpxSaK2/Nq2NTIxcQJnAOLOxzsOwUlpClfm/htC/lyq/1/7l\nfNS5ietft+r/t3eHKhEFYRiG309MNq/AahQsVq/BICaxmWwWm3gNRqNXIBi0iCCWTeIFmIVNIoji\njmFmcRENzm5a3qec4XD4d7bst+c/8J+231GSj/I9U2j043PKL+sFYKOU8jZZsAXK65/fRJoxn4Fo\nriRZpf6jH1LHUT+38NgEVtplL9RXB49dA3tJllqNyRbWNJ6A9bbe6qyxPXG8b+sr6uRcAJKsddaW\npuIdiObB+BkI1LbVbinlM8k5cNHaRAPq5FlKKcMkd0keqa2kw/YjPEjyDlwCRzPY1zFwluQEuOms\nsZzkgXrHstPOHQCn7fwicAvsT7lX6d+cxitJ6mILS5LUxQCRJHUxQCRJXQwQSVIXA0SS1MUAkSR1\nMUAkSV0MEElSly/rQYr67EglQQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXINUSJRlX77",
        "colab_type": "text"
      },
      "source": [
        "##### Testing Our Model\n",
        "Once the model was trained, we got to actually test it! As mentioned above, we tested with both a training set from the set of original data we downloaded as well as with data we collected. This was not as much of a challenge computationally as we first assumed it would be. Essentially, we first formatted our inputs, then rant the various batches through our model. After that, it was a matter of simply taking the percentage of images which the model selected correctly in relation to the total number of pictures. \n",
        "\n",
        "We were also able to get the accuracy of our model picture by picture, basically, we chose images an image and the model output a guess and the correct value. To do this, we ran the image through the trained network from above. One point that was interesting from our image by image analysis is that we included the confidence of the model in its prediction. We calculated that by taking the maximum of all of the confidence metrics which the model had and dividing by the sum of the confidences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ13LfcF3Ji2",
        "colab_type": "code",
        "outputId": "080433fb-3665-45f7-bc0e-6fa675ff6565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "def get_accuracy(net, loader):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        # Get inputs in right form\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "        n_total += labels.shape[0]\n",
        "    return n_correct/n_total\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))\n",
        "print(\"Test accuracy our data is\", get_accuracy(net, test_loader2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.9139159921597002\n",
            "Test accuracy is 0.9095614141659579\n",
            "Test accuracy our data is 0.7023809523809523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMZxrWj47rA",
        "colab_type": "code",
        "outputId": "30f73d8d-3334-49e9-871e-45a70fc313dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "def examine_label(idx):\n",
        "    image, label = test_set[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    confidence = class_scores.cpu().detach().numpy()\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "    print('Prediction is: ', prediction)\n",
        "    print('Actual is: ', label)\n",
        "    print('Weights are: ', confidence)\n",
        "\n",
        "def examine_labelours(idx):\n",
        "    image, label = test_set2[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    confidence = class_scores.cpu().detach().numpy()\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "    print('Prediction is: ',prediction)\n",
        "    print('Actual is: ',label)\n",
        "    print('Weights are: ', confidence)\n",
        "\n",
        "examine_label(7)\n",
        "examine_labelours(23)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EROsfxETrH8RE6x/EROsfxETv0fP2SxLtTJ6O4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Prediction is:  4\n",
            "Actual is:  4\n",
            "Weights are:  [[-4.933739   -1.9634188  -2.8627555  -4.373681   -0.25245857 -5.1467133 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIFP/DxQ82jEwOqIWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Prediction is:  1\n",
            "Actual is:  1\n",
            "Weights are:  [[-7.195395   -0.53327596 -5.0945406  -5.306212   -0.91400886 -7.4776554 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdrxLTWBrLOG",
        "colab_type": "text"
      },
      "source": [
        "#### The Results and Bigger Picture\n",
        "\n",
        "Our model didn't really have too many big picture applications. If a sketch of a broom is misidentified as a sketch of broccoli, at the end of the day not too many people care very much. With that said, though, to think of this in terms of our broader application, teaching a child that the thing that they drew is incorrect or that what they sketched is correct when it shouldn't be is also not desirable.\n",
        "\n",
        "Unfortunately with our accuracy on the test images that we collected averaging around 70%, we would not feel very confident giving this algorithm to anyone who wanted to use it with potential users. We enjoyed making it and learned a lot, but to give it out to people to use, we would need to do a lot more to improve the accuracy on user data. On a more positive note, we were able to get our test data accuracy up to around 90%, which was awesome. "
      ]
    }
  ]
}