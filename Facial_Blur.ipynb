{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial_Blur.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiam6242/Facial_Blur/blob/master/Facial_Blur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qag1NL_FWy88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hey! this is wacky!!\n",
        "# can you see this?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t20OdGNZI7l",
        "colab_type": "text"
      },
      "source": [
        "yooo can you see me writing this?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4x1Y1xtZMAi",
        "colab_type": "code",
        "outputId": "e8bcf1bf-5768-4af6-ae03-f56b3a45c14e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "!pip install torchviz\n",
        "# !CUDA_LAUNCH_BLOCKING=1\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy\n",
        "import time\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy', 'eiffeltower.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy', 'bear.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy', 'airplane.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy', 'broccoli.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy', 'dog.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy', 'broom.npy', False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy\n",
            "To: /content/eiffeltower.npy\n",
            "100%|██████████| 106M/106M [00:00<00:00, 212MB/s]  \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy\n",
            "To: /content/bear.npy\n",
            "100%|██████████| 106M/106M [00:00<00:00, 175MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
            "To: /content/airplane.npy\n",
            "100%|██████████| 119M/119M [00:00<00:00, 168MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy\n",
            "To: /content/broccoli.npy\n",
            "100%|██████████| 104M/104M [00:00<00:00, 196MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy\n",
            "To: /content/dog.npy\n",
            "100%|██████████| 119M/119M [00:01<00:00, 77.2MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy\n",
            "To: /content/broom.npy\n",
            "100%|██████████| 91.7M/91.7M [00:00<00:00, 204MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'broom.npy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPERGQfVgc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tower = np.load('eiffeltower.npy') #type = 1\n",
        "bear = np.load('bear.npy') # type = 0\n",
        "airplane = np.load('airplane.npy')\n",
        "broccoli = np.load('broccoli.npy')\n",
        "dog = np.load('dog.npy')\n",
        "broom = np.load('broom.npy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICO8bUo6V2He",
        "colab_type": "code",
        "outputId": "bc501cb9-1fe8-45ce-c85c-26bbc04f4972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X = airplane[750]\n",
        "X = np.resize(X,(28,28))\n",
        "X = np.invert(X)\n",
        "plt.imshow(X, cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnoRPCJ8wWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yhVEn8Ql_h",
        "colab_type": "code",
        "outputId": "e53d25eb-9329-40ea-e93c-9a8c7a2ea121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import torch\n",
        "\n",
        "# class QuickDrawData(Dataset):\n",
        "#     def __init__(self, tower, bear, airplane, broccoli):\n",
        "#         super(QuickDrawData, self).__init__()\n",
        "#         self.data = np.vstack((tower, bear, airplane, broccoli))\n",
        "#         self.targets = np.concatenate((0*np.ones(tower.shape[0]), 1*np.ones(bear.shape[0]), 2*np.ones(airplane.shape[0]), 3*np.ones(broccoli.shape[0])))\n",
        "#         print(len(self.data))\n",
        "#         self.classes = ['tower', 'bear', 'airplane', 'broccoli']\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         return self.targets.shape[0]\n",
        "    \n",
        "#     def __getitem__(self, index):\n",
        "#         return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "class QuickDrawData(Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super(QuickDrawData, self).__init__()\n",
        "        count = 0\n",
        "        # self.data = np.empty(args[0].shape, dtype=int)\n",
        "        # self.targets = np.empty(args[0].shape, dtype=int)\n",
        "        self.classes = []\n",
        "        for arg in args:\n",
        "          # print(str(arg))\n",
        "          if type(arg) == str:\n",
        "            self.classes += arg\n",
        "          else:\n",
        "\n",
        "            if count == 0:\n",
        "              print(arg.shape)\n",
        "              self.data = np.array(arg)\n",
        "              self.targets = np.array(0*np.ones(arg.shape[0], dtype = int))\n",
        "              print(self.targets)\n",
        "              print(type(self.targets))\n",
        "              print(type(self.data))\n",
        "              print(type(self.classes))\n",
        "            else:\n",
        "              self.data = np.vstack((self.data, arg))\n",
        "              print(int(count)*np.ones(arg.shape[0], dtype = int))\n",
        "              print(self.targets)\n",
        "              self.targets = np.hstack((self.targets, int(count)*np.ones(arg.shape[0], dtype = int)))\n",
        "            count+=1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.targets.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "\n",
        "quick_draw_data = QuickDrawData(tower, bear, airplane, broccoli, dog, broom, 'tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "im, target = quick_draw_data[55102]\n",
        "plt.imshow(im.squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "im.shape\n",
        "print(target)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(134801, 784)\n",
            "[0 0 0 ... 0 0 0]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "[1 1 1 ... 1 1 1]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[2 2 2 ... 2 2 2]\n",
            "[0 0 0 ... 1 1 1]\n",
            "[3 3 3 ... 3 3 3]\n",
            "[0 0 0 ... 2 2 2]\n",
            "[4 4 4 ... 4 4 4]\n",
            "[0 0 0 ... 3 3 3]\n",
            "[5 5 5 ... 5 5 5]\n",
            "[0 0 0 ... 4 4 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICjCDwRF+IGgCD8QFOEHgiL8QFD/B8Pzhyim7SBFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi8X_pFZAcm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9fMDbMQBAdAc",
        "colab": {}
      },
      "source": [
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import random_split\n",
        "\n",
        "# import torch\n",
        "\n",
        "# class QuickDrawData(Dataset):\n",
        "#     def __init__(self,airplane, broccoli):\n",
        "#         super(QuickDrawData, self).__init__()\n",
        "#         self.data = np.vstack((airplane, broccoli))\n",
        "#         self.targets = np.concatenate((0*np.ones(airplane.shape[0]), 1*np.ones(broccoli.shape[0])))\n",
        "#         print(len(self.data))\n",
        "#         # self.classes = ['tower', 'bear', 'airplane', 'broccoli']\n",
        "#         self.classes = ['airplane', 'broccoli']\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         return self.targets.shape[0]\n",
        "    \n",
        "#     def __getitem__(self, index):\n",
        "#         return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "\n",
        "# quick_draw_data = QuickDrawData(airplane, broccoli)\n",
        "\n",
        "# im, target = quick_draw_data[502]\n",
        "# plt.imshow(im.squeeze(), cmap='gray')\n",
        "# plt.show()\n",
        "# im.shape\n",
        "# print(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbaebNQ7RwG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = int(len(quick_draw_data)*.9)\n",
        "train, test = random_split(quick_draw_data, [x,(len(quick_draw_data) - x)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMrsY0PERwJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data set information\n",
        "\n",
        "image_dims = 1, 28, 28\n",
        "n_training_samples = len(train) # How many training images to use\n",
        "n_test_samples = len(test) # How many test images to use\n",
        "classes = ('tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "# Load the training set\n",
        "train_set = train\n",
        "train_sampler = SubsetRandomSampler(\n",
        "    np.arange(n_training_samples, dtype=np.int64))\n",
        "\n",
        "#Load the test set\n",
        "test_set = test\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dv6-ImXRwMA",
        "colab_type": "code",
        "outputId": "5dbf432a-08a7-43dd-a338-8c93723b9771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(test_set)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QoJj27WRwO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyCNN, self).__init__()\n",
        "    \n",
        "    num_kernels = 16\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.maxpool_output_size = int(num_kernels*(image_dims[1]/2) * (image_dims[2]/2))\n",
        "    \n",
        "    fcl_size = 64\n",
        "    self.fc1 = nn.Linear(self.maxpool_output_size, fcl_size)\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "    fc2_size = len(classes)\n",
        "    self.fc2 = nn.Linear(fcl_size, fc2_size)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.activation_func(x)\n",
        "    \n",
        "    x = x.view(-1, self.maxpool_output_size)\n",
        "    x = self.fc1(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "  def get_loss(self, learning_rate):\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    return loss, optimizer\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHaayoLKRwaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = MyCNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SbsGk9qjpmY",
        "colab_type": "code",
        "outputId": "d04d4ec5-54f4-4b11-b287-fbd2657d907e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    fake_input = Variable(torch.zeros((1, image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "# \n",
        "visualize_network(net)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fb89d640e48>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"267pt\" height=\"493pt\"\n viewBox=\"0.00 0.00 266.50 493.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 489)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-489 262.5,-489 262.5,4 -4,4\"/>\n<!-- 140430414355816 -->\n<g id=\"node1\" class=\"node\">\n<title>140430414355816</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"171,-21 67,-21 67,0 171,0 171,-21\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140430414356208 -->\n<g id=\"node2\" class=\"node\">\n<title>140430414356208</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-91 0,-91 0,-57 54,-57 54,-91\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6)</text>\n</g>\n<!-- 140430414356208&#45;&gt;140430414355816 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140430414356208&#45;&gt;140430414355816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M51.6543,-56.9832C65.1894,-47.641 81.8926,-36.1122 95.278,-26.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.2865,-29.7398 103.5283,-21.1788 93.3102,-23.9788 97.2865,-29.7398\"/>\n</g>\n<!-- 140430414354416 -->\n<g id=\"node3\" class=\"node\">\n<title>140430414354416</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"166,-84.5 72,-84.5 72,-63.5 166,-63.5 166,-84.5\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140430414354416&#45;&gt;140430414355816 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140430414354416&#45;&gt;140430414355816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M119,-63.2281C119,-54.5091 119,-41.9699 119,-31.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.5001,-31.1128 119,-21.1128 115.5001,-31.1129 122.5001,-31.1128\"/>\n</g>\n<!-- 140430414354472 -->\n<g id=\"node4\" class=\"node\">\n<title>140430414354472</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"170,-154.5 66,-154.5 66,-133.5 170,-133.5 170,-154.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-140.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140430414354472&#45;&gt;140430414354416 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140430414354472&#45;&gt;140430414354416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118.1519,-133.3685C118.2972,-123.1925 118.5206,-107.5606 118.7016,-94.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.2034,-94.7806 118.8467,-84.7315 115.2041,-94.6805 122.2034,-94.7806\"/>\n</g>\n<!-- 140429112636696 -->\n<g id=\"node5\" class=\"node\">\n<title>140429112636696</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-231 0,-231 0,-197 54,-197 54,-231\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-217.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 140429112636696&#45;&gt;140430414354472 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140429112636696&#45;&gt;140430414354472</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49.4944,-196.6966C63.7034,-185.7666 81.9745,-171.7119 96.0735,-160.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.447,-163.4565 104.2393,-154.5852 94.179,-157.9081 98.447,-163.4565\"/>\n</g>\n<!-- 140429112637480 -->\n<g id=\"node6\" class=\"node\">\n<title>140429112637480</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-224.5 72.5,-224.5 72.5,-203.5 163.5,-203.5 163.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 140429112637480&#45;&gt;140430414354472 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140429112637480&#45;&gt;140430414354472</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-203.3685C118,-193.1925 118,-177.5606 118,-164.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-164.7315 118,-154.7315 114.5001,-164.7316 121.5001,-164.7315\"/>\n</g>\n<!-- 140429112635520 -->\n<g id=\"node7\" class=\"node\">\n<title>140429112635520</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"165,-294.5 71,-294.5 71,-273.5 165,-273.5 165,-294.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-280.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140429112635520&#45;&gt;140429112637480 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140429112635520&#45;&gt;140429112637480</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-273.3685C118,-263.1925 118,-247.5606 118,-234.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-234.7315 118,-224.7315 114.5001,-234.7316 121.5001,-234.7315\"/>\n</g>\n<!-- 140429112636080 -->\n<g id=\"node8\" class=\"node\">\n<title>140429112636080</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"208,-358 28,-358 28,-337 208,-337 208,-358\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140429112636080&#45;&gt;140429112635520 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140429112636080&#45;&gt;140429112635520</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-336.7281C118,-328.0091 118,-315.4699 118,-304.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-304.6128 118,-294.6128 114.5001,-304.6129 121.5001,-304.6128\"/>\n</g>\n<!-- 140429112638768 -->\n<g id=\"node9\" class=\"node\">\n<title>140429112638768</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"196.5,-415 39.5,-415 39.5,-394 196.5,-394 196.5,-415\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140429112638768&#45;&gt;140429112636080 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140429112638768&#45;&gt;140429112636080</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-393.7787C118,-386.6134 118,-376.9517 118,-368.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-368.1732 118,-358.1732 114.5001,-368.1732 121.5001,-368.1732\"/>\n</g>\n<!-- 140429112637704 -->\n<g id=\"node10\" class=\"node\">\n<title>140429112637704</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"112.5,-485 31.5,-485 31.5,-451 112.5,-451 112.5,-485\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"72\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16, 1, 3, 3)</text>\n</g>\n<!-- 140429112637704&#45;&gt;140429112638768 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140429112637704&#45;&gt;140429112638768</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M84.3272,-450.9832C90.4107,-442.5853 97.7742,-432.4204 104.0621,-423.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.0945,-425.5204 110.1266,-415.3687 101.4256,-421.4138 107.0945,-425.5204\"/>\n</g>\n<!-- 140429112636248 -->\n<g id=\"node11\" class=\"node\">\n<title>140429112636248</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"199,-485 131,-485 131,-451 199,-451 199,-485\"/>\n<text text-anchor=\"middle\" x=\"165\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"165\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140429112636248&#45;&gt;140429112638768 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140429112636248&#45;&gt;140429112638768</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M152.4049,-450.9832C146.1237,-442.4969 138.5069,-432.2062 132.0384,-423.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"134.8071,-421.3243 126.0445,-415.3687 129.1806,-425.4888 134.8071,-421.3243\"/>\n</g>\n<!-- 140429112636752 -->\n<g id=\"node12\" class=\"node\">\n<title>140429112636752</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"254.5,-224.5 181.5,-224.5 181.5,-203.5 254.5,-203.5 254.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140429112636752&#45;&gt;140430414354472 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140429112636752&#45;&gt;140430414354472</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M202.8122,-203.3685C186.492,-191.9444 160.3486,-173.644 141.398,-160.3786\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"143.2026,-157.3695 133.0031,-154.5022 139.1883,-163.1042 143.2026,-157.3695\"/>\n</g>\n<!-- 140429112635856 -->\n<g id=\"node13\" class=\"node\">\n<title>140429112635856</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"252.5,-301 183.5,-301 183.5,-267 252.5,-267 252.5,-301\"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-287.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"218\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 3136)</text>\n</g>\n<!-- 140429112635856&#45;&gt;140429112636752 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140429112635856&#45;&gt;140429112636752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218,-266.6966C218,-257.0634 218,-245.003 218,-234.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"221.5001,-234.7912 218,-224.7913 214.5001,-234.7913 221.5001,-234.7912\"/>\n</g>\n<!-- 140430414353744 -->\n<g id=\"node14\" class=\"node\">\n<title>140430414353744</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"258.5,-84.5 185.5,-84.5 185.5,-63.5 258.5,-63.5 258.5,-84.5\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140430414353744&#45;&gt;140430414355816 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140430414353744&#45;&gt;140430414355816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M204.5275,-63.2281C188.1519,-53.1325 163.4682,-37.9149 144.8209,-26.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"146.5636,-23.3814 136.2145,-21.1128 142.8901,-29.3401 146.5636,-23.3814\"/>\n</g>\n<!-- 140430414353856 -->\n<g id=\"node15\" class=\"node\">\n<title>140430414353856</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"255.5,-161 188.5,-161 188.5,-127 255.5,-127 255.5,-161\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-147.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"222\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6, 64)</text>\n</g>\n<!-- 140430414353856&#45;&gt;140430414353744 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140430414353856&#45;&gt;140430414353744</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M222,-126.6966C222,-117.0634 222,-105.003 222,-94.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"225.5001,-94.7912 222,-84.7913 218.5001,-94.7913 225.5001,-94.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_9CTlb4Yx7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training parameters\n",
        "batch_size = 1024\n",
        "learning_rate = 1e-2\n",
        "n_epochs = 10\n",
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, sampler=train_sampler, num_workers = 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, sampler=test_sampler, num_workers = 2)\n",
        "\n",
        "def train_model(net):\n",
        "    \"\"\" Train a the specified network.\n",
        "\n",
        "        Outputs a tuple with the following four elements\n",
        "        train_hist_x: the x-values (batch number) that the training set was \n",
        "            evaluated on.\n",
        "        train_loss_hist: the loss values for the training set corresponding to\n",
        "            the batch numbers returned in train_hist_x\n",
        "        test_hist_x: the x-values (batch number) that the test set was \n",
        "            evaluated on.\n",
        "        test_loss_hist: the loss values for the test set corresponding to\n",
        "            the batch numbers returned in test_hist_x\n",
        "    \"\"\" \n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 20\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    test_hist_x = []\n",
        "    test_loss_hist = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    # Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "            # Get inputs in right form\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "            \n",
        "            # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Compute the loss and find the loss with respect to each parameter of the model\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            \n",
        "            # Change each parameter with respect to the recently computed loss.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print every 20th batch of an epoch\n",
        "            if (i % print_every) == print_every-1:\n",
        "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "                # Reset running loss and time\n",
        "                train_loss_hist.append(running_loss / print_every)\n",
        "                train_hist_x.append(idx)\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            idx += 1\n",
        "\n",
        "        # At the end of the epoch, do a pass on the test set\n",
        "        total_test_loss = 0\n",
        "        for inputs, labels in test_loader:\n",
        "\n",
        "            # Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            test_outputs = net(inputs)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.data.item()\n",
        "        test_loss_hist.append(total_test_loss / len(test_loader))\n",
        "        test_hist_x.append(idx)\n",
        "        print(\"Validation loss = {:.2f}\".format(\n",
        "            total_test_loss / len(test_loader)))\n",
        "\n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "    return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGyZdjgNjf_h",
        "colab_type": "code",
        "outputId": "b000f094-3845-4393-8445-21dfae35225c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 20\t train_loss: 57.10 took: 1.09s\n",
            "Epoch 1, Iteration 40\t train_loss: 1.40 took: 0.98s\n",
            "Epoch 1, Iteration 60\t train_loss: 1.30 took: 0.77s\n",
            "Epoch 1, Iteration 80\t train_loss: 1.22 took: 0.77s\n",
            "Epoch 1, Iteration 100\t train_loss: 1.17 took: 0.76s\n",
            "Epoch 1, Iteration 120\t train_loss: 1.10 took: 0.75s\n",
            "Epoch 1, Iteration 140\t train_loss: 1.02 took: 0.77s\n",
            "Epoch 1, Iteration 160\t train_loss: 1.00 took: 0.79s\n",
            "Epoch 1, Iteration 180\t train_loss: 0.95 took: 0.74s\n",
            "Epoch 1, Iteration 200\t train_loss: 0.92 took: 0.74s\n",
            "Epoch 1, Iteration 220\t train_loss: 0.88 took: 0.87s\n",
            "Epoch 1, Iteration 240\t train_loss: 0.81 took: 0.94s\n",
            "Epoch 1, Iteration 260\t train_loss: 0.77 took: 0.81s\n",
            "Epoch 1, Iteration 280\t train_loss: 0.72 took: 0.84s\n",
            "Epoch 1, Iteration 300\t train_loss: 0.68 took: 0.76s\n",
            "Epoch 1, Iteration 320\t train_loss: 0.65 took: 0.73s\n",
            "Epoch 1, Iteration 340\t train_loss: 0.62 took: 0.77s\n",
            "Epoch 1, Iteration 360\t train_loss: 0.61 took: 0.74s\n",
            "Epoch 1, Iteration 380\t train_loss: 0.59 took: 0.75s\n",
            "Epoch 1, Iteration 400\t train_loss: 0.60 took: 0.76s\n",
            "Epoch 1, Iteration 420\t train_loss: 0.56 took: 0.89s\n",
            "Epoch 1, Iteration 440\t train_loss: 0.56 took: 0.76s\n",
            "Epoch 1, Iteration 460\t train_loss: 0.54 took: 0.75s\n",
            "Epoch 1, Iteration 480\t train_loss: 0.55 took: 0.75s\n",
            "Epoch 1, Iteration 500\t train_loss: 0.54 took: 0.77s\n",
            "Epoch 1, Iteration 520\t train_loss: 0.54 took: 0.76s\n",
            "Epoch 1, Iteration 540\t train_loss: 0.52 took: 0.79s\n",
            "Epoch 1, Iteration 560\t train_loss: 0.52 took: 0.78s\n",
            "Epoch 1, Iteration 580\t train_loss: 0.51 took: 0.73s\n",
            "Epoch 1, Iteration 600\t train_loss: 0.50 took: 0.74s\n",
            "Epoch 1, Iteration 620\t train_loss: 0.51 took: 0.92s\n",
            "Epoch 1, Iteration 640\t train_loss: 0.50 took: 0.76s\n",
            "Epoch 1, Iteration 660\t train_loss: 0.49 took: 0.76s\n",
            "Epoch 1, Iteration 680\t train_loss: 0.49 took: 0.76s\n",
            "Epoch 1, Iteration 700\t train_loss: 0.49 took: 0.87s\n",
            "Epoch 1, Iteration 720\t train_loss: 0.48 took: 0.90s\n",
            "Validation loss = 0.48\n",
            "Epoch 2, Iteration 20\t train_loss: 0.49 took: 1.29s\n",
            "Epoch 2, Iteration 40\t train_loss: 0.50 took: 0.98s\n",
            "Epoch 2, Iteration 60\t train_loss: 0.50 took: 0.75s\n",
            "Epoch 2, Iteration 80\t train_loss: 0.48 took: 0.74s\n",
            "Epoch 2, Iteration 100\t train_loss: 0.47 took: 0.78s\n",
            "Epoch 2, Iteration 120\t train_loss: 0.47 took: 0.71s\n",
            "Epoch 2, Iteration 140\t train_loss: 0.47 took: 0.86s\n",
            "Epoch 2, Iteration 160\t train_loss: 0.46 took: 0.80s\n",
            "Epoch 2, Iteration 180\t train_loss: 0.46 took: 0.74s\n",
            "Epoch 2, Iteration 200\t train_loss: 0.46 took: 0.76s\n",
            "Epoch 2, Iteration 220\t train_loss: 0.46 took: 0.76s\n",
            "Epoch 2, Iteration 240\t train_loss: 0.46 took: 0.87s\n",
            "Epoch 2, Iteration 260\t train_loss: 0.47 took: 0.75s\n",
            "Epoch 2, Iteration 280\t train_loss: 0.46 took: 0.78s\n",
            "Epoch 2, Iteration 300\t train_loss: 0.46 took: 0.76s\n",
            "Epoch 2, Iteration 320\t train_loss: 0.46 took: 0.75s\n",
            "Epoch 2, Iteration 340\t train_loss: 0.45 took: 0.78s\n",
            "Epoch 2, Iteration 360\t train_loss: 0.46 took: 0.83s\n",
            "Epoch 2, Iteration 380\t train_loss: 0.45 took: 0.90s\n",
            "Epoch 2, Iteration 400\t train_loss: 0.46 took: 0.78s\n",
            "Epoch 2, Iteration 420\t train_loss: 0.43 took: 0.84s\n",
            "Epoch 2, Iteration 440\t train_loss: 0.44 took: 0.75s\n",
            "Epoch 2, Iteration 460\t train_loss: 0.45 took: 0.83s\n",
            "Epoch 2, Iteration 480\t train_loss: 0.43 took: 0.87s\n",
            "Epoch 2, Iteration 500\t train_loss: 0.43 took: 0.78s\n",
            "Epoch 2, Iteration 520\t train_loss: 0.44 took: 0.74s\n",
            "Epoch 2, Iteration 540\t train_loss: 0.43 took: 0.77s\n",
            "Epoch 2, Iteration 560\t train_loss: 0.43 took: 0.76s\n",
            "Epoch 2, Iteration 580\t train_loss: 0.44 took: 0.82s\n",
            "Epoch 2, Iteration 600\t train_loss: 0.42 took: 0.75s\n",
            "Epoch 2, Iteration 620\t train_loss: 0.42 took: 0.89s\n",
            "Epoch 2, Iteration 640\t train_loss: 0.43 took: 0.76s\n",
            "Epoch 2, Iteration 660\t train_loss: 0.42 took: 0.78s\n",
            "Epoch 2, Iteration 680\t train_loss: 0.42 took: 0.82s\n",
            "Epoch 2, Iteration 700\t train_loss: 0.42 took: 0.75s\n",
            "Epoch 2, Iteration 720\t train_loss: 0.41 took: 0.76s\n",
            "Validation loss = 0.42\n",
            "Epoch 3, Iteration 20\t train_loss: 0.41 took: 1.06s\n",
            "Epoch 3, Iteration 40\t train_loss: 0.40 took: 0.92s\n",
            "Epoch 3, Iteration 60\t train_loss: 0.40 took: 0.79s\n",
            "Epoch 3, Iteration 80\t train_loss: 0.40 took: 0.81s\n",
            "Epoch 3, Iteration 100\t train_loss: 0.40 took: 0.78s\n",
            "Epoch 3, Iteration 120\t train_loss: 0.40 took: 0.76s\n",
            "Epoch 3, Iteration 140\t train_loss: 0.40 took: 0.75s\n",
            "Epoch 3, Iteration 160\t train_loss: 0.39 took: 0.78s\n",
            "Epoch 3, Iteration 180\t train_loss: 0.40 took: 0.75s\n",
            "Epoch 3, Iteration 200\t train_loss: 0.40 took: 0.76s\n",
            "Epoch 3, Iteration 220\t train_loss: 0.38 took: 0.77s\n",
            "Epoch 3, Iteration 240\t train_loss: 0.39 took: 0.84s\n",
            "Epoch 3, Iteration 260\t train_loss: 0.38 took: 0.76s\n",
            "Epoch 3, Iteration 280\t train_loss: 0.38 took: 0.78s\n",
            "Epoch 3, Iteration 300\t train_loss: 0.38 took: 0.76s\n",
            "Epoch 3, Iteration 320\t train_loss: 0.37 took: 0.76s\n",
            "Epoch 3, Iteration 340\t train_loss: 0.38 took: 0.78s\n",
            "Epoch 3, Iteration 360\t train_loss: 0.38 took: 0.78s\n",
            "Epoch 3, Iteration 380\t train_loss: 0.38 took: 0.79s\n",
            "Epoch 3, Iteration 400\t train_loss: 0.38 took: 0.78s\n",
            "Epoch 3, Iteration 420\t train_loss: 0.39 took: 0.97s\n",
            "Epoch 3, Iteration 440\t train_loss: 0.38 took: 0.81s\n",
            "Epoch 3, Iteration 460\t train_loss: 0.37 took: 0.80s\n",
            "Epoch 3, Iteration 480\t train_loss: 0.38 took: 0.81s\n",
            "Epoch 3, Iteration 500\t train_loss: 0.39 took: 0.91s\n",
            "Epoch 3, Iteration 520\t train_loss: 0.36 took: 0.77s\n",
            "Epoch 3, Iteration 540\t train_loss: 0.37 took: 0.75s\n",
            "Epoch 3, Iteration 560\t train_loss: 0.38 took: 0.84s\n",
            "Epoch 3, Iteration 580\t train_loss: 0.38 took: 0.84s\n",
            "Epoch 3, Iteration 600\t train_loss: 0.37 took: 0.78s\n",
            "Epoch 3, Iteration 620\t train_loss: 0.39 took: 0.83s\n",
            "Epoch 3, Iteration 640\t train_loss: 0.38 took: 0.77s\n",
            "Epoch 3, Iteration 660\t train_loss: 0.36 took: 0.76s\n",
            "Epoch 3, Iteration 680\t train_loss: 0.39 took: 0.77s\n",
            "Epoch 3, Iteration 700\t train_loss: 0.38 took: 0.76s\n",
            "Epoch 3, Iteration 720\t train_loss: 0.38 took: 0.77s\n",
            "Validation loss = 0.37\n",
            "Epoch 4, Iteration 20\t train_loss: 0.37 took: 1.16s\n",
            "Epoch 4, Iteration 40\t train_loss: 0.36 took: 0.95s\n",
            "Epoch 4, Iteration 60\t train_loss: 0.36 took: 0.81s\n",
            "Epoch 4, Iteration 80\t train_loss: 0.36 took: 0.81s\n",
            "Epoch 4, Iteration 100\t train_loss: 0.38 took: 0.79s\n",
            "Epoch 4, Iteration 120\t train_loss: 0.36 took: 0.80s\n",
            "Epoch 4, Iteration 140\t train_loss: 0.36 took: 0.77s\n",
            "Epoch 4, Iteration 160\t train_loss: 0.37 took: 0.78s\n",
            "Epoch 4, Iteration 180\t train_loss: 0.38 took: 0.84s\n",
            "Epoch 4, Iteration 200\t train_loss: 0.37 took: 0.92s\n",
            "Epoch 4, Iteration 220\t train_loss: 0.36 took: 0.84s\n",
            "Epoch 4, Iteration 240\t train_loss: 0.37 took: 0.90s\n",
            "Epoch 4, Iteration 260\t train_loss: 0.37 took: 0.80s\n",
            "Epoch 4, Iteration 280\t train_loss: 0.37 took: 0.78s\n",
            "Epoch 4, Iteration 300\t train_loss: 0.36 took: 0.77s\n",
            "Epoch 4, Iteration 320\t train_loss: 0.36 took: 0.76s\n",
            "Epoch 4, Iteration 340\t train_loss: 0.35 took: 0.75s\n",
            "Epoch 4, Iteration 360\t train_loss: 0.36 took: 0.75s\n",
            "Epoch 4, Iteration 380\t train_loss: 0.37 took: 0.74s\n",
            "Epoch 4, Iteration 400\t train_loss: 0.37 took: 0.78s\n",
            "Epoch 4, Iteration 420\t train_loss: 0.36 took: 0.88s\n",
            "Epoch 4, Iteration 440\t train_loss: 0.35 took: 0.75s\n",
            "Epoch 4, Iteration 460\t train_loss: 0.35 took: 0.75s\n",
            "Epoch 4, Iteration 480\t train_loss: 0.36 took: 0.79s\n",
            "Epoch 4, Iteration 500\t train_loss: 0.36 took: 0.77s\n",
            "Epoch 4, Iteration 520\t train_loss: 0.36 took: 0.79s\n",
            "Epoch 4, Iteration 540\t train_loss: 0.37 took: 0.78s\n",
            "Epoch 4, Iteration 560\t train_loss: 0.36 took: 0.77s\n",
            "Epoch 4, Iteration 580\t train_loss: 0.36 took: 0.86s\n",
            "Epoch 4, Iteration 600\t train_loss: 0.36 took: 0.85s\n",
            "Epoch 4, Iteration 620\t train_loss: 0.36 took: 0.87s\n",
            "Epoch 4, Iteration 640\t train_loss: 0.36 took: 0.76s\n",
            "Epoch 4, Iteration 660\t train_loss: 0.37 took: 0.76s\n",
            "Epoch 4, Iteration 680\t train_loss: 0.36 took: 0.89s\n",
            "Epoch 4, Iteration 700\t train_loss: 0.36 took: 0.81s\n",
            "Epoch 4, Iteration 720\t train_loss: 0.35 took: 0.74s\n",
            "Validation loss = 0.36\n",
            "Epoch 5, Iteration 20\t train_loss: 0.36 took: 1.08s\n",
            "Epoch 5, Iteration 40\t train_loss: 0.35 took: 1.09s\n",
            "Epoch 5, Iteration 60\t train_loss: 0.35 took: 0.74s\n",
            "Epoch 5, Iteration 80\t train_loss: 0.36 took: 0.72s\n",
            "Epoch 5, Iteration 100\t train_loss: 0.36 took: 0.74s\n",
            "Epoch 5, Iteration 120\t train_loss: 0.35 took: 0.75s\n",
            "Epoch 5, Iteration 140\t train_loss: 0.36 took: 0.76s\n",
            "Epoch 5, Iteration 160\t train_loss: 0.36 took: 0.72s\n",
            "Epoch 5, Iteration 180\t train_loss: 0.34 took: 0.80s\n",
            "Epoch 5, Iteration 200\t train_loss: 0.35 took: 0.83s\n",
            "Epoch 5, Iteration 220\t train_loss: 0.35 took: 0.80s\n",
            "Epoch 5, Iteration 240\t train_loss: 0.34 took: 0.86s\n",
            "Epoch 5, Iteration 260\t train_loss: 0.34 took: 0.74s\n",
            "Epoch 5, Iteration 280\t train_loss: 0.35 took: 0.75s\n",
            "Epoch 5, Iteration 300\t train_loss: 0.36 took: 0.76s\n",
            "Epoch 5, Iteration 320\t train_loss: 0.34 took: 0.80s\n",
            "Epoch 5, Iteration 340\t train_loss: 0.36 took: 0.74s\n",
            "Epoch 5, Iteration 360\t train_loss: 0.37 took: 0.75s\n",
            "Epoch 5, Iteration 380\t train_loss: 0.35 took: 0.82s\n",
            "Epoch 5, Iteration 400\t train_loss: 0.37 took: 0.87s\n",
            "Epoch 5, Iteration 420\t train_loss: 0.35 took: 0.89s\n",
            "Epoch 5, Iteration 440\t train_loss: 0.35 took: 0.77s\n",
            "Epoch 5, Iteration 460\t train_loss: 0.36 took: 0.75s\n",
            "Epoch 5, Iteration 480\t train_loss: 0.35 took: 0.76s\n",
            "Epoch 5, Iteration 500\t train_loss: 0.36 took: 0.75s\n",
            "Epoch 5, Iteration 520\t train_loss: 0.35 took: 0.76s\n",
            "Epoch 5, Iteration 540\t train_loss: 0.35 took: 0.77s\n",
            "Epoch 5, Iteration 560\t train_loss: 0.35 took: 0.76s\n",
            "Epoch 5, Iteration 580\t train_loss: 0.35 took: 0.76s\n",
            "Epoch 5, Iteration 600\t train_loss: 0.34 took: 0.76s\n",
            "Epoch 5, Iteration 620\t train_loss: 0.34 took: 0.88s\n",
            "Epoch 5, Iteration 640\t train_loss: 0.34 took: 0.78s\n",
            "Epoch 5, Iteration 660\t train_loss: 0.34 took: 0.82s\n",
            "Epoch 5, Iteration 680\t train_loss: 0.34 took: 0.83s\n",
            "Epoch 5, Iteration 700\t train_loss: 0.35 took: 0.77s\n",
            "Epoch 5, Iteration 720\t train_loss: 0.34 took: 0.74s\n",
            "Validation loss = 0.36\n",
            "Epoch 6, Iteration 20\t train_loss: 0.34 took: 1.07s\n",
            "Epoch 6, Iteration 40\t train_loss: 0.34 took: 1.03s\n",
            "Epoch 6, Iteration 60\t train_loss: 0.34 took: 0.77s\n",
            "Epoch 6, Iteration 80\t train_loss: 0.34 took: 0.76s\n",
            "Epoch 6, Iteration 100\t train_loss: 0.35 took: 0.77s\n",
            "Epoch 6, Iteration 120\t train_loss: 0.35 took: 0.74s\n",
            "Epoch 6, Iteration 140\t train_loss: 0.35 took: 0.74s\n",
            "Epoch 6, Iteration 160\t train_loss: 0.36 took: 0.74s\n",
            "Epoch 6, Iteration 180\t train_loss: 0.33 took: 0.78s\n",
            "Epoch 6, Iteration 200\t train_loss: 0.35 took: 0.77s\n",
            "Epoch 6, Iteration 220\t train_loss: 0.35 took: 0.78s\n",
            "Epoch 6, Iteration 240\t train_loss: 0.35 took: 0.87s\n",
            "Epoch 6, Iteration 260\t train_loss: 0.34 took: 0.76s\n",
            "Epoch 6, Iteration 280\t train_loss: 0.35 took: 0.74s\n",
            "Epoch 6, Iteration 300\t train_loss: 0.33 took: 0.74s\n",
            "Epoch 6, Iteration 320\t train_loss: 0.33 took: 0.76s\n",
            "Epoch 6, Iteration 340\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 6, Iteration 360\t train_loss: 0.33 took: 0.74s\n",
            "Epoch 6, Iteration 380\t train_loss: 0.34 took: 0.77s\n",
            "Epoch 6, Iteration 400\t train_loss: 0.34 took: 0.84s\n",
            "Epoch 6, Iteration 420\t train_loss: 0.33 took: 0.86s\n",
            "Epoch 6, Iteration 440\t train_loss: 0.32 took: 0.77s\n",
            "Epoch 6, Iteration 460\t train_loss: 0.34 took: 0.75s\n",
            "Epoch 6, Iteration 480\t train_loss: 0.35 took: 0.77s\n",
            "Epoch 6, Iteration 500\t train_loss: 0.35 took: 0.78s\n",
            "Epoch 6, Iteration 520\t train_loss: 0.35 took: 0.77s\n",
            "Epoch 6, Iteration 540\t train_loss: 0.34 took: 0.74s\n",
            "Epoch 6, Iteration 560\t train_loss: 0.36 took: 0.77s\n",
            "Epoch 6, Iteration 580\t train_loss: 0.34 took: 0.76s\n",
            "Epoch 6, Iteration 600\t train_loss: 0.33 took: 0.73s\n",
            "Epoch 6, Iteration 620\t train_loss: 0.35 took: 0.89s\n",
            "Epoch 6, Iteration 640\t train_loss: 0.36 took: 0.78s\n",
            "Epoch 6, Iteration 660\t train_loss: 0.34 took: 0.77s\n",
            "Epoch 6, Iteration 680\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 6, Iteration 700\t train_loss: 0.33 took: 0.76s\n",
            "Epoch 6, Iteration 720\t train_loss: 0.34 took: 0.76s\n",
            "Validation loss = 0.35\n",
            "Epoch 7, Iteration 20\t train_loss: 0.32 took: 1.17s\n",
            "Epoch 7, Iteration 40\t train_loss: 0.32 took: 1.05s\n",
            "Epoch 7, Iteration 60\t train_loss: 0.33 took: 0.85s\n",
            "Epoch 7, Iteration 80\t train_loss: 0.34 took: 0.79s\n",
            "Epoch 7, Iteration 100\t train_loss: 0.34 took: 0.91s\n",
            "Epoch 7, Iteration 120\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 7, Iteration 140\t train_loss: 0.34 took: 0.76s\n",
            "Epoch 7, Iteration 160\t train_loss: 0.34 took: 0.77s\n",
            "Epoch 7, Iteration 180\t train_loss: 0.34 took: 0.77s\n",
            "Epoch 7, Iteration 200\t train_loss: 0.35 took: 0.77s\n",
            "Epoch 7, Iteration 220\t train_loss: 0.34 took: 0.75s\n",
            "Epoch 7, Iteration 240\t train_loss: 0.32 took: 0.87s\n",
            "Epoch 7, Iteration 260\t train_loss: 0.33 took: 0.86s\n",
            "Epoch 7, Iteration 280\t train_loss: 0.33 took: 0.84s\n",
            "Epoch 7, Iteration 300\t train_loss: 0.33 took: 0.79s\n",
            "Epoch 7, Iteration 320\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 7, Iteration 340\t train_loss: 0.34 took: 0.79s\n",
            "Epoch 7, Iteration 360\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 7, Iteration 380\t train_loss: 0.32 took: 0.75s\n",
            "Epoch 7, Iteration 400\t train_loss: 0.33 took: 0.76s\n",
            "Epoch 7, Iteration 420\t train_loss: 0.33 took: 0.92s\n",
            "Epoch 7, Iteration 440\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 7, Iteration 460\t train_loss: 0.34 took: 0.76s\n",
            "Epoch 7, Iteration 480\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 7, Iteration 500\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 7, Iteration 520\t train_loss: 0.34 took: 0.77s\n",
            "Epoch 7, Iteration 540\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 7, Iteration 560\t train_loss: 0.33 took: 0.80s\n",
            "Epoch 7, Iteration 580\t train_loss: 0.33 took: 0.81s\n",
            "Epoch 7, Iteration 600\t train_loss: 0.33 took: 0.96s\n",
            "Epoch 7, Iteration 620\t train_loss: 0.34 took: 0.86s\n",
            "Epoch 7, Iteration 640\t train_loss: 0.33 took: 0.78s\n",
            "Epoch 7, Iteration 660\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 7, Iteration 680\t train_loss: 0.33 took: 0.79s\n",
            "Epoch 7, Iteration 700\t train_loss: 0.34 took: 0.82s\n",
            "Epoch 7, Iteration 720\t train_loss: 0.33 took: 0.77s\n",
            "Validation loss = 0.33\n",
            "Epoch 8, Iteration 20\t train_loss: 0.33 took: 1.06s\n",
            "Epoch 8, Iteration 40\t train_loss: 0.32 took: 0.93s\n",
            "Epoch 8, Iteration 60\t train_loss: 0.33 took: 0.74s\n",
            "Epoch 8, Iteration 80\t train_loss: 0.32 took: 0.72s\n",
            "Epoch 8, Iteration 100\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 8, Iteration 120\t train_loss: 0.32 took: 0.72s\n",
            "Epoch 8, Iteration 140\t train_loss: 0.33 took: 0.72s\n",
            "Epoch 8, Iteration 160\t train_loss: 0.32 took: 0.72s\n",
            "Epoch 8, Iteration 180\t train_loss: 0.33 took: 0.73s\n",
            "Epoch 8, Iteration 200\t train_loss: 0.33 took: 0.72s\n",
            "Epoch 8, Iteration 220\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 8, Iteration 240\t train_loss: 0.32 took: 0.82s\n",
            "Epoch 8, Iteration 260\t train_loss: 0.33 took: 0.72s\n",
            "Epoch 8, Iteration 280\t train_loss: 0.32 took: 0.75s\n",
            "Epoch 8, Iteration 300\t train_loss: 0.32 took: 0.73s\n",
            "Epoch 8, Iteration 320\t train_loss: 0.32 took: 0.73s\n",
            "Epoch 8, Iteration 340\t train_loss: 0.34 took: 0.76s\n",
            "Epoch 8, Iteration 360\t train_loss: 0.33 took: 0.74s\n",
            "Epoch 8, Iteration 380\t train_loss: 0.34 took: 0.77s\n",
            "Epoch 8, Iteration 400\t train_loss: 0.34 took: 0.71s\n",
            "Epoch 8, Iteration 420\t train_loss: 0.33 took: 0.85s\n",
            "Epoch 8, Iteration 440\t train_loss: 0.32 took: 0.72s\n",
            "Epoch 8, Iteration 460\t train_loss: 0.34 took: 0.83s\n",
            "Epoch 8, Iteration 480\t train_loss: 0.32 took: 0.81s\n",
            "Epoch 8, Iteration 500\t train_loss: 0.31 took: 0.71s\n",
            "Epoch 8, Iteration 520\t train_loss: 0.32 took: 0.75s\n",
            "Epoch 8, Iteration 540\t train_loss: 0.32 took: 0.70s\n",
            "Epoch 8, Iteration 560\t train_loss: 0.33 took: 0.79s\n",
            "Epoch 8, Iteration 580\t train_loss: 0.32 took: 0.85s\n",
            "Epoch 8, Iteration 600\t train_loss: 0.32 took: 0.74s\n",
            "Epoch 8, Iteration 620\t train_loss: 0.32 took: 0.84s\n",
            "Epoch 8, Iteration 640\t train_loss: 0.31 took: 0.73s\n",
            "Epoch 8, Iteration 660\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 8, Iteration 680\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 8, Iteration 700\t train_loss: 0.32 took: 0.76s\n",
            "Epoch 8, Iteration 720\t train_loss: 0.32 took: 0.75s\n",
            "Validation loss = 0.33\n",
            "Epoch 9, Iteration 20\t train_loss: 0.33 took: 1.11s\n",
            "Epoch 9, Iteration 40\t train_loss: 0.32 took: 0.98s\n",
            "Epoch 9, Iteration 60\t train_loss: 0.32 took: 0.82s\n",
            "Epoch 9, Iteration 80\t train_loss: 0.33 took: 0.78s\n",
            "Epoch 9, Iteration 100\t train_loss: 0.32 took: 0.77s\n",
            "Epoch 9, Iteration 120\t train_loss: 0.31 took: 0.94s\n",
            "Epoch 9, Iteration 140\t train_loss: 0.31 took: 0.77s\n",
            "Epoch 9, Iteration 160\t train_loss: 0.31 took: 0.78s\n",
            "Epoch 9, Iteration 180\t train_loss: 0.31 took: 0.77s\n",
            "Epoch 9, Iteration 200\t train_loss: 0.31 took: 0.76s\n",
            "Epoch 9, Iteration 220\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 9, Iteration 240\t train_loss: 0.31 took: 0.86s\n",
            "Epoch 9, Iteration 260\t train_loss: 0.31 took: 0.77s\n",
            "Epoch 9, Iteration 280\t train_loss: 0.32 took: 0.74s\n",
            "Epoch 9, Iteration 300\t train_loss: 0.31 took: 0.75s\n",
            "Epoch 9, Iteration 320\t train_loss: 0.31 took: 0.77s\n",
            "Epoch 9, Iteration 340\t train_loss: 0.32 took: 0.74s\n",
            "Epoch 9, Iteration 360\t train_loss: 0.33 took: 0.73s\n",
            "Epoch 9, Iteration 380\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 9, Iteration 400\t train_loss: 0.33 took: 0.76s\n",
            "Epoch 9, Iteration 420\t train_loss: 0.32 took: 0.88s\n",
            "Epoch 9, Iteration 440\t train_loss: 0.32 took: 0.78s\n",
            "Epoch 9, Iteration 460\t train_loss: 0.32 took: 0.78s\n",
            "Epoch 9, Iteration 480\t train_loss: 0.33 took: 0.88s\n",
            "Epoch 9, Iteration 500\t train_loss: 0.32 took: 0.78s\n",
            "Epoch 9, Iteration 520\t train_loss: 0.32 took: 0.76s\n",
            "Epoch 9, Iteration 540\t train_loss: 0.31 took: 0.78s\n",
            "Epoch 9, Iteration 560\t train_loss: 0.31 took: 0.77s\n",
            "Epoch 9, Iteration 580\t train_loss: 0.32 took: 0.78s\n",
            "Epoch 9, Iteration 600\t train_loss: 0.33 took: 0.88s\n",
            "Epoch 9, Iteration 620\t train_loss: 0.33 took: 0.90s\n",
            "Epoch 9, Iteration 640\t train_loss: 0.32 took: 0.79s\n",
            "Epoch 9, Iteration 660\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 9, Iteration 680\t train_loss: 0.34 took: 0.80s\n",
            "Epoch 9, Iteration 700\t train_loss: 0.33 took: 0.78s\n",
            "Epoch 9, Iteration 720\t train_loss: 0.32 took: 0.77s\n",
            "Validation loss = 0.33\n",
            "Epoch 10, Iteration 20\t train_loss: 0.30 took: 1.05s\n",
            "Epoch 10, Iteration 40\t train_loss: 0.32 took: 0.95s\n",
            "Epoch 10, Iteration 60\t train_loss: 0.30 took: 0.82s\n",
            "Epoch 10, Iteration 80\t train_loss: 0.30 took: 0.85s\n",
            "Epoch 10, Iteration 100\t train_loss: 0.30 took: 0.72s\n",
            "Epoch 10, Iteration 120\t train_loss: 0.32 took: 0.76s\n",
            "Epoch 10, Iteration 140\t train_loss: 0.32 took: 0.85s\n",
            "Epoch 10, Iteration 160\t train_loss: 0.31 took: 0.84s\n",
            "Epoch 10, Iteration 180\t train_loss: 0.33 took: 0.76s\n",
            "Epoch 10, Iteration 200\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 10, Iteration 220\t train_loss: 0.32 took: 0.75s\n",
            "Epoch 10, Iteration 240\t train_loss: 0.32 took: 0.87s\n",
            "Epoch 10, Iteration 260\t train_loss: 0.32 took: 0.87s\n",
            "Epoch 10, Iteration 280\t train_loss: 0.31 took: 0.87s\n",
            "Epoch 10, Iteration 300\t train_loss: 0.32 took: 0.77s\n",
            "Epoch 10, Iteration 320\t train_loss: 0.33 took: 0.74s\n",
            "Epoch 10, Iteration 340\t train_loss: 0.31 took: 0.75s\n",
            "Epoch 10, Iteration 360\t train_loss: 0.31 took: 0.82s\n",
            "Epoch 10, Iteration 380\t train_loss: 0.31 took: 0.77s\n",
            "Epoch 10, Iteration 400\t train_loss: 0.33 took: 0.81s\n",
            "Epoch 10, Iteration 420\t train_loss: 0.32 took: 0.85s\n",
            "Epoch 10, Iteration 440\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 10, Iteration 460\t train_loss: 0.32 took: 0.75s\n",
            "Epoch 10, Iteration 480\t train_loss: 0.30 took: 0.74s\n",
            "Epoch 10, Iteration 500\t train_loss: 0.32 took: 0.74s\n",
            "Epoch 10, Iteration 520\t train_loss: 0.32 took: 0.92s\n",
            "Epoch 10, Iteration 540\t train_loss: 0.33 took: 0.80s\n",
            "Epoch 10, Iteration 560\t train_loss: 0.32 took: 0.83s\n",
            "Epoch 10, Iteration 580\t train_loss: 0.33 took: 0.77s\n",
            "Epoch 10, Iteration 600\t train_loss: 0.33 took: 0.75s\n",
            "Epoch 10, Iteration 620\t train_loss: 0.33 took: 0.81s\n",
            "Epoch 10, Iteration 640\t train_loss: 0.32 took: 0.77s\n",
            "Epoch 10, Iteration 660\t train_loss: 0.32 took: 0.78s\n",
            "Epoch 10, Iteration 680\t train_loss: 0.33 took: 0.76s\n",
            "Epoch 10, Iteration 700\t train_loss: 0.34 took: 0.74s\n",
            "Epoch 10, Iteration 720\t train_loss: 0.35 took: 0.76s\n",
            "Validation loss = 0.33\n",
            "Training finished, took 328.95s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNSsFzVji6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "c27397df-dd3b-4c18-c994-da81e99025fd"
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlim(1000,7000)\n",
        "plt.ylim(0,.75)\n",
        "plt.show()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD0GslnsQi8dBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ13LfcF3Ji2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9f9878e6-16d2-4b06-e251-e35a1294621f"
      },
      "source": [
        "def get_accuracy(net, loader):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        # Get inputs in right form\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "        n_total += labels.shape[0]\n",
        "    return n_correct/n_total\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.89418835078322\n",
            "Test accuracy is 0.8880087474182967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMZxrWj47rA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "d67a0904-39c5-4a6b-ee3c-55fae2203681"
      },
      "source": [
        "def examine_label(idx):\n",
        "    image, label = test_set[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "    print(prediction)\n",
        "    print(label)\n",
        "\n",
        "examine_label(10003)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIrwA0ERfiAowg8ERfiBoP4PXtNy2R12gmcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSeuwv8R5NGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}