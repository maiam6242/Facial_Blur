{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial_Blur.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiam6242/Facial_Blur/blob/master/Facial_Blur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qag1NL_FWy88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hey! this is wacky!!\n",
        "# can you see this?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t20OdGNZI7l",
        "colab_type": "text"
      },
      "source": [
        "yooo can you see me writing this?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4x1Y1xtZMAi",
        "colab_type": "code",
        "outputId": "92c479a5-b740-4fc0-a57b-039649fa0a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install torchviz\n",
        "# !CUDA_LAUNCH_BLOCKING=1\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy\n",
        "import time\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy', 'eiffeltower.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy', 'bear.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy', 'airplane.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy', 'broccoli.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy', 'dog.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy', 'broom.npy', False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.2.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.5)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=b34afb0f7b4cc87c00fff3527c6547aec8e05fab7a960bead9ddd8feef36ceb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy\n",
            "To: /content/eiffeltower.npy\n",
            "100%|██████████| 106M/106M [00:02<00:00, 52.2MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy\n",
            "To: /content/bear.npy\n",
            "100%|██████████| 106M/106M [00:01<00:00, 73.8MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
            "To: /content/airplane.npy\n",
            "100%|██████████| 119M/119M [00:02<00:00, 58.6MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy\n",
            "To: /content/broccoli.npy\n",
            "100%|██████████| 104M/104M [00:01<00:00, 80.2MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy\n",
            "To: /content/dog.npy\n",
            "100%|██████████| 119M/119M [00:01<00:00, 78.4MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy\n",
            "To: /content/broom.npy\n",
            "100%|██████████| 91.7M/91.7M [00:01<00:00, 79.7MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'broom.npy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPERGQfVgc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tower = np.load('eiffeltower.npy') #type = 1\n",
        "bear = np.load('bear.npy') # type = 0\n",
        "airplane = np.load('airplane.npy')\n",
        "broccoli = np.load('broccoli.npy')\n",
        "dog = np.load('dog.npy')\n",
        "broom = np.load('broom.npy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICO8bUo6V2He",
        "colab_type": "code",
        "outputId": "85519dbb-36d6-43c2-a6fc-47b945af055c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X = airplane[750]\n",
        "X = np.resize(X,(28,28))\n",
        "X = np.invert(X)\n",
        "plt.imshow(X, cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnoRPCJ8wWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgR_pbWwS4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "82bc8d8e-bb95-48f7-e013-78ba7039437c"
      },
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
        "\n",
        "\n",
        "airplane_list = []\n",
        "for filename in glob.glob('airplane*.jpg'):\n",
        "    im=Image.open(filename)\n",
        "    airplane_list.append( np.array((np.invert(np.array(im))))[1:29,1:29])\n",
        "\n",
        "broom_list = []\n",
        "for filename in glob.glob('broom*.jpg'):\n",
        "    im=Image.open(filename)\n",
        "    broom_list.append( np.array((np.invert(np.array(im))))[1:29,1:29])\n",
        "\n",
        "bear_list = []\n",
        "for filename in glob.glob('bear*.jpg'):\n",
        "    im=Image.open(filename)\n",
        "    bear_list.append( np.array((np.invert(np.array(im))))[1:29,1:29])\n",
        "\n",
        "broccoli_list = []\n",
        "for filename in glob.glob('broccoli*.jpg'):\n",
        "    im=Image.open(filename)\n",
        "    broccoli_list.append( np.array((np.invert(np.array(im))))[1:29,1:29])\n",
        "\n",
        "tower_list = []\n",
        "for filename in glob.glob('tower*.jpg'):\n",
        "    im=Image.open(filename)\n",
        "    tower_list.append( np.array((np.invert(np.array(im))))[1:29,1:29])\n",
        "\n",
        "dog_list = []\n",
        "for filename in glob.glob('dog*.jpg'):\n",
        "    im=Image.open(filename)\n",
        "    dog_list.append( np.array((np.invert(np.array(im))))[1:29,1:29])\n",
        "\n",
        "dog_list = np.array(dog_list)\n",
        "broccoli_list = np.array(broccoli_list)\n",
        "bear_list = np.array(bear_list)\n",
        "broom_list = np.array(broom_list)\n",
        "airplane_list = np.array(airplane_list)\n",
        "print(len(tower_list))\n",
        "tower_list = np.array(tower_list)\n",
        "\n",
        "\n",
        "# dog_list = np.reshape(dog_list, (14,1024))\n",
        "\n",
        "# plt.imshow(dog_list[6],cmap=\"gray\")\n",
        "# print(np.array(airplane_list[0]))\n",
        "# print(len(np.array(dog_list)))\n",
        "print(np.shape(np.array(airplane_list[0])))\n",
        "# print(dog_list[6])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz77DUyKpWse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yhVEn8Ql_h",
        "colab_type": "code",
        "outputId": "cec18352-ee38-4160-b7b9-896f1d722aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# class QuickDrawData(Dataset):\n",
        "#     def __init__(self, tower, bear, airplane, broccoli):\n",
        "#         super(QuickDrawData, self).__init__()\n",
        "#         self.data = np.vstack((tower, bear, airplane, broccoli))\n",
        "#         self.targets = np.concatenate((0*np.ones(tower.shape[0]), 1*np.ones(bear.shape[0]), 2*np.ones(airplane.shape[0]), 3*np.ones(broccoli.shape[0])))\n",
        "#         print(len(self.data))\n",
        "#         self.classes = ['tower', 'bear', 'airplane', 'broccoli']\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         return self.targets.shape[0]\n",
        "    \n",
        "#     def __getitem__(self, index):\n",
        "#         return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "class QuickDrawData(Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super(QuickDrawData, self).__init__()\n",
        "        count = 0\n",
        "        # self.data = np.empty(args[0].shape, dtype=int)\n",
        "        # self.targets = np.empty(args[0].shape, dtype=int)\n",
        "        self.classes = []\n",
        "        for arg in args:\n",
        "          # print(str(arg))\n",
        "          if type(arg) == str:\n",
        "            self.classes += arg\n",
        "          else:\n",
        "\n",
        "            if count == 0:\n",
        "              print(arg.shape)\n",
        "              self.data = np.array(arg)\n",
        "              self.targets = np.array(0*np.ones(arg.shape[0], dtype = int))\n",
        "              print(self.targets)\n",
        "              print(type(self.targets))\n",
        "              print(type(self.data))\n",
        "              print(type(self.classes))\n",
        "            else:\n",
        "              self.data = np.vstack((self.data, arg))\n",
        "              print(int(count)*np.ones(arg.shape[0], dtype = int))\n",
        "              print(self.targets)\n",
        "              self.targets = np.hstack((self.targets, int(count)*np.ones(arg.shape[0], dtype = int)))\n",
        "            count+=1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.targets.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # print(self.data[index, :])\n",
        "        # print(type(self.data[index, :]))\n",
        "        # print(np.size(self.data[index, :]))\n",
        "        return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "\n",
        "quick_draw_data = QuickDrawData(tower, bear, airplane, broccoli, dog, broom, 'tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "print(type(tower_list))\n",
        "\n",
        "testdata = QuickDrawData(np.array(tower_list), np.array(bear_list), np.array(airplane_list), np.array(broccoli_list), np.array(dog_list), np.array(broom_list), 'tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "# im, target = quick_draw_data[55102]\n",
        "# plt.imshow(im.squeeze(), cmap='gray')\n",
        "# plt.show()\n",
        "# im.shape\n",
        "# print(target)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(134801, 784)\n",
            "[0 0 0 ... 0 0 0]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "[1 1 1 ... 1 1 1]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[2 2 2 ... 2 2 2]\n",
            "[0 0 0 ... 1 1 1]\n",
            "[3 3 3 ... 3 3 3]\n",
            "[0 0 0 ... 2 2 2]\n",
            "[4 4 4 ... 4 4 4]\n",
            "[0 0 0 ... 3 3 3]\n",
            "[5 5 5 ... 5 5 5]\n",
            "[0 0 0 ... 4 4 4]\n",
            "<class 'numpy.ndarray'>\n",
            "(14, 28, 28)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2]\n",
            "[4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            "[5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi8X_pFZAcm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9fMDbMQBAdAc",
        "colab": {}
      },
      "source": [
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import random_split\n",
        "\n",
        "# import torch\n",
        "\n",
        "# class QuickDrawData(Dataset):\n",
        "#     def __init__(self,airplane, broccoli):\n",
        "#         super(QuickDrawData, self).__init__()\n",
        "#         self.data = np.vstack((airplane, broccoli))\n",
        "#         self.targets = np.concatenate((0*np.ones(airplane.shape[0]), 1*np.ones(broccoli.shape[0])))\n",
        "#         print(len(self.data))\n",
        "#         # self.classes = ['tower', 'bear', 'airplane', 'broccoli']\n",
        "#         self.classes = ['airplane', 'broccoli']\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         return self.targets.shape[0]\n",
        "    \n",
        "#     def __getitem__(self, index):\n",
        "#         return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "\n",
        "# quick_draw_data = QuickDrawData(airplane, broccoli)\n",
        "\n",
        "# im, target = quick_draw_data[502]\n",
        "# plt.imshow(im.squeeze(), cmap='gray')\n",
        "# plt.show()\n",
        "# im.shape\n",
        "# print(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbaebNQ7RwG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = int(len(quick_draw_data)*.9)\n",
        "train, test = random_split(quick_draw_data, [x,(len(quick_draw_data) - x)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMrsY0PERwJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data set information\n",
        "\n",
        "image_dims = 1, 28, 28\n",
        "n_training_samples = len(train) # How many training images to use\n",
        "# n_test_samples = len(test) # How many test images to use\n",
        "n_test_samples = 84 # How many test images to use                       --------------\n",
        "classes = ('tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "# Load the training set\n",
        "train_set = train\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "\n",
        "#Load the test set\n",
        "\n",
        "# choose correct set                                                    ---------------\n",
        "\n",
        "# test_set = test\n",
        "test_set = testdata\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dv6-ImXRwMA",
        "colab_type": "code",
        "outputId": "df5f178e-af6b-4885-adaf-d9f22b1ce505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(test_set)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.QuickDrawData"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QoJj27WRwO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyCNN, self).__init__()\n",
        "    \n",
        "    num_kernels = 16\n",
        "\n",
        "    fcl_size = 256\n",
        "    fcl_size2 = 128\n",
        "    fcl_size3 = 64\n",
        "\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.conv2 = nn.Conv2d(num_kernels, num_kernels*2, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.maxpool_output_size1 = int(num_kernels*(image_dims[1]/2) * (image_dims[2]/2))\n",
        "    self.maxpool_output_size2 = int(num_kernels*(image_dims[1]/4) * (image_dims[2]/4)*2)\n",
        "\n",
        "    self.batchnorm1 = nn.BatchNorm2d(num_kernels)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(num_kernels*2)\n",
        "    self.batchnorm3 = nn.BatchNorm1d(fcl_size)\n",
        "    self.batchnorm4 = nn.BatchNorm1d(fcl_size2)\n",
        "    self.batchnorm5 = nn.BatchNorm1d(fcl_size3)\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Linear(1568, fcl_size)\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "    # fc2_size = fcl_size\n",
        "\n",
        "    self.fc2 = nn.Linear(fcl_size, fcl_size2)\n",
        "    self.fc4 = nn.Linear(fcl_size2, fcl_size3)\n",
        "    # self.fc7 = nn.Linear(fcl_size, fcl_size)\n",
        "\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "\n",
        "    fc3_size = len(classes)\n",
        "    # fc3_size = 6\n",
        "    self.fc3 = nn.Linear(fcl_size3, fc3_size)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.batchnorm1(x)\n",
        "\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.batchnorm2(x)\n",
        "\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    # x = self.pool1(x)\n",
        "    # x = self.activation_func(x)\n",
        "    # x = x.view(-1, self.maxpool_output_size1)\n",
        "    # x = self.pool2(x)\n",
        "    # x = self.activation_func(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = x.view(-1, self.maxpool_output_size2)\n",
        "\n",
        "    # print(x.size())\n",
        "\n",
        "    x = self.fc1(x)\n",
        "\n",
        "\n",
        "    # print(x.size())\n",
        "\n",
        "    # x = self.batchnorm3(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    # x = self.batchnorm4(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.fc4(x)\n",
        "\n",
        "    # x = self.batchnorm5(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "    # x = self.fc2(x)\n",
        "    # x = self.activation_func(x)\n",
        "    x = self.fc3(x)\n",
        "    x = torch.nn.functional.log_softmax(x)\n",
        "    # x = self.activation_func(x)\n",
        "\n",
        "    # x = self.activation_func7(x)\n",
        "    return x\n",
        "\n",
        "  def get_loss(self, learning_rate):\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    return loss, optimizer\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHaayoLKRwaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = MyCNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SbsGk9qjpmY",
        "colab_type": "code",
        "outputId": "90b1da53-47fa-41a0-992b-fc7c5cce67a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    fake_input = Variable(torch.zeros((1, image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "# \n",
        "visualize_network(net)\n",
        "\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fe4f05fdac8>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"463pt\" height=\"864pt\"\n viewBox=\"0.00 0.00 463.04 864.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.7135 .7135) rotate(0) translate(4 1207)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1207 645,-1207 645,4 -4,4\"/>\n<!-- 140621361658120 -->\n<g id=\"node1\" class=\"node\">\n<title>140621361658120</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"539,-21 414,-21 414,0 539,0 539,-21\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogSoftmaxBackward</text>\n</g>\n<!-- 140621361659352 -->\n<g id=\"node2\" class=\"node\">\n<title>140621361659352</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"528.5,-78 424.5,-78 424.5,-57 528.5,-57 528.5,-78\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140621361659352&#45;&gt;140621361658120 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140621361659352&#45;&gt;140621361658120</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M476.5,-56.7787C476.5,-49.6134 476.5,-39.9517 476.5,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"480.0001,-31.1732 476.5,-21.1732 473.0001,-31.1732 480.0001,-31.1732\"/>\n</g>\n<!-- 140621361657448 -->\n<g id=\"node3\" class=\"node\">\n<title>140621361657448</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"411.5,-148 357.5,-148 357.5,-114 411.5,-114 411.5,-148\"/>\n<text text-anchor=\"middle\" x=\"384.5\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.bias</text>\n<text text-anchor=\"middle\" x=\"384.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6)</text>\n</g>\n<!-- 140621361657448&#45;&gt;140621361659352 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140621361657448&#45;&gt;140621361659352</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M409.1543,-113.9832C422.6894,-104.641 439.3926,-93.1122 452.778,-83.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.7865,-86.7398 461.0283,-78.1788 450.8102,-80.9788 454.7865,-86.7398\"/>\n</g>\n<!-- 140621361657000 -->\n<g id=\"node4\" class=\"node\">\n<title>140621361657000</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"523.5,-141.5 429.5,-141.5 429.5,-120.5 523.5,-120.5 523.5,-141.5\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140621361657000&#45;&gt;140621361659352 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140621361657000&#45;&gt;140621361659352</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M476.5,-120.2281C476.5,-111.5091 476.5,-98.9699 476.5,-88.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"480.0001,-88.1128 476.5,-78.1128 473.0001,-88.1129 480.0001,-88.1128\"/>\n</g>\n<!-- 140621361656048 -->\n<g id=\"node5\" class=\"node\">\n<title>140621361656048</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"527.5,-211.5 423.5,-211.5 423.5,-190.5 527.5,-190.5 527.5,-211.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-197.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140621361656048&#45;&gt;140621361657000 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140621361656048&#45;&gt;140621361657000</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.6519,-190.3685C475.7972,-180.1925 476.0206,-164.5606 476.2016,-151.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.7034,-151.7806 476.3467,-141.7315 472.7041,-151.6805 479.7034,-151.7806\"/>\n</g>\n<!-- 140621361657672 -->\n<g id=\"node6\" class=\"node\">\n<title>140621361657672</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-288 356.5,-288 356.5,-254 410.5,-254 410.5,-288\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc4.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-261.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 140621361657672&#45;&gt;140621361656048 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140621361657672&#45;&gt;140621361656048</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M406.2416,-253.6966C420.6068,-242.7666 439.0787,-228.7119 453.3325,-217.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.7491,-220.4258 461.5881,-211.5852 451.5104,-214.855 455.7491,-220.4258\"/>\n</g>\n<!-- 140621361658736 -->\n<g id=\"node7\" class=\"node\">\n<title>140621361658736</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"522.5,-281.5 428.5,-281.5 428.5,-260.5 522.5,-260.5 522.5,-281.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140621361658736&#45;&gt;140621361656048 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140621361658736&#45;&gt;140621361656048</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-260.3685C475.5,-250.1925 475.5,-234.5606 475.5,-221.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-221.7315 475.5,-211.7315 472.0001,-221.7316 479.0001,-221.7315\"/>\n</g>\n<!-- 140620970500616 -->\n<g id=\"node8\" class=\"node\">\n<title>140620970500616</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"527.5,-351.5 423.5,-351.5 423.5,-330.5 527.5,-330.5 527.5,-351.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-337.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140620970500616&#45;&gt;140621361658736 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140620970500616&#45;&gt;140621361658736</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-330.3685C475.5,-320.1925 475.5,-304.5606 475.5,-291.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-291.7315 475.5,-281.7315 472.0001,-291.7316 479.0001,-291.7315\"/>\n</g>\n<!-- 140620970502632 -->\n<g id=\"node9\" class=\"node\">\n<title>140620970502632</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-428 356.5,-428 356.5,-394 410.5,-394 410.5,-428\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-414.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 140620970502632&#45;&gt;140620970500616 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140620970502632&#45;&gt;140620970500616</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M406.2416,-393.6966C420.6068,-382.7666 439.0787,-368.7119 453.3325,-357.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.7491,-360.4258 461.5881,-351.5852 451.5104,-354.855 455.7491,-360.4258\"/>\n</g>\n<!-- 140620970500392 -->\n<g id=\"node10\" class=\"node\">\n<title>140620970500392</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"522.5,-421.5 428.5,-421.5 428.5,-400.5 522.5,-400.5 522.5,-421.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140620970500392&#45;&gt;140620970500616 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140620970500392&#45;&gt;140620970500616</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-400.3685C475.5,-390.1925 475.5,-374.5606 475.5,-361.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-361.7315 475.5,-351.7315 472.0001,-361.7316 479.0001,-361.7315\"/>\n</g>\n<!-- 140620970500504 -->\n<g id=\"node11\" class=\"node\">\n<title>140620970500504</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"526.5,-491.5 422.5,-491.5 422.5,-470.5 526.5,-470.5 526.5,-491.5\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-477.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140620970500504&#45;&gt;140620970500392 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140620970500504&#45;&gt;140620970500392</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M474.6519,-470.3685C474.7972,-460.1925 475.0206,-444.5606 475.2016,-431.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"478.7034,-431.7806 475.3467,-421.7315 471.7041,-431.6805 478.7034,-431.7806\"/>\n</g>\n<!-- 140620970500280 -->\n<g id=\"node12\" class=\"node\">\n<title>140620970500280</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-568 356.5,-568 356.5,-534 410.5,-534 410.5,-568\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-554.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-541.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 140620970500280&#45;&gt;140620970500504 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140620970500280&#45;&gt;140620970500504</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M405.9944,-533.6966C420.2034,-522.7666 438.4745,-508.7119 452.5735,-497.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.947,-500.4565 460.7393,-491.5852 450.679,-494.9081 454.947,-500.4565\"/>\n</g>\n<!-- 140620970501568 -->\n<g id=\"node13\" class=\"node\">\n<title>140620970501568</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"520,-561.5 429,-561.5 429,-540.5 520,-540.5 520,-561.5\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-547.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 140620970501568&#45;&gt;140620970500504 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140620970501568&#45;&gt;140620970500504</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M474.5,-540.3685C474.5,-530.1925 474.5,-514.5606 474.5,-501.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"478.0001,-501.7315 474.5,-491.7315 471.0001,-501.7316 478.0001,-501.7315\"/>\n</g>\n<!-- 140620970501512 -->\n<g id=\"node14\" class=\"node\">\n<title>140620970501512</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"520.5,-631.5 426.5,-631.5 426.5,-610.5 520.5,-610.5 520.5,-631.5\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-617.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140620970501512&#45;&gt;140620970501568 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140620970501512&#45;&gt;140620970501568</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.6519,-610.3685C473.7972,-600.1925 474.0206,-584.5606 474.2016,-571.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.7034,-571.7806 474.3467,-561.7315 470.7041,-571.6805 477.7034,-571.7806\"/>\n</g>\n<!-- 140620970503416 -->\n<g id=\"node15\" class=\"node\">\n<title>140620970503416</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"563.5,-695 383.5,-695 383.5,-674 563.5,-674 563.5,-695\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-681.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140620970503416&#45;&gt;140620970501512 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140620970503416&#45;&gt;140620970501512</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-673.7281C473.5,-665.0091 473.5,-652.4699 473.5,-641.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-641.6128 473.5,-631.6128 470.0001,-641.6129 477.0001,-641.6128\"/>\n</g>\n<!-- 140620970500336 -->\n<g id=\"node16\" class=\"node\">\n<title>140620970500336</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"550,-752 397,-752 397,-731 550,-731 550,-752\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-738.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnBatchNormBackward</text>\n</g>\n<!-- 140620970500336&#45;&gt;140620970503416 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140620970500336&#45;&gt;140620970503416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-730.7787C473.5,-723.6134 473.5,-713.9517 473.5,-705.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-705.1732 473.5,-695.1732 470.0001,-705.1732 477.0001,-705.1732\"/>\n</g>\n<!-- 140621361556952 -->\n<g id=\"node17\" class=\"node\">\n<title>140621361556952</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"401.5,-815.5 307.5,-815.5 307.5,-794.5 401.5,-794.5 401.5,-815.5\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-801.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140621361556952&#45;&gt;140620970500336 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140621361556952&#45;&gt;140620970500336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M374.4179,-794.3715C393.553,-784.1608 422.6839,-768.6162 444.4131,-757.0212\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"446.3423,-759.9589 453.5171,-752.1631 443.0468,-753.7831 446.3423,-759.9589\"/>\n</g>\n<!-- 140621361556672 -->\n<g id=\"node18\" class=\"node\">\n<title>140621361556672</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"433,-879 276,-879 276,-858 433,-858 433,-879\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-865.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140621361556672&#45;&gt;140621361556952 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140621361556672&#45;&gt;140621361556952</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-857.7281C354.5,-849.0091 354.5,-836.4699 354.5,-825.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-825.6128 354.5,-815.6128 351.0001,-825.6129 358.0001,-825.6128\"/>\n</g>\n<!-- 140621361555272 -->\n<g id=\"node19\" class=\"node\">\n<title>140621361555272</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"295.5,-942.5 115.5,-942.5 115.5,-921.5 295.5,-921.5 295.5,-942.5\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-928.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140621361555272&#45;&gt;140621361556672 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140621361555272&#45;&gt;140621361556672</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M230.4392,-921.3715C255.0429,-910.8861 292.8449,-894.7758 320.2558,-883.094\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"321.6522,-886.3036 329.4794,-879.1631 318.9077,-879.864 321.6522,-886.3036\"/>\n</g>\n<!-- 140621361555440 -->\n<g id=\"node20\" class=\"node\">\n<title>140621361555440</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"282,-1006 129,-1006 129,-985 282,-985 282,-1006\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-992.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnBatchNormBackward</text>\n</g>\n<!-- 140621361555440&#45;&gt;140621361555272 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140621361555440&#45;&gt;140621361555272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M205.5,-984.7281C205.5,-976.0091 205.5,-963.4699 205.5,-952.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0001,-952.6128 205.5,-942.6128 202.0001,-952.6129 209.0001,-952.6128\"/>\n</g>\n<!-- 140621361556336 -->\n<g id=\"node21\" class=\"node\">\n<title>140621361556336</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"133.5,-1069.5 39.5,-1069.5 39.5,-1048.5 133.5,-1048.5 133.5,-1069.5\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-1055.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140621361556336&#45;&gt;140621361555440 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140621361556336&#45;&gt;140621361555440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.4179,-1048.3715C125.553,-1038.1608 154.6839,-1022.6162 176.4131,-1011.0212\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"178.3423,-1013.9589 185.5171,-1006.1631 175.0468,-1007.7831 178.3423,-1013.9589\"/>\n</g>\n<!-- 140621361554096 -->\n<g id=\"node22\" class=\"node\">\n<title>140621361554096</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"165,-1133 8,-1133 8,-1112 165,-1112 165,-1133\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-1119.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140621361554096&#45;&gt;140621361556336 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140621361554096&#45;&gt;140621361556336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M86.5,-1111.7281C86.5,-1103.0091 86.5,-1090.4699 86.5,-1079.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"90.0001,-1079.6128 86.5,-1069.6128 83.0001,-1079.6129 90.0001,-1079.6128\"/>\n</g>\n<!-- 140621361555664 -->\n<g id=\"node23\" class=\"node\">\n<title>140621361555664</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"81,-1203 0,-1203 0,-1169 81,-1169 81,-1203\"/>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-1189.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-1176.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16, 1, 3, 3)</text>\n</g>\n<!-- 140621361555664&#45;&gt;140621361554096 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140621361555664&#45;&gt;140621361554096</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M52.8272,-1168.9832C58.9107,-1160.5853 66.2742,-1150.4204 72.5621,-1141.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5945,-1143.5204 78.6266,-1133.3687 69.9256,-1139.4138 75.5945,-1143.5204\"/>\n</g>\n<!-- 140621361555552 -->\n<g id=\"node24\" class=\"node\">\n<title>140621361555552</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"167.5,-1203 99.5,-1203 99.5,-1169 167.5,-1169 167.5,-1203\"/>\n<text text-anchor=\"middle\" x=\"133.5\" y=\"-1189.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"133.5\" y=\"-1176.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140621361555552&#45;&gt;140621361554096 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140621361555552&#45;&gt;140621361554096</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M120.9049,-1168.9832C114.6237,-1160.4969 107.0069,-1150.2062 100.5384,-1141.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.3071,-1139.3243 94.5445,-1133.3687 97.6806,-1143.4888 103.3071,-1139.3243\"/>\n</g>\n<!-- 140621361553984 -->\n<g id=\"node25\" class=\"node\">\n<title>140621361553984</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"259.5,-1076 151.5,-1076 151.5,-1042 259.5,-1042 259.5,-1076\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-1062.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm1.weight</text>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-1049.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140621361553984&#45;&gt;140621361555440 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140621361553984&#45;&gt;140621361555440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M205.5,-1041.9832C205.5,-1034.1157 205.5,-1024.6973 205.5,-1016.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0001,-1016.3686 205.5,-1006.3687 202.0001,-1016.3687 209.0001,-1016.3686\"/>\n</g>\n<!-- 140621361554600 -->\n<g id=\"node26\" class=\"node\">\n<title>140621361554600</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"373,-1076 278,-1076 278,-1042 373,-1042 373,-1076\"/>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-1062.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm1.bias</text>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-1049.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140621361554600&#45;&gt;140621361555440 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140621361554600&#45;&gt;140621361555440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M293.3422,-1041.9832C275.1833,-1032.3741 252.6526,-1020.4516 234.9561,-1011.0872\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"236.1563,-1007.7625 225.6804,-1006.1788 232.8822,-1013.9496 236.1563,-1007.7625\"/>\n</g>\n<!-- 140621361554992 -->\n<g id=\"node27\" class=\"node\">\n<title>140621361554992</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"395,-949 314,-949 314,-915 395,-915 395,-949\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-935.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.weight</text>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-922.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32, 16, 3, 3)</text>\n</g>\n<!-- 140621361554992&#45;&gt;140621361556672 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140621361554992&#45;&gt;140621361556672</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-914.9832C354.5,-907.1157 354.5,-897.6973 354.5,-889.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-889.3686 354.5,-879.3687 351.0001,-889.3687 358.0001,-889.3686\"/>\n</g>\n<!-- 140621361554208 -->\n<g id=\"node28\" class=\"node\">\n<title>140621361554208</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"481.5,-949 413.5,-949 413.5,-915 481.5,-915 481.5,-949\"/>\n<text text-anchor=\"middle\" x=\"447.5\" y=\"-935.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.bias</text>\n<text text-anchor=\"middle\" x=\"447.5\" y=\"-922.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140621361554208&#45;&gt;140621361556672 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140621361554208&#45;&gt;140621361556672</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M422.5777,-914.9832C408.8955,-905.641 392.0107,-894.1122 378.4799,-884.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.372,-881.9273 370.1398,-879.1788 376.4248,-887.7082 380.372,-881.9273\"/>\n</g>\n<!-- 140621361556840 -->\n<g id=\"node29\" class=\"node\">\n<title>140621361556840</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"527.5,-822 419.5,-822 419.5,-788 527.5,-788 527.5,-822\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-808.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm2.weight</text>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-795.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140621361556840&#45;&gt;140620970500336 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140621361556840&#45;&gt;140620970500336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-787.9832C473.5,-780.1157 473.5,-770.6973 473.5,-762.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-762.3686 473.5,-752.3687 470.0001,-762.3687 477.0001,-762.3686\"/>\n</g>\n<!-- 140621361557232 -->\n<g id=\"node30\" class=\"node\">\n<title>140621361557232</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"641,-822 546,-822 546,-788 641,-788 641,-822\"/>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-808.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm2.bias</text>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-795.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140621361557232&#45;&gt;140620970500336 -->\n<g id=\"edge29\" class=\"edge\">\n<title>140621361557232&#45;&gt;140620970500336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M561.3422,-787.9832C543.1833,-778.3741 520.6526,-766.4516 502.9561,-757.0872\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"504.1563,-753.7625 493.6804,-752.1788 500.8822,-759.9496 504.1563,-753.7625\"/>\n</g>\n<!-- 140620970502408 -->\n<g id=\"node31\" class=\"node\">\n<title>140620970502408</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"612,-561.5 539,-561.5 539,-540.5 612,-540.5 612,-561.5\"/>\n<text text-anchor=\"middle\" x=\"575.5\" y=\"-547.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140620970502408&#45;&gt;140620970500504 -->\n<g id=\"edge30\" class=\"edge\">\n<title>140620970502408&#45;&gt;140620970500504</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M560.1603,-540.3685C543.6024,-528.8927 517.033,-510.4783 497.8726,-497.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"499.8659,-494.3219 489.6532,-491.5022 495.8784,-500.0752 499.8659,-494.3219\"/>\n</g>\n<!-- 140620970503472 -->\n<g id=\"node32\" class=\"node\">\n<title>140620970503472</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"614,-638 539,-638 539,-604 614,-604 614,-638\"/>\n<text text-anchor=\"middle\" x=\"576.5\" y=\"-624.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"576.5\" y=\"-611.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256, 1568)</text>\n</g>\n<!-- 140620970503472&#45;&gt;140620970502408 -->\n<g id=\"edge31\" class=\"edge\">\n<title>140620970503472&#45;&gt;140620970502408</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M576.2528,-603.6966C576.1152,-594.0634 575.9429,-582.003 575.7979,-571.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"579.2967,-571.7402 575.6542,-561.7913 572.2975,-571.8403 579.2967,-571.7402\"/>\n</g>\n<!-- 140620970502520 -->\n<g id=\"node33\" class=\"node\">\n<title>140620970502520</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"615,-421.5 542,-421.5 542,-400.5 615,-400.5 615,-421.5\"/>\n<text text-anchor=\"middle\" x=\"578.5\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140620970502520&#45;&gt;140620970500616 -->\n<g id=\"edge32\" class=\"edge\">\n<title>140620970502520&#45;&gt;140620970500616</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.8565,-400.3685C545.9707,-388.8927 518.8752,-370.4783 499.3354,-357.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"501.1913,-354.2284 490.9532,-351.5022 497.2567,-360.0179 501.1913,-354.2284\"/>\n</g>\n<!-- 140620970502912 -->\n<g id=\"node34\" class=\"node\">\n<title>140620970502912</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"614,-498 545,-498 545,-464 614,-464 614,-498\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-484.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128, 256)</text>\n</g>\n<!-- 140620970502912&#45;&gt;140620970502520 -->\n<g id=\"edge33\" class=\"edge\">\n<title>140620970502912&#45;&gt;140620970502520</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.2528,-463.6966C579.1152,-454.0634 578.9429,-442.003 578.7979,-431.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"582.2967,-431.7402 578.6542,-421.7913 575.2975,-431.8403 582.2967,-431.7402\"/>\n</g>\n<!-- 140620970501736 -->\n<g id=\"node35\" class=\"node\">\n<title>140620970501736</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"615,-281.5 542,-281.5 542,-260.5 615,-260.5 615,-281.5\"/>\n<text text-anchor=\"middle\" x=\"578.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140620970501736&#45;&gt;140621361656048 -->\n<g id=\"edge34\" class=\"edge\">\n<title>140620970501736&#45;&gt;140621361656048</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.8565,-260.3685C545.9707,-248.8927 518.8752,-230.4783 499.3354,-217.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"501.1913,-214.2284 490.9532,-211.5022 497.2567,-220.0179 501.1913,-214.2284\"/>\n</g>\n<!-- 140620970501120 -->\n<g id=\"node36\" class=\"node\">\n<title>140620970501120</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"613,-358 546,-358 546,-324 613,-324 613,-358\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc4.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-331.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 128)</text>\n</g>\n<!-- 140620970501120&#45;&gt;140620970501736 -->\n<g id=\"edge35\" class=\"edge\">\n<title>140620970501120&#45;&gt;140620970501736</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.2528,-323.6966C579.1152,-314.0634 578.9429,-302.003 578.7979,-291.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"582.2967,-291.7402 578.6542,-281.7913 575.2975,-291.8403 582.2967,-291.7402\"/>\n</g>\n<!-- 140621361657224 -->\n<g id=\"node37\" class=\"node\">\n<title>140621361657224</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"616,-141.5 543,-141.5 543,-120.5 616,-120.5 616,-141.5\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140621361657224&#45;&gt;140621361659352 -->\n<g id=\"edge36\" class=\"edge\">\n<title>140621361657224&#45;&gt;140621361659352</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.0275,-120.2281C545.6519,-110.1325 520.9682,-94.9149 502.3209,-83.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"504.0636,-80.3814 493.7145,-78.1128 500.3901,-86.3401 504.0636,-80.3814\"/>\n</g>\n<!-- 140621361657392 -->\n<g id=\"node38\" class=\"node\">\n<title>140621361657392</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"613,-218 546,-218 546,-184 613,-184 613,-218\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6, 64)</text>\n</g>\n<!-- 140621361657392&#45;&gt;140621361657224 -->\n<g id=\"edge37\" class=\"edge\">\n<title>140621361657392&#45;&gt;140621361657224</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.5,-183.6966C579.5,-174.0634 579.5,-162.003 579.5,-151.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"583.0001,-151.7912 579.5,-141.7913 576.0001,-151.7913 583.0001,-151.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_9CTlb4Yx7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training parameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2\n",
        "n_epochs = 1\n",
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, sampler=train_sampler, num_workers = 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, sampler=test_sampler, num_workers = 2)\n",
        "\n",
        "def train_model(net):\n",
        "    \"\"\" Train a the specified network.\n",
        "\n",
        "        Outputs a tuple with the following four elements\n",
        "        train_hist_x: the x-values (batch number) that the training set was \n",
        "            evaluated on.\n",
        "        train_loss_hist: the loss values for the training set corresponding to\n",
        "            the batch numbers returned in train_hist_x\n",
        "        test_hist_x: the x-values (batch number) that the test set was \n",
        "            evaluated on.\n",
        "        test_loss_hist: the loss values for the test set corresponding to\n",
        "            the batch numbers returned in test_hist_x\n",
        "    \"\"\" \n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 200\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    test_hist_x = []\n",
        "    test_loss_hist = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    # Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "            # Get inputs in right form\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "            \n",
        "            # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Compute the loss and find the loss with respect to each parameter of the model\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            \n",
        "            # Change each parameter with respect to the recently computed loss.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print every 20th batch of an epoch\n",
        "            if (i % print_every) == print_every-1:\n",
        "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "                # Reset running loss and time\n",
        "                train_loss_hist.append(running_loss / print_every)\n",
        "                train_hist_x.append(idx)\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            idx += 1\n",
        "\n",
        "        # At the end of the epoch, do a pass on the test set\n",
        "        total_test_loss = 0\n",
        "        for inputs, labels in test_loader:\n",
        "\n",
        "            # Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            test_outputs = net(inputs)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.data.item()\n",
        "        test_loss_hist.append(total_test_loss / len(test_loader))\n",
        "        test_hist_x.append(idx)\n",
        "        print(\"Validation loss = {:.2f}\".format(\n",
        "            total_test_loss / len(test_loader)))\n",
        "\n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "    return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJtMARZpp_z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6620e976-c808-49f1-fdda-053c58bd8b23"
      },
      "source": [
        "test_set[0]"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   4.,\n",
              "           111., 120.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,   0.,   0.,\n",
              "           217., 240.,   0.,   3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   2.,   0.,\n",
              "           199., 253.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,   2.,  34.,\n",
              "           188., 223.,  21.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,   0.,   0., 232.,\n",
              "           253., 250., 236.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,   4., 225.,\n",
              "           233., 249., 193.,   8.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   4.,   0.,   0., 144.,\n",
              "            81., 138., 115.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   4.,   0.,   2., 170.,\n",
              "            34., 129., 129.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   8., 179.,\n",
              "            17., 100., 151.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   7.,  33., 173.,\n",
              "             2.,  64., 185.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,  68., 153.,\n",
              "             0.,  31., 220.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,  95., 124.,\n",
              "             0.,   3., 218.,  20.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   8., 140., 128.,\n",
              "            35.,  21., 171.,  77.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   4.,   5., 163., 200.,\n",
              "           179., 230., 230., 122.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   7., 203., 175.,\n",
              "             3., 131., 197., 168.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   5.,   0.,   5.,   0.,   0.,   4.,   4.,  42., 172., 178.,\n",
              "           153., 167.,  22., 210.,  10.,   2.,   0.,   2.,   3.,   2.,   3.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  1.,   1.,   0.,   2.,   0.,   0.,   2.,   0., 104., 120.,  91.,\n",
              "           240.,  83.,   8., 167.,  58.,   1.,   0.,   3.,   0.,   0.,   2.,\n",
              "             2.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  2.,   0.,   0.,   7.,   1.,   1.,   2.,   0., 174., 109., 177.,\n",
              "           115., 187.,  31., 104., 137.,   4.,   5.,   3.,   0.,   1.,   1.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  1.,   1.,   2.,   0.,   0.,   0.,   0.,   8., 203., 185.,  61.,\n",
              "             0.,  96., 204.,  36., 171.,  21.,   0.,   0.,   0.,   1.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   4.,   2.,   0.,   0.,   5.,   4.,  74., 156.,  29.,   0.,\n",
              "             0.,   2., 133., 134., 160.,  93.,   0.,   6.,   0.,   0.,   0.,\n",
              "             2.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   2.,   0.,   1.,   8.,   0.,   0., 153.,  97.,   0.,   0.,\n",
              "             3.,   0.,  19., 174., 113., 163.,   4.,   6.,   0.,   2.,   1.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  1.,   0.,   0.,   2.,   0.,   0.,  39., 203.,  10.,   1.,   0.,\n",
              "             0.,   0.,   0.,  56.,  56., 172.,  74.,   0.,   0.,   1.,   3.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   3.,   6.,   0.,  18., 151., 119.,  15., 219.,  72.,\n",
              "             0.,   4.,   5.,   0.,  46., 173., 189.,   1.,   5.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  2.,   0.,   2.,   7.,   0.,  46., 213.,   2.,   0.,  59., 166.,\n",
              "           133.,  10.,  80., 155., 177., 122., 199.,  27.,   0.,   1.,   4.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  3.,   7.,   0.,   0.,   1., 164.,  82.,   0.,   6.,   0.,  40.,\n",
              "           183., 230., 173.,  78.,   7.,   0., 137., 122.,   1.,   0.,   0.,\n",
              "             6.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   2.,  72., 176.,   6.,   3.,   0.,  84., 203.,\n",
              "           171., 158., 210.,  37.,   0.,   0.,  14., 247.,  31.,   0.,   2.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  1.,   4.,   0.,  39., 180.,  63.,  16.,  96., 210., 160.,  53.,\n",
              "            10.,   0.,  89., 168.,  66.,   0.,   0.,  85., 161.,  25.,   0.,\n",
              "             1.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  1.,   3.,  11., 224.,  89.,  90., 168., 141.,  39.,   1.,   0.,\n",
              "             2.,   0.,  10.,  58., 195., 148.,   4.,   1., 133., 161.,  21.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.]]]), 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGyZdjgNjf_h",
        "colab_type": "code",
        "outputId": "cd7c8004-774a-4e74-afcc-fe997fccaef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 200\t train_loss: 0.76 took: 2.03s\n",
            "Epoch 1, Iteration 400\t train_loss: 0.53 took: 1.78s\n",
            "Epoch 1, Iteration 600\t train_loss: 0.45 took: 1.80s\n",
            "Epoch 1, Iteration 800\t train_loss: 0.41 took: 1.78s\n",
            "Epoch 1, Iteration 1000\t train_loss: 0.40 took: 1.81s\n",
            "Epoch 1, Iteration 1200\t train_loss: 0.37 took: 1.77s\n",
            "Epoch 1, Iteration 1400\t train_loss: 0.39 took: 1.77s\n",
            "Epoch 1, Iteration 1600\t train_loss: 0.37 took: 1.76s\n",
            "Epoch 1, Iteration 1800\t train_loss: 0.36 took: 1.75s\n",
            "Epoch 1, Iteration 2000\t train_loss: 0.36 took: 1.77s\n",
            "Epoch 1, Iteration 2200\t train_loss: 0.35 took: 1.75s\n",
            "Epoch 1, Iteration 2400\t train_loss: 0.36 took: 1.75s\n",
            "Epoch 1, Iteration 2600\t train_loss: 0.35 took: 1.74s\n",
            "Epoch 1, Iteration 2800\t train_loss: 0.34 took: 1.76s\n",
            "Epoch 1, Iteration 3000\t train_loss: 0.36 took: 1.76s\n",
            "Epoch 1, Iteration 3200\t train_loss: 0.35 took: 1.78s\n",
            "Epoch 1, Iteration 3400\t train_loss: 0.34 took: 1.80s\n",
            "Epoch 1, Iteration 3600\t train_loss: 0.34 took: 1.78s\n",
            "Epoch 1, Iteration 3800\t train_loss: 0.34 took: 1.76s\n",
            "Epoch 1, Iteration 4000\t train_loss: 0.34 took: 1.78s\n",
            "Epoch 1, Iteration 4200\t train_loss: 0.35 took: 1.76s\n",
            "Epoch 1, Iteration 4400\t train_loss: 0.34 took: 1.76s\n",
            "Epoch 1, Iteration 4600\t train_loss: 0.33 took: 1.76s\n",
            "Epoch 1, Iteration 4800\t train_loss: 0.34 took: 1.75s\n",
            "Epoch 1, Iteration 5000\t train_loss: 0.33 took: 1.76s\n",
            "Epoch 1, Iteration 5200\t train_loss: 0.32 took: 1.79s\n",
            "Epoch 1, Iteration 5400\t train_loss: 0.32 took: 1.76s\n",
            "Epoch 1, Iteration 5600\t train_loss: 0.31 took: 1.75s\n",
            "Epoch 1, Iteration 5800\t train_loss: 0.32 took: 1.78s\n",
            "Epoch 1, Iteration 6000\t train_loss: 0.33 took: 1.73s\n",
            "Epoch 1, Iteration 6200\t train_loss: 0.32 took: 1.74s\n",
            "Epoch 1, Iteration 6400\t train_loss: 0.34 took: 1.75s\n",
            "Epoch 1, Iteration 6600\t train_loss: 0.32 took: 1.73s\n",
            "Epoch 1, Iteration 6800\t train_loss: 0.33 took: 1.74s\n",
            "Epoch 1, Iteration 7000\t train_loss: 0.33 took: 1.80s\n",
            "Epoch 1, Iteration 7200\t train_loss: 0.31 took: 1.75s\n",
            "Epoch 1, Iteration 7400\t train_loss: 0.33 took: 1.78s\n",
            "Epoch 1, Iteration 7600\t train_loss: 0.34 took: 1.82s\n",
            "Epoch 1, Iteration 7800\t train_loss: 0.34 took: 1.82s\n",
            "Epoch 1, Iteration 8000\t train_loss: 0.31 took: 1.78s\n",
            "Epoch 1, Iteration 8200\t train_loss: 0.32 took: 1.78s\n",
            "Epoch 1, Iteration 8400\t train_loss: 0.33 took: 1.82s\n",
            "Epoch 1, Iteration 8600\t train_loss: 0.30 took: 1.81s\n",
            "Epoch 1, Iteration 8800\t train_loss: 0.33 took: 1.78s\n",
            "Epoch 1, Iteration 9000\t train_loss: 0.31 took: 1.73s\n",
            "Epoch 1, Iteration 9200\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 9400\t train_loss: 0.31 took: 1.75s\n",
            "Epoch 1, Iteration 9600\t train_loss: 0.33 took: 1.75s\n",
            "Epoch 1, Iteration 9800\t train_loss: 0.32 took: 1.79s\n",
            "Epoch 1, Iteration 10000\t train_loss: 0.30 took: 1.73s\n",
            "Epoch 1, Iteration 10200\t train_loss: 0.32 took: 1.77s\n",
            "Epoch 1, Iteration 10400\t train_loss: 0.31 took: 1.78s\n",
            "Epoch 1, Iteration 10600\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 10800\t train_loss: 0.31 took: 1.79s\n",
            "Epoch 1, Iteration 11000\t train_loss: 0.31 took: 1.79s\n",
            "Epoch 1, Iteration 11200\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 1, Iteration 11400\t train_loss: 0.32 took: 1.76s\n",
            "Epoch 1, Iteration 11600\t train_loss: 0.33 took: 1.77s\n",
            "Epoch 1, Iteration 11800\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 1, Iteration 12000\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 1, Iteration 12200\t train_loss: 0.31 took: 1.73s\n",
            "Epoch 1, Iteration 12400\t train_loss: 0.33 took: 1.72s\n",
            "Epoch 1, Iteration 12600\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 12800\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 13000\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 13200\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 1, Iteration 13400\t train_loss: 0.32 took: 1.74s\n",
            "Epoch 1, Iteration 13600\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 1, Iteration 13800\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 1, Iteration 14000\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 14200\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 1, Iteration 14400\t train_loss: 0.31 took: 1.79s\n",
            "Epoch 1, Iteration 14600\t train_loss: 0.32 took: 1.79s\n",
            "Epoch 1, Iteration 14800\t train_loss: 0.31 took: 1.77s\n",
            "Epoch 1, Iteration 15000\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 15200\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 15400\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 1, Iteration 15600\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 15800\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 1, Iteration 16000\t train_loss: 0.29 took: 1.72s\n",
            "Epoch 1, Iteration 16200\t train_loss: 0.30 took: 1.73s\n",
            "Epoch 1, Iteration 16400\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 16600\t train_loss: 0.29 took: 1.79s\n",
            "Epoch 1, Iteration 16800\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 17000\t train_loss: 0.29 took: 1.80s\n",
            "Epoch 1, Iteration 17200\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 1, Iteration 17400\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 1, Iteration 17600\t train_loss: 0.32 took: 1.75s\n",
            "Epoch 1, Iteration 17800\t train_loss: 0.31 took: 1.73s\n",
            "Epoch 1, Iteration 18000\t train_loss: 0.31 took: 1.75s\n",
            "Epoch 1, Iteration 18200\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 1, Iteration 18400\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 1, Iteration 18600\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 1, Iteration 18800\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 1, Iteration 19000\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 1, Iteration 19200\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 1, Iteration 19400\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 1, Iteration 19600\t train_loss: 0.30 took: 1.78s\n",
            "Epoch 1, Iteration 19800\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 20000\t train_loss: 0.30 took: 1.72s\n",
            "Epoch 1, Iteration 20200\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 20400\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 1, Iteration 20600\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 1, Iteration 20800\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 1, Iteration 21000\t train_loss: 0.28 took: 1.71s\n",
            "Epoch 1, Iteration 21200\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 1, Iteration 21400\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 1, Iteration 21600\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 21800\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 1, Iteration 22000\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 1, Iteration 22200\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 1, Iteration 22400\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 1, Iteration 22600\t train_loss: 0.28 took: 1.73s\n",
            "Epoch 1, Iteration 22800\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 23000\t train_loss: 0.30 took: 1.74s\n",
            "Validation loss = 0.88\n",
            "Training finished, took 204.12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNSsFzVji6W",
        "colab_type": "code",
        "outputId": "69c66ea5-96ff-4d88-cf2d-94b904994f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "# plt.xlim(20000,120000)\n",
        "# plt.ylim(0,.4)\n",
        "plt.show()\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4VFX+x/H3N50S0mkphE4IhARC\nlyaIoCuIiqCgYGN1dXWbK/rbVWRXl7WtDQsqdmUVV0QFEZXei/SaAIFQQyAhENLP74+5GSZlQoAM\nCZnv63nyMHPnzr3nXpL5zDnnnnPFGINSSikF4FHdBVBKKVVzaCgopZSy01BQSillp6GglFLKTkNB\nKaWUnYaCUkopOw0FpZRSdhoKSiml7DQUlFJK2XlVdwEuVGhoqImOjq7uYiil1BVl3bp1x40xYedb\nz6WhICJDgFcAT+BdY8yUUq83A6YDYcAJYKwxJrWibUZHR7N27VoXlVgppWonEUmpzHouaz4SEU9g\nKjAUaA/cJiLtS632AvCRMSYOmAz8y1XlUUopdX6u7FPoBiQZY/YYY/KAGcDwUuu0B36xHi8o53Wl\nlFKXkStDIRw44PA81VrmaCNwk/V4BOAvIiGlNyQiE0RkrYisTUtLc0lhlVJKVX9H81+A10VkPLAY\nOAgUll7JGDMNmAaQmJhYZq7v/Px8UlNTycnJcW1p1SXz8/MjIiICb2/v6i6KUqocrgyFg0Ckw/MI\na5mdMeYQVk1BROoDNxtjMi50R6mpqfj7+xMdHY2IXEKRlSsZY0hPTyc1NZXmzZtXd3GUUuVwZfPR\nGqC1iDQXER9gNDDbcQURCRWR4jI8ju1KpAuWk5NDSEiIBkINJyKEhIRojU6pGsxloWCMKQAeAuYB\n24EvjDFbRWSyiAyzVusP7BSRXUAj4JmL3Z8GwpVB/5+Uqtlc2qdgjJkDzCm17EmHxzOBma4sg1JK\nqcrTaS6qQEZGBm+88cZFvfe6664jI6Py3SiTJk3ihRdeuKh9KaXU+WgoVIGKQqGgoKDC986ZM4fA\nwEBXFEsppS6YhkIVmDhxIsnJycTHx/Poo4+ycOFC+vTpw7Bhw2jf3jaI+8Ybb6RLly7ExsYybdo0\n+3ujo6M5fvw4+/btIyYmhvvuu4/Y2FgGDx7M2bNnK9zvhg0b6NGjB3FxcYwYMYKTJ08C8Oqrr9K+\nfXvi4uIYPXo0AIsWLSI+Pp74+HgSEhLIyspy0dlQSl3JqnucQpV7+tutbDt0qkq32b5pA566Idbp\n61OmTGHLli1s2LABgIULF7J+/Xq2bNliv/Ry+vTpBAcHc/bsWbp27crNN99MSEjJcXq7d+/m888/\n55133uHWW2/lq6++YuzYsU73e+edd/Laa6/Rr18/nnzySZ5++mlefvllpkyZwt69e/H19bU3Tb3w\nwgtMnTqV3r17c/r0afz8/C71tCilaiGtKbhIt27dSlyL/+qrr9KpUyd69OjBgQMH2L17d5n3NG/e\nnPj4eAC6dOnCvn37nG4/MzOTjIwM+vXrB8C4ceNYvHgxAHFxcYwZM4ZPPvkELy9b7vfu3Zs//elP\nvPrqq2RkZNiXK6WUo1r3yVDRN/rLqV69evbHCxcu5KeffmLFihXUrVuX/v37l3utvq+vr/2xp6fn\neZuPnPn+++9ZvHgx3377Lc888wybN29m4sSJXH/99cyZM4fevXszb9482rVrd1HbV0rVXlpTqAL+\n/v4VttFnZmYSFBRE3bp12bFjBytXrrzkfQYEBBAUFMSSJUsA+Pjjj+nXrx9FRUUcOHCAAQMG8O9/\n/5vMzExOnz5NcnIyHTt25LHHHqNr167s2LHjksuglKp9al1NoTqEhITQu3dvOnTowNChQ7n++utL\nvD5kyBDeeustYmJiaNu2LT169KiS/X744Yfcf//9ZGdn06JFC95//30KCwsZO3YsmZmZGGN4+OGH\nCQwM5O9//zsLFizAw8OD2NhYhg4dWiVlUErVLmJMmfnlarTExERT+iY727dvJyYmpppKpC6U/n8p\ndfmJyDpjTOL51tPmI6WUUnYaCkoppew0FJRSStlpKCillLLTUFBKKWWnoaCUUspOQ6Ga1K9fH4BD\nhw5xyy23lLtO//79KX35bWkvv/wy2dnZ9ucXOhW3MzpFt1LuSUOhmjVt2pSZMy/+PkOlQ0Gn4lZK\nXQoNhSowceJEpk6dan9e/C379OnTDBw4kM6dO9OxY0e++eabMu/dt28fHTp0AODs2bOMHj2amJgY\nRowYUWLuowceeIDExERiY2N56qmnANske4cOHWLAgAEMGDAAODcVN8BLL71Ehw4d6NChAy+//LJ9\nfzpFt1LKmdo3zcXciXBkc9Vus3FHGDrF6cujRo3iD3/4Aw8++CAAX3zxBfPmzcPPz4+vv/6aBg0a\ncPz4cXr06MGwYcOc3qf4zTffpG7dumzfvp1NmzbRuXNn+2vPPPMMwcHBFBYWMnDgQDZt2sTDDz/M\nSy+9xIIFCwgNDS2xrXXr1vH++++zatUqjDF0796dfv36ERQUpFN0K6Wc0ppCFUhISODYsWMcOnSI\njRs3EhQURGRkJMYYnnjiCeLi4hg0aBAHDx7k6NGjTrezePFi+4dzXFwccXFx9te++OILOnfuTEJC\nAlu3bmXbtm0Vlmnp0qWMGDGCevXqUb9+fW666Sb75Hk6RbdSypna9xdbwTd6Vxo5ciQzZ87kyJEj\njBo1CoBPP/2UtLQ01q1bh7e3N9HR0eVOmX0+e/fu5YUXXmDNmjUEBQUxfvz4i9pOMZ2iWynljNYU\nqsioUaOYMWMGM2fOZOTIkYDtW3bDhg3x9vZmwYIFpKSkVLiNvn378tlnnwGwZcsWNm3aBMCpU6eo\nV68eAQEBHD16lLlz59rf42za7j59+jBr1iyys7M5c+YMX3/9NX369Lng49IpupVyL7WvplBNYmNj\nycrKIjw8nCZNmgAwZswYbrjhBjp27EhiYuJ5vzE/8MAD3HXXXcTExBATE0OXLl0A6NSpEwkJCbRr\n147IyEh69+5tf8+ECRMYMmQITZs2ZcGCBfblnTt3Zvz48XTr1g2Ae++9l4SEhAqbipzRKbqVch8u\nnTpbRIYArwCewLvGmCmlXo8CPgQCrXUmGmPmVLRNnTr7yqf/X0pdftU+dbaIeAJTgaFAe+A2EWlf\narW/AV8YYxKA0cAbriqPUkqp83Nln0I3IMkYs8cYkwfMAIaXWscADazHAcAhF5ZHKaXUebiyTyEc\nOODwPBXoXmqdScCPIvJ7oB4w6GJ3Zoxxev2/qjmutDv9KeVuqvvqo9uAD4wxEcB1wMciUqZMIjJB\nRNaKyNq0tLQyG/Hz8yM9PV0/cGo4Ywzp6ek6oE2pGsyVNYWDQKTD8whrmaN7gCEAxpgVIuIHhALH\nHFcyxkwDpoGto7n0jiIiIkhNTaW8wFA1i5+fHxEREdVdDKWUE64MhTVAaxFpji0MRgO3l1pnPzAQ\n+EBEYgA/4II/2b29vWnevPklFlcppZTLmo+MMQXAQ8A8YDu2q4y2ishkERlmrfZn4D4R2Qh8Dow3\n2gaklFLVxqWD16wxB3NKLXvS4fE2oHfp9ymllKoe1d3RrJRSqgbRUFBKKWWnoaCUUspOQ0EppZSd\nhoJSSik7DQWllFJ2bhMKHyzbS9ykeeTkF1Z3UZRSqsZym1AoNHAqp4Dc/KLqLopSStVYbhMKft62\nQ80t0JqCUko54z6h4OUJQI7WFJRSyin3CQVvKxS0pqCUUk65USjYDlU7mpVSyjm3CQVfbT5SSqnz\ncptQ0I5mpZQ6PzcKBa0pKKXU+bhRKGifglJKnY/bhMK5PgUNBaWUcsZ9QqG4plCgzUdKKeWM24RC\ncZ9CrtYUlFLKKfcJBav5KFdrCkop5ZTbhIK3p+Ah2qeglFIVcZtQEBF8vTw1FJRSqgJuEwpguyxV\nxykopZRzbhYKWlNQSqmKuDQURGSIiOwUkSQRmVjO6/8RkQ3Wzy4RyXBlefy8PbWjWSmlKuDlqg2L\niCcwFbgGSAXWiMhsY8y24nWMMX90WP/3QIKrygPg6+WhNQWllKqAK2sK3YAkY8weY0weMAMYXsH6\ntwGfu7A8+Hp76uA1pZSqgCtDIRw44PA81VpWhog0A5oDvzh5fYKIrBWRtWlpaRddID+tKSilVIVq\nSkfzaGCmMabcT2xjzDRjTKIxJjEsLOyid+Ln7akjmpVSqgKuDIWDQKTD8whrWXlG4+KmI7Bdkqod\nzUop5ZwrQ2EN0FpEmouID7YP/tmlVxKRdkAQsMKFZQHQwWtKKXUeLgsFY0wB8BAwD9gOfGGM2Soi\nk0VkmMOqo4EZxhjjqrIU08FrSilVMZddkgpgjJkDzCm17MlSzye5sgyO/Lw9ydHbcSqllFM1paP5\nstARzUopVTH3CgUvW0fzZWipUkqpK5JbhYKvtyfGQF6h9isopVR53CsUvKxbcmpns1JKlcutQkFv\nyamUUhVzy1DQmoJSSpXPzULBdri5elmqUkqVy61CwddLawpKKVURtwqF4pqCDmBTSqnyuVkoFNcU\nNBSUUqo87hUK2nyklFIVcqtQ8NWOZqWUqpBbhYLWFJRSqmLuFQrFHc3ap6CUUuVyq1Dw1Y5mpZSq\nkFuFwrnBa9p8pJRS5XGrUPDx9EBE5z5SSiln3CoURARfLw9ytKaglFLlcqtQAL37mlJKVcT9QsFL\nQ0EppZxxu1Dw9fbQcQpKKeWE24WCn5enjmhWSikn3C8UtKaglFJOuTQURGSIiOwUkSQRmehknVtF\nZJuIbBWRz1xZHrANYNM+BaWUKp+XqzYsIp7AVOAaIBVYIyKzjTHbHNZpDTwO9DbGnBSRhq4qTzE/\nb08yz+a7ejdKKXVFqlRNQURaioiv9bi/iDwsIoHneVs3IMkYs8cYkwfMAIaXWuc+YKox5iSAMebY\nhRX/wvl6eejgNaWUcqKyzUdfAYUi0gqYBkQC52vqCQcOODxPtZY5agO0EZFlIrJSRIZUsjwXzc/b\nU6e5UEopJyrbfFRkjCkQkRHAa8aY10Tk1yraf2ugPxABLBaRjsaYDMeVRGQCMAEgKirqknbo5+Wh\nfQpKKeVEZWsK+SJyGzAO+M5a5n2e9xzEVqMoFmEtc5QKzDbG5Btj9gK7sIVECcaYacaYRGNMYlhY\nWCWLXD4d0ayUUs5VNhTuAnoCzxhj9opIc+Dj87xnDdBaRJqLiA8wGphdap1Z2GoJiEgotuakPZUs\n00XRS1KVUsq5SjUfWVcMPQwgIkGAvzHm3+d5T4GIPATMAzyB6caYrSIyGVhrjJltvTZYRLYBhcCj\nxpj0iz+c8/P18iSnoBBjDCLiyl0ppdQVp1KhICILgWHW+uuAYyKyzBjzp4reZ4yZA8wptexJh8cG\n+JP1c1n4eXtgDOQXGny8NBSUUspRZZuPAowxp4CbgI+MMd2BQa4rluv4Fd99Tae6UEqpMiobCl4i\n0gS4lXMdzVckvSWnUko5V9lQmIyt/T/ZGLNGRFoAu11XLNfx9bJuyamdzUopVUZlO5q/BL50eL4H\nuNlVhXIlP60pKKWUU5Wd5iJCRL4WkWPWz1ciEuHqwrmCn1VT0MtSlVKqrMo2H72PbYxBU+vnW2vZ\nFae4pqD3VFBKqbIqGwphxpj3jTEF1s8HwKUNLa4m55qPtKaglFKlVTYU0kVkrIh4Wj9jAZcOMnMV\nX3vzkdYUlFKqtMqGwt3YLkc9AhwGbgHGu6hMLqXjFJRSyrlKhYIxJsUYM8wYE2aMaWiMuZEr9uoj\n7WhWSilnLuV2nJdtaoqqpB3NSinl3KWEwhU5cZCfl3Y0K6WUM5cSCqbKSnEZ+XprR7NSSjlT4Yhm\nEcmi/A9/Aeq4pEQudm6aCw0FpZQqrcJQMMb4X66CXC4igq+XBzl6n2allCrjUpqPrlh+3p5aU1BK\nqXK4ZSj4euktOZVSqjxuGQp+3p46eE0ppcrhpqHgoVcfKaVUOdwyFALqeJORnV/dxVBKqRrHLUOh\ncUAdDmfmVHcxlFKqxnHLUGga4MeRzByMuSLH3ymllMu4ZSg0DvAjr7CIE2fyqrsoSilVo7hlKDQJ\n8APQJiSllCrFpaEgIkNEZKeIJInIxHJeHy8iaSKywfq515XlKdY4wDZDxxENBaWUKqHCaS4uhYh4\nAlOBa4BUYI2IzDbGbCu16n+NMQ+5qhzlsdcUTmkoKKWUI1fWFLoBScaYPcaYPGAGMNyF+6u00Pq+\neHoIRzLPVndRlFKqRnFlKIQDBxyep1rLSrtZRDaJyEwRiSxvQyIyQUTWisjatLS0Sy6Yp4fQyN9X\n+xSUUqqU6u5o/haINsbEAfOBD8tbyRgzzRiTaIxJDAsLq5IdNw7w43CGhoJSSjlyZSgcBBy/+UdY\ny+yMMenGmFzr6btAFxeWp4QmgXU4on0KSilVgitDYQ3QWkSai4gPMBqY7biCiDRxeDoM2O7C8pTQ\npIEfhzPP6gA2pZRy4LKrj4wxBSLyEDAP8ASmG2O2ishkYK0xZjbwsIgMAwqAE8B4V5WntMYBfuTk\nF5F5Np/Auj6Xa7dKKVWjuSwUAIwxc4A5pZY96fD4ceBxV5bBmSbWWIXDmTkaCkopZanujuZq09ga\nq6AD2JRS6hy3DQWd6kIppcpy21AI8/fFQ9ABbEop5cBtQ8Hb04MwHcCmlFIluG0ogK2zWUNBKaXO\ncfNQsI1VUEopZePWodA4wI/Degc2pZSyc+tQaBLgR3ZeIVm5BdVdFKWUqhHcOhT0ZjtKKVWSW4eC\njlVQSqmS3DoUmgXXBSDp2OlqLolSStUMbh0KDRv40biBHxsPZFR3UZRSqkZw61AAiI8MZGOqhoJS\nSoGGAp0iA0lJz+bEmbzqLopSSlU7tw+F+MhAAK0tKKUUGgrERQTgIbBhv4aCUkq5fSjU8/WidUN/\nrSkopRQaCoDV2XwgQ6e7UEq5PQ0FbJ3NJ7PzSUnPru6iKKVUtdJQQDublVKqmIYC0KZRfep4e/Kr\ndjYrpdychgLg5elBx/AArSkopdyehoIlISqQrQdP6YypSim35tJQEJEhIrJTRJJEZGIF690sIkZE\nEl1ZnoqM6d4MBKbM3V5dRVBKqWrnslAQEU9gKjAUaA/cJiLty1nPH3gEWOWqslRGVEhdJvRpwawN\nh1iz70R1FkUppaqNK2sK3YAkY8weY0weMAMYXs56/wD+DVR7u83vBrSkSYAfT32zlcIiHbOglHI/\nrgyFcOCAw/NUa5mdiHQGIo0x37uwHJVW18eLJ66LYdvhU8xYs7+6i6OUUpddtXU0i4gH8BLw50qs\nO0FE1orI2rS0NJeW6zdxTWjfpAFfrz9Y5jUd8ayUqu1cGQoHgUiH5xHWsmL+QAdgoYjsA3oAs8vr\nbDbGTDPGJBpjEsPCwlxYZBARrmodyqbUTHLyC+3Lpy1OZsALC8kvLHLp/pVSqjq5MhTWAK1FpLmI\n+ACjgdnFLxpjMo0xocaYaGNMNLASGGaMWevCMlVKt+hg8gqL2OBwR7bZGw+xLz2bZUnHq7FkSinl\nWi4LBWNMAfAQMA/YDnxhjNkqIpNFZJir9lsVEqODAFiz13YVUlpWLlsOngJg9oZD1VYupZRyNS9X\nbtwYMweYU2rZk07W7e/KslyIwLo+tGvsz2rr0tQlu239GHERAczbeoSzeYXU8fGsziIqpZRL6Ihm\nJ7pGB7M+5SQFhUUs3pVGSD0f/nptO87kFfLLjmPVXTyllHIJDQUnujUP5kxeIVsOnWLx7uP0bRNG\nz5YhNPT35ZsNZa9MUkqp2kBDwYluzYMBeH/ZXk6cyaNvm1A8PYQbOjVl4c40MrPzq7mESilV9TQU\nnGjUwI9mIXWZvdHWsdynte1S2OHxTckrLGLOlsPVWTyllHIJDYUKdI0OxhjoGB5AaH1fwPa4XWN/\npszdwY4jp6q5hEopVbU0FCpQ3ITUt02ofZmI8M6difh5e3DHe6vZr7fwVErVIhoKFejfNowO4Q0Y\nHl9iyiYig+vyyT3dyS8sYsx7K0k6dvqi91GgI6SVUjWIhkIFGvr78d3v+9CmkX+Z11o38ufDu7qR\nlVPA9a8u4YNleym6wJlVf95+lLinf+S7TTogTilVM2goXIJOkYH8+Ie+9GoZwqRvtzFq2gqW7E7D\nGENBYRHztx1l0uytzN18uMQ8SmCbXO/FH3eRnVfIIzM2aDAopWoEl45odgcNG/gxfXxX/rvmAC/N\n38Ud760mpkkDTpzJ5eipXDw9hA+W76Oejye3d4/iietiEBEW7kxj2+FTTLqhPd9vPswjMzYgCNfH\nNanuQ1JKuTENhSogIozuFsWIzuHM+vUgn6zcT/smDfjH8Cj6tgljXcpJZqw5wDtL9lLP14tHBrbm\n9QVJhAfWYUyPZoxMjGTc9NX86YsNtG1cn1YNyzZXKaXU5aChUIV8vTwZ1TWKUV2jSizv3SqUXi1D\n8PH04OWfdpORnc+6lJNMHh6Lt6cH3p4evDG2M0NeXsLvP9/ArAd74et1/rmV1qWcZNGuNG5KCCc6\ntJ6rDksp5Ua0T+EyERGevakDXaOD+GD5PkLr+3Jr4rnbTTT09+O5m+PYfvgUL8zbWeG2cvILeeb7\nbdzy1nJe/Xk3A19axF9nbuRQxllXHwYAyWkXf7WVUqpm05rCZeTr5clbY7vw24/XcVu3KPy8S9YG\nBrVvxNgeUbyzZC+eHh4Mjm1EbNMGJB87w6bUDHYdPc3+E2fYcvAUR07lMKZ7FPf2acFHK/bx6ar9\nrNp7gh8e6evSGVyXJx3n9ndX8f5dXRnQtqHL9qOUqh5ypd1iMjEx0axdW+334XGZs3mFPPjZehbu\nPEaRAREo/i+q4+1Js5C6RIfU446ezejd6tyguuXJx7n9nVX8tl8LHh8aA8D2w6f4YcsRbu8eRaMG\nflVSvsf/t5nPV+/n5s4RvHhrp4vaRmZ2PvO3H+WHLUdoGVaPx6+LqZKyKaWcE5F1xpgyd7YsTWsK\nNUwdH0+mj+9KRnYei3cfZ8fhU7Rt7E+niECahdRFRMp9X6+WoYzuGsm7S/ZyQ1xTzuQWcO+Ha8nK\nLeCtRcmM7xXNA/1bEljX56LLVlhkmL/tCAA/7zhKfmER3p6Vb4EsKjK8uSiZl3/aRX6hwcfTg8W7\n0vjDoDZ6fwqlagjtU6ihAuv6MKxTU/46pB3D420dyc4CodjjQ2MIrufD7z5dz53TV9OwgS9f3t+T\n6+OaMG3JHu6cvvqSRlCv3XeC46fzGB7flIzsfFZbd6YrZoxh99Es5m4+XGYgX/rpXMZ/sIbn5+1k\ncGxjvnmwN2/f0YW8wiLWpZy86DJVpKCwiJpWE/5x6xE+XL6vuouhlFMaCrVIQF1v/jE8lv0nsmnX\n2J8v7+9F1+hgXro1ntduS2BTaibvL9tX6e0ZYzibd27Q3Q9bj+Dj5cGTv2lPHW9PfthyxL7e8/N2\n0PNfv3DNfxbzwKfr+W7zuVlkcwsKGfHGclbuSeeZER14/bYEOkUG0q15MF4ewrLk8u97nZJ+hr3H\nz5RY9uv+k4ybvpqsnPNPXX73h2sZ+94q8gpqzlQiU37YweTvtnE48/JcFKDUhdJQqGWGdGjCzPt7\n8vmEHgTXO9dUdH3HJlzTvhEvzt9JSvqZCrZgY4zhj//dQM8pP5N07DTGGOZtOULf1mGE1PelX5sw\n5m09QlGR4ZNV+5m6IJmYJv5MuakjzULq8vGKffZtzd5wiP0nsnlzTGfGdG9mr/HU8/UiPjKQ5cnp\nZfa/LOk4Q19Zwj0frimxfOa6VBbtSjtvuKWkn2HxrjSWJaXz1Oyt5z3ei1FYZNh4IIM3Fibx2MxN\n5w2q5LTT7Ek7Q2GR4eMVKS4pk1KXSkOhFkqMDqauT8nuIhHhH8M74O3hwRNfb+ZUTj5n8wopdDJf\n03tL9zJrwyGy8wq5+4M1LNqVxqHMHIZ0aAzAkA6NOZaVy5frDvDP77bRt00Y743ryuhuUYzt3ow1\n+06y/fApjDG8t3QvbRv5c3W7slcr9WoVyubUDDLPnvtA/WHLEe56fw1FxrAn7QwHTpybibY4QN5Z\nsqfCGx19t8lWU7m5cwSfr97PJyvLfggbYy66eckYw81vLmf41GU898NO/rv2AAt3plX4nvnbjgKQ\nEBXI56v3l5n6RKmaQEPBjTQO8OOvQ9uxLCmduEk/EvPkD3R/9meW7i7ZfLNyTzr/mruDa2Mb8fl9\nPThyKocJH6/Dy0MYFGP7YB/QriHensLE/22mvq8XL4yMw8PDVgO4pUsEvl4efLwyheXJ6ew4ksU9\nfZqX2yfSq2UIRQZW7bF92C/alcaDn60nNrwBn97bHYAlVvkOZpxl7/Ez3JoYQVZOAe8s2eP0WL/d\neIjEZkE8d0sc/duGMWn2VjanZtpfN8Yw8q0VPDpz00Wdyx1HsthwIIP7+7Vk5eMD8fXy4Nf9GRW+\nZ/62o3QIb8Bfr23Hyex8Zv1audu6FhYZ9tTisSE1rd/H3WkouJkx3aJ4/fYE/nZ9DI8NaUdQXW/u\nnL6KtxYlk3QsizcWJvHgp+tpFlKXF0Z2okuzIF4Y2Ym8giJ6tAixX70UUMebXi1DMQaeuyWOhv7n\nLnkNqufDDZ2aMuvXg7zy825C6/syPL5pueVJiArEz9uD5cnpnM0r5In/baZ5aD0+uac7naOCaBrg\nx5Ldtm/gy5Js4XD3Vc25Pq4J05ftJf10bplt7jqaxY4jWdzQqSmeHsIroxOo4+PJNIcQWb8/g7Up\nJ5m5LpXlTvo0KvLLjmO2svSOpnGAH3ERAfx6wHmHeVpWLuv3n+SamMb0aBFMu8b+vL9sX6U+EN9e\nnMyglxZd9E2d0k/nlrkooCaZ+NVmbpu2kjO5BdVdFIWGgtvx8BB+E9eUe/u04IH+LZn1YG+GdmjC\nlLk7GPTSYp77YScRwXWZdkcX/P28ARjWqSnv39WVf97YocS2nrguhldGxzMwplGZ/dzZsxnZeYWs\n3nuCO3s2czpth6+XJ12jg1mefJxXf9nNwYyzPHNjB+r5eiEi9GkdxrKk4xQUFrE86Tih9X1o28if\nPw5qTU5+IW8uTC6zze82HsJDYGhHW1NXQB1vRnaJ5IcthzmWlQPAZ6v2U9/Xi8jgOjz1zVbyy7kq\nKye/kIzsPHILCst8eC/ceYw+dA9qAAAWYklEQVQO4Q1oaI3/SIgKYuvBU+QWlN8k9PP2oxgD17Rv\nhIhwd+/m7DyaVW5/iqOCwiI+Wp5CkYF3l+ytcN3y/LTtKIP/s5hb317BS/N31bhv5VsOZvLftQdY\nsSedBz9bX+7/g7q8NBTcXD1fL16/PYH/jOrEP4bHsuLxq/nmwd5lJuUb0LZhmfmV2jb2L3MDomJx\nEYF0igjAx8uDMd2jyl2nWK+Woew6epp3Fu9hZJcIurcIsb/Wp00op3IK2JiaybLkdHq2DEVEaNXQ\nnxsTwvloZUqJ6T2MMXy76TA9W4aUqL2M7RFFfqHhv6sPkJmdz3ebDjE8vimTbohl97HTfODQcZ2d\nV8DUBUl0e+Yn4ifPp+3ffiDhH/PZlGprHsrIzmNdyskSI7oTIgPJKyxi26Hyv83P33aU8MA6xDSx\nnddh8U1p1MCXZ+dsr/Ay4fnbjnLkVA5tG/nzzYaDHDuVU+G5LJZXUMQTX2/m3o/W0qiBH8Pjm/Lq\nz7v5v1lbnPYjVYeXf9pNAz8vnriuHQt3pvHE/zbXuOByNy4dvCYiQ4BXAE/gXWPMlFKv3w88CBQC\np4EJxphtriyTKktEGJEQUeXbffHWThzJzCXEur+1M71a2kLA38+rzOjm3i1DEYH3l+0lLSuX3i3P\nBcYfB7Xhu42HeeWn3fz7ljgAth46xd7jZ/ht3xYlttMirD59Wofy2er9+Hl7kltQxO3do4htGsDA\ndg15+addbD2USW5BEWtTTpKWlcugmIb0ahnK2fxC3l+2l2fnbOfz+3qwePdxioytX6VY52ZBAPy6\nP4OEqKAS+87OK2Bp0nFu6xZl71fx8/Zk0g2xPPDpej5Yvo97+9jKm19YRJEx9prVB8v3ERFUhzfH\ndmbgS4v4cMU+Hr22XYXns6jI8JcvNzJ74yF+27cFfx7cFm9PITywDm8sTGbtvhMMiW3M4NjGdAgP\nqHBbrrQpNYOfth/lz9e0YULflpzOLeTVn3fz47ajtAirR6eIQP7+m/Z4elQ8Pqeq7Dhyikc+38DU\nMZ1p1bD+ZdlnTeSymoKIeAJTgaFAe+A2EWlfarXPjDEdjTHxwHPAS64qj7r8WjX056rWoeddr0N4\nAP3ahPHsiI4lLqMFW/9EXHiA/Woix6k9IoPrMqZHFF+uO0DSsSxOnsnj0ZmbqOPtybWxjcvs544e\nzTicmcOL83fSKTKQ2Ka2D8RJw2JpEVaf9fszSDp2mtimDZh5f0/eHdeVu69qzoMDWvG7/q1YuecE\ny5LSWbjjGEF1vekUEWjfdqMGfjQN8OPXA2U7m7/deIjcgiIGty/ZzDakQ2MGtmvIiz/uIvVkNiuS\n07n6xYUMeH4hm1Mz2XHkFKv2nuCOHs1oEVafwe0b8cnK/WTnlW17Lx6LYYzhqdlbmb3xEI8Nacfj\n18Xg4+WBiPDXIe146dZOBNbx4fUFSfzmtaU8P29HtX0z/8/8XQTW9WZ872gA/jioNc/fEsdv4ppQ\nVGT4YPk+NlTQT1OVCosME7/azM6jWcxcl1ql216//ySnKjGupjxn8wp5d8meyzbZJbi2ptANSDLG\n7AEQkRnAcMBeEzDGONa16wFab3RDnh7Ch3d3c/p6n9ZhbEzNJDK4DpHBdUu89tCAVny5NpXJ320n\nLSuX5LTTvHtnIkH1yk7nMTCmEeGBdTiYcZYx3c41aUUG1+Xb319VYRlv7x7FO0v28PyPOzlwIpt+\nbcLKfINNiAri1/0lP8Qys/N57oeddI4KpIdDsxjYamiTb+zANS8tYvS0laSePEt0SF2KBG55aznt\nmjTA18vDPpvufX1aMG/rUWauS+XOntH27Xy8MoW/z9pCkwA/Gvr7sjE1k9/2tfUZlXZT5whu6hzB\niTN5PPfDDqYuSKawCB4b0va8I+ZLS0k/Q3hgHbwuYKqTYgt3HmPBzjT+OqStve9KRBiZGMnIxEhO\nnMmj8z/msywpnS7Ngi94+xfq01UpbDiQQXA9H+ZuOXxR56M8u45mcfOby7mtWxTPjuhY6ffl5Bfy\n6ar9vLkwmeOnc/EQ4e6rml9yeSrDlX0K4cABh+ep1rISRORBEUnGVlN42IXlUVeoPlZto3fLsrWO\nkPq+3NenBYt3pdkDoW+bsHK34+khTOjbgvDAOvym04Xd4c7P25OHB7Zm44EMTpzJK9F0VCwhKpDU\nk2ftndkAL87fycnsPCYP72C/ZNdReGAdHr22LaknzzK+VzRzH+nL7Id6kxAVyMYDGQyPb2oPuC7N\ngkiICuTtRXvsHdr5hUW8uSCJNo3q09MKnQf6t2Ti0IqbmILr+fDsiI6M7RHFW4uSeeb77SWmJjl5\nJo/3lu7ltJMrgj5asY9+zy8kfvJ87v5gDZ+t2l+iTK/8tJvuz/7E9KV7y/Rh/Lj1CBM+XkfbRv6M\ncwi30uVr36SB/YqzqpJ5Np+PV+zjN68tIfGfPzF1QRJJx07z3A876dM6lEevbUtKejbbDl/clV6l\n/Wf+LoyBb3496PRclmaM4Z4P1/CP77bRumF9vvhtz8sWCFADJsQzxkwFporI7cDfgHGl1xGRCcAE\ngKioijstVe3TuVkQw+ObMrpb+f/39/Zpzr70M9zcOeK8zVXjekUzrlf0RZXjli4RvLUomQMnsunb\numzwJETZmpM27M9gcGxjthzM5JOVKdzRo1mFbfd39W7OiIRw++W+dXw8+fie7nz960EGOoSPiPCH\nQW0YN301X6w5wB09o5mz+TCHMnN478bEcq8Cq4iHh21Ao6cI7y7dy770bP4zqhOHMnK476O17D+R\nTdKx0/zrppLfcLcczOSf322nZ4sQWjasx/KkdJ74ejNTFyRxb5/mzPr1IBtTM2kRWo/J323jm42H\n+F3/lvh4erDn+BmenbOdDuEBfHhXV+r5Ov8I6t0qhA+Xp3A2r7BKJkzcn57N9a8tISungPZNGhDT\nxJ/n5+3k+Xk78fXy4J83dsDfz5u/zdrC3M1H7M2LlfXpqhTeWJDMi7d2okeLELYeymTuliNc3a4h\nv+w4xjcbDjKme7PzbmdFcjrLktJ5fGg7ftuvbG3P1Vw2dbaI9AQmGWOutZ4/DmCM+ZeT9T2Ak8aY\nCv8navvU2apmW5diG6k9tkfZP+6c/EI6TprHPVe1oF+bMJ7+divHT+fy85/7E1DHu0r2b4xh1Nsr\n2Zd+hkWPDuDmN5eTV1jEj3/oW25NpLLb/GhFCpO/20ZkUB2OZeVS39eLrtHBfL/5MJ/f14OeVgf/\n6dwCbnhtKWfzCpnzSB+C6/lgjGFp0nFemLeTjamZBNb15p83duD6jk2YvfEQk7/dRvqZPPv+erYI\n4Z1xidSvIBAAFuw8xl3vr+Gju7s5rf1diH/N2c57S/fyxf096WxdDLDxQAbvLNlD3zZh9ma6Me+u\n5HBGDj//uV+lmpCMMbzw406mLkjGx8sDbw/h0/t68Povu1m99wRLHrua0dNWIsD3D1913m3e+vYK\nUqz/39L3XLkUNWHq7DVAaxFpDhwERgO3O64gIq2NMbutp9cDu1GqBuvSLIguzYLKfc3P25P2TQOY\ntjiZtxYlE1jXmxdu6VRlgQC22sKfB7dh1LSV/P7z9Ww7fIp/39zxogOheJvjekXTtrE/D322njaN\n/Hn7ji408PNmy6FMHv/fJuY+0peT2Xk8NXsrKelnmDGhp/2igOLxJFe1CmXFnnRaNaxvvxx4eHw4\nV7dryO5jpxHAy8ODmCb+leqH6BZ9bsLEvm3CKCoyzNpwkAFtG5boMzqceRYfT48Kr3LLKyhi5rpU\nBsY0tAcCQKfIQF6/vXOJdYd2aMLfZm1h19HTtG1c8f3Ss3Ly+fusLczacIjbukXy0NWtGT1tBXe8\nu4qs3AL+MrgNAXW8ub17FH+ftYWNqZnERwY63d6K5HRW7z3BpBvaV2kgXAiXhYIxpkBEHgLmYbsk\ndboxZquITAbWGmNmAw+JyCAgHzhJOU1HSl1JbukcjpeHMCoxkmHxTV3yh929RQh9Wofy0/Zj1mjx\n8seKXKgeLUJY+tjV+Hh62EPmXzd15PZ3VnHj1GUkWVNtPHFdDN2al+38FRF6ldPv4+/nXeKDuLLq\n+XqREBXI8iTbAL+PV6bw1OytXN+xCVPH2D7Is3LyGfb6Mnw8Pfj+4auc3i/kp+1HST+T57QJ0tG1\nsY35+zdb+Gp9KrFNG/DjtqNEBtXlzp7NaBpYx77eLzuO8n9fb+HoqRz+fE0bHrq6FSLCp/f0YOTb\ny/HyFMb3tvUF3BjflH/N2c5nq1IqDIWXf9pFQ3/fSpXTVVzap2CMmQPMKbXsSYfHj7hy/0pdbnf0\njOYOJ52nVekvg9uyNOk4d/WOrtLgKb2tXi1DGdezGf9bf5B7rmrOuF7RhDt8MLpar5ahvPrLbjal\nZjBl7g78/bz4fvNhxu87QdfoYF79eTfHT+fi5SH8+YuNvHNnYrm1ps9X7yc8sE65fUGlhfn70i06\nmGmLbdOihNb3Ye6Zw7ZmptahFBQZDmacZU/aGdo0qs+bY3uX+KCPCqnL9w/3ITu30N5E5u/nzfD4\npvxv/UEKigytGtbnqlahxDlc1jzr14Os2nuCp6qxlgB6O06lrlhJx07TPLSeywd32WaT5ZKaqC7W\n6r0nuPXtFQTX8yG/oIhZD/Xm9ndW0riBH8+P7MR1ryzhli4RtGvsz6Rvt5XbOXvgRDZ9n1/AIwNb\n84dBbSq137X7TjBv6xGuad+YxGZBHMo8y0crUpi39QiBdbxpElCHhKhAxveOdjqFS2mpJ7P5+6wt\nbD+cxRFrZPqIhHD+dE0bpi3ew8crU+gUGch/J/RwSShUtk9BQ0EpVWPlFRTR6ekfOZtfyJSbOjK6\nWxT/W5/Kn77YSEN/X3LyC1nwl/4E1/Phwc/WM2/rUW5KCKd7ixBimviTV1DEl+tSmbF6P0sfu7pE\n8091yszOZ9qSZN5ZvJc8a5qT+/o059Fr2+Hj5ZqRAjWho1kppS6Jj5eH/favo7rarg66MT6cD5bv\nY1NqJk8Pi7V3ME+5OQ5Pjy38uO0oX5YalTwopmGNCQSw3SXx0WvbMbJLJG8v3sPg2EYl5tKqTlpT\nUEpdcZKOZfHtxsP8/upWZa5kKioy7Dyaxd7jZ6jj7UkdH09imzawj5x2V9p8pJRSyq6yoaBTZyul\nlLLTUFBKKWWnoaCUUspOQ0EppZSdhoJSSik7DQWllFJ2GgpKKaXsNBSUUkrZXXGD10QkDUi5wLeF\nAlV7X78rk54HGz0P5+i5sHGH89DMGHPeaWKvuFC4GCKytjIj+Wo7PQ82eh7O0XNho+fhHG0+Ukop\nZaehoJRSys5dQmFadReghtDzYKPn4Rw9FzZ6Hixu0aeglFKqctylpqCUUqoSanUoiMgQEdkpIkki\nMrG6y+MKIrJPRDaLyAYRWWstCxaR+SKy2/o3yFouIvKqdT42iUhnh+2Ms9bfLSLjqut4LoSITBeR\nYyKyxWFZlR27iHSxzm2S9d7Lf5PiSnByHiaJyEHr92KDiFzn8Nrj1jHtFJFrHZaX+/ciIs1FZJW1\n/L8i4nP5jq7yRCRSRBaIyDYR2Soij1jL3e534pLYbspd+34ATyAZaAH4ABuB9tVdLhcc5z4gtNSy\n54CJ1uOJwL+tx9cBcwEBegCrrOXBwB7r3yDrcVB1H1sljr0v0BnY4opjB1Zb64r13qHVfcwXcB4m\nAX8pZ9321t+CL9Dc+hvxrOjvBfgCGG09fgt4oLqP2cl5aAJ0th77A7us43W734lL+anNNYVuQJIx\nZo8xJg+YAQyv5jJdLsOBD63HHwI3Oiz/yNisBAJFpAlwLTDfGHPCGHMSmA8MudyFvlDGmMXAiVKL\nq+TYrdcaGGNWGtunwUcO26pRnJwHZ4YDM4wxucaYvUAStr+Vcv9erG/CVwMzrfc7ntMaxRhz2Biz\n3nqcBWwHwnHD34lLUZtDIRw44PA81VpW2xjgRxFZJyITrGWNjDGHrcdHgEbWY2fnpDadq6o69nDr\ncenlV5KHrGaR6cVNJlz4eQgBMowxBaWW12giEg0kAKvQ34kLUptDwV1cZYzpDAwFHhSRvo4vWt9o\n3PISM3c+duBNoCUQDxwGXqze4lw+IlIf+Ar4gzHmlONrbv47USm1ORQOApEOzyOsZbWKMeag9e8x\n4GtszQBHraou1r/HrNWdnZPadK6q6tgPWo9LL78iGGOOGmMKjTFFwDvYfi/gws9DOrZmFa9Sy2sk\nEfHGFgifGmP+Zy3W34kLUJtDYQ3Q2rpywgcYDcyu5jJVKRGpJyL+xY+BwcAWbMdZfMXEOOAb6/Fs\n4E7rqoseQKZVrZ4HDBaRIKuZYbC17EpUJcduvXZKRHpY7ep3Omyrxiv+ELSMwPZ7AbbzMFpEfEWk\nOdAaW+dpuX8v1jfrBcAt1vsdz2mNYv0/vQdsN8a85PCS/k5ciOru6XblD7arC3Zhu6ri/6q7PC44\nvhbYrhLZCGwtPkZs7cA/A7uBn4Bga7kAU63zsRlIdNjW3dg6HZOAu6r72Cp5/J9jaxrJx9a+e09V\nHjuQiO3DNBl4HWuwZ037cXIePraOcxO2D78mDuv/n3VMO3G4esbZ34v1e7baOj9fAr7VfcxOzsNV\n2JqGNgEbrJ/r3PF34lJ+dESzUkopu9rcfKSUUuoCaSgopZSy01BQSillp6GglFLKTkNBKaWUnYaC\nuuKJSKE1E+hGEVkvIr3Os36giPyuEttdKCLVdt9esc2AG1pd+1fuSUNB1QZnjTHxxphOwOPAv86z\nfiBw3lC4kjmMQFbqgmgoqNqmAXASbHPgiMjPVu1hs4gUz5I7BWhp1S6et9Z9zFpno4hMcdjeSBFZ\nLSK7RKRP6Z2JSH+rRjFTRHaIyKfFc+w7ftMXkUQRWWg9niQiH4rIEhFJEZGbROQ5a/8/WFM1FPur\ntXy1iLSy3h8mIl+JyBrrp7fDdj8WkWXYBq8pdcH024SqDeqIyAbAD9uc+ldby3OAEcaYU9aH80oR\nmY1tTv0Oxph4ABEZim0a5e7GmGwRCXbYtpcxppvYblLzFDConP0nALHAIWAZ0BtYep4ytwQGYJvv\nfwVwszHmryLyNXA9MMtaL9MY01FE7gReBn4DvAL8xxizVESisE3LEGOt3x7bJIlnz7N/pcqloaBq\ng7MOH/A9gY9EpAO2aQyetWaOLcI2zXGjct4/CHjfGJMNYIxxvDdB8aRq64BoJ/tfbYxJtfa/wVrv\nfKEw1xiTLyKbsd3g5gdr+eZS+/nc4d//OJS3vZy76VcDa2ZQsM1XpIGgLpqGgqpVjDErrFpBGLZ5\nb8KALtYH8D5stYkLkWv9W4jzv5dch8eO6xVwrom29H5zrfIWiUi+OTffTFGp/ZhyHnsAPYwxOY4b\ntELijNMjUaoStE9B1Soi0g7bN+90IAA4ZgXCAKCZtVoWtts1FpsP3CUida1tODYfXYp9QBfr8c0X\nuY1RDv+usB7/CPy+eAURib/IbStVhtYUVG1Q3KcAtiajccaYQhH5FPjWaqJZC+wAMMaki8gysd3o\nfq4x5lHrg3WtiOQBc4AnqqBcTwPvicg/gIUXuY0gEdmErWZxm7XsYWCqtdwLWAzcf4llVQpAZ0lV\nSil1jjYfKaWUstNQUEopZaehoJRSyk5DQSmllJ2GglJKKTsNBaWUUnYaCkoppew0FJRSStn9P9wK\nDA1URaesAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ13LfcF3Ji2",
        "colab_type": "code",
        "outputId": "6949bd72-53e3-4218-bdd0-8dbcd07dc136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def get_accuracy(net, loader):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        # Get inputs in right form\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "        n_total += labels.shape[0]\n",
        "    return n_correct/n_total\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.9013874414812335\n",
            "Test accuracy is 0.7738095238095238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHuAKpYIcPSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMZxrWj47rA",
        "colab_type": "code",
        "outputId": "428bf911-4e8e-4757-b353-23e12903d7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "def examine_label(idx):\n",
        "    image, label = test_set[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    confidence = class_scores.cpu().detach().numpy()\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "    print(prediction)\n",
        "    print(label)\n",
        "    print(max(confidence[0])/sum(confidence[0]))\n",
        "    print(confidence)\n",
        "\n",
        "examine_label(7)\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxeInCorFTxQUi58oqP8HF/PF8xRzujUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n",
            "0.015891763349854066\n",
            "[[-4.5522947  -0.27497983 -3.4170406  -2.9529047  -2.060661   -4.0454116 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSeuwv8R5NGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdown.download('https://l.facebook.com/l.php?u=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1K7Erwhj8naBng_RoUWnI2hJmfwzgzMKQ%3Fusp%3Dsharing%26fbclid%3DIwAR1PKsukqEVTM0NUn0KB_YmqAhgO1Sr0DnAb_xGUKSHRC4OemawFx-5gU_Q&h=AT3lp_oIwm9ztQaGRUMXdkq6oUG-Y4Gcyz1KvOqwX7WXRGPWY0qcpMlA3gHDjBsPe80nkLJo3VcTJiB-fPM4VoftAwq9P7yT5PvXs-Azjya2RsqiNJpnwKl30cL0jixwxVGLfrFryiI', 'testimages', False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OcGb6BBsLao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "image_list = []\n",
        "for filename in glob.glob('*.jpg'):\n",
        "    im=Image.open(filename)\n",
        "    image_list.append(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj7mzywDsvGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(image_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd95vXAlufFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageArray = []\n",
        "for image in image_list:\n",
        "    print(np.invert(np.array(image)))\n",
        "    # imageArray = np.concatenate(imageArray, np.array(image))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq_5m4isr3Ai",
        "colab_type": "text"
      },
      "source": [
        "√"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "909JOya1axX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}