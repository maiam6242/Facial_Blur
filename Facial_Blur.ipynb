{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial_Blur.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nT2e7XpvVrcl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiam6242/Facial_Blur/blob/master/Facial_Blur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t20OdGNZI7l",
        "colab_type": "text"
      },
      "source": [
        "# Drawing recognition with convolutional neural networks\n",
        "\n",
        "Colin Snow and Maia Matterman\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT2e7XpvVrcl",
        "colab_type": "text"
      },
      "source": [
        "##### Data\n",
        "We begin by importing required python modules and our datasets from google and then converting them to numpy arrays. We import four datasets, each containing one type of image. We chose to use datasets of tower, bear, airplane, broccoli, dog, and broom images. This was mostly based on which ones we thought looked good/funny, but also gives a good representation of both living and static items and both geometric and organic shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4x1Y1xtZMAi",
        "colab_type": "code",
        "outputId": "7167951d-6eea-47ef-a526-2dc2b6a414c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "!pip install torchviz\n",
        "# !CUDA_LAUNCH_BLOCKING=1\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy\n",
        "import time\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy', 'eiffeltower.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy', 'bear.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy', 'airplane.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy', 'broccoli.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy', 'dog.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy', 'broom.npy', False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy\n",
            "To: /content/eiffeltower.npy\n",
            "100%|██████████| 106M/106M [00:00<00:00, 148MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy\n",
            "To: /content/bear.npy\n",
            "100%|██████████| 106M/106M [00:01<00:00, 73.6MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
            "To: /content/airplane.npy\n",
            "100%|██████████| 119M/119M [00:00<00:00, 155MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy\n",
            "To: /content/broccoli.npy\n",
            "100%|██████████| 104M/104M [00:00<00:00, 154MB/s]  \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy\n",
            "To: /content/dog.npy\n",
            "100%|██████████| 119M/119M [00:00<00:00, 126MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy\n",
            "To: /content/broom.npy\n",
            "100%|██████████| 91.7M/91.7M [00:00<00:00, 191MB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'broom.npy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPERGQfVgc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tower = np.load('eiffeltower.npy') #type = 1\n",
        "bear = np.load('bear.npy') # type = 0\n",
        "airplane = np.load('airplane.npy')\n",
        "broccoli = np.load('broccoli.npy')\n",
        "dog = np.load('dog.npy')\n",
        "broom = np.load('broom.npy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7USekPFvWGHP",
        "colab_type": "text"
      },
      "source": [
        "##### Visualization\n",
        "\n",
        "Because our data is an array of numpy arrays, we can display an image as an index of the list using PIL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICO8bUo6V2He",
        "colab_type": "code",
        "outputId": "60fd70a6-ade3-4d33-86dc-cf4c554eb1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X = airplane[750]\n",
        "X = np.resize(X,(28,28))\n",
        "X = np.invert(X)\n",
        "plt.imshow(X, cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnoRPCJ8wWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnqGEb9KWZz8",
        "colab_type": "text"
      },
      "source": [
        "##### Our Dataset\n",
        "\n",
        "In addition to the google dataset, we decided to create our own data to see how changing the method of collection affected the result. We collected a full set of 6 images from 14 different people around Olin for a total of 84 new test images. We recorded the data in photoshop by giving each participant 20 seconds to draw the required image on a 28 x 28 pixel grid. We saved each of these images named by type and then imported them below to create a numpy array for each of these datasets as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgR_pbWwS4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24c4e36e-71da-4d89-e8e2-d1628bb7b11f"
      },
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
        "\n",
        "\n",
        "airplane_list = []\n",
        "for filename in glob.glob('airplane*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    airplane_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "broom_list = []\n",
        "for filename in glob.glob('broom*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    broom_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "bear_list = []\n",
        "for filename in glob.glob('bear*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    bear_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "broccoli_list = []\n",
        "for filename in glob.glob('broccoli*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    broccoli_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "tower_list = []\n",
        "for filename in glob.glob('tower*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    tower_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "dog_list = []\n",
        "for filename in glob.glob('dog*.jpg'):\n",
        "    im=Image.open(filename).resize((28,28))\n",
        "    dog_list.append( np.array((np.invert(np.array(im)))))\n",
        "\n",
        "dog_list = np.array(dog_list)\n",
        "broccoli_list = np.array(broccoli_list)\n",
        "bear_list = np.array(bear_list)\n",
        "broom_list = np.array(broom_list)\n",
        "airplane_list = np.array(airplane_list)\n",
        "print(len(tower_list))\n",
        "tower_list = np.array(tower_list)\n",
        "\n",
        "\n",
        "# dog_list = np.reshape(dog_list, (14,1024))\n",
        "\n",
        "# plt.imshow(dog_list[6],cmap=\"gray\")\n",
        "# print(np.array(airplane_list[0]))\n",
        "# print(len(np.array(dog_list)))\n",
        "print(np.shape(np.array(airplane_list[0])))\n",
        "# print(dog_list[6])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz77DUyKpWse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yhVEn8Ql_h",
        "colab_type": "code",
        "outputId": "d0ca62d9-c92f-4e3e-b864-97d65398beb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import torch\n",
        "\n",
        "class QuickDrawData(Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super(QuickDrawData, self).__init__()\n",
        "        count = 0\n",
        "        # self.data = np.empty(args[0].shape, dtype=int)\n",
        "        # self.targets = np.empty(args[0].shape, dtype=int)\n",
        "        self.classes = []\n",
        "        for arg in args:\n",
        "          # print(str(arg))\n",
        "          if type(arg) == str:\n",
        "            self.classes += arg\n",
        "          else:\n",
        "\n",
        "            if count == 0:\n",
        "              print(arg.shape)\n",
        "              self.data = np.array(arg)\n",
        "              self.targets = np.array(0*np.ones(arg.shape[0], dtype = int))\n",
        "              print(self.targets)\n",
        "              print(type(self.targets))\n",
        "              print(type(self.data))\n",
        "              print(type(self.classes))\n",
        "            else:\n",
        "              self.data = np.vstack((self.data, arg))\n",
        "              print(int(count)*np.ones(arg.shape[0], dtype = int))\n",
        "              print(self.targets)\n",
        "              self.targets = np.hstack((self.targets, int(count)*np.ones(arg.shape[0], dtype = int)))\n",
        "            count+=1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.targets.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # print(self.data[index, :])\n",
        "        # print(type(self.data[index, :]))\n",
        "        # print(np.size(self.data[index, :]))\n",
        "        return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "\n",
        "quick_draw_data = QuickDrawData(tower, bear, airplane, broccoli, dog, broom, 'tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "testdata = QuickDrawData(np.array(tower_list), np.array(bear_list), np.array(airplane_list), np.array(broccoli_list), np.array(dog_list), np.array(broom_list), 'tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(134801, 784)\n",
            "[0 0 0 ... 0 0 0]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "[1 1 1 ... 1 1 1]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[2 2 2 ... 2 2 2]\n",
            "[0 0 0 ... 1 1 1]\n",
            "[3 3 3 ... 3 3 3]\n",
            "[0 0 0 ... 2 2 2]\n",
            "[4 4 4 ... 4 4 4]\n",
            "[0 0 0 ... 3 3 3]\n",
            "[5 5 5 ... 5 5 5]\n",
            "[0 0 0 ... 4 4 4]\n",
            "<class 'numpy.ndarray'>\n",
            "(14, 28, 28)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2]\n",
            "[4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            "[5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi8X_pFZAcm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbaebNQ7RwG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = int(len(quick_draw_data)*.9) \n",
        "train, test = random_split(quick_draw_data, [x,(len(quick_draw_data) - x)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMrsY0PERwJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data set information\n",
        "\n",
        "image_dims = 1, 28, 28\n",
        "n_training_samples = len(train) # How many training images to use\n",
        "# n_test_samples = len(test) # How many test images to use\n",
        "n_test_samples = 84 # How many test images to use                       --------------\n",
        "classes = ('tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "# Load the training set\n",
        "train_set = train\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "\n",
        "#Load the test set\n",
        "\n",
        "# choose correct set                                                    ---------------\n",
        "\n",
        "test_set = test\n",
        "test_set2 = testdata\n",
        "\n",
        "# test_set = torch.cat([test, testdata],1)\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dv6-ImXRwMA",
        "colab_type": "code",
        "outputId": "0a33d97b-6113-4b0e-d019-68455b4e3d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(test_set)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QoJj27WRwO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyCNN, self).__init__()\n",
        "    \n",
        "    num_kernels = 16\n",
        "\n",
        "    fcl_size = 256\n",
        "    fcl_size2 = 128\n",
        "    fcl_size3 = 64\n",
        "\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.conv2 = nn.Conv2d(num_kernels, num_kernels*2, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.maxpool_output_size1 = int(num_kernels*(image_dims[1]/2) * (image_dims[2]/2))\n",
        "    self.maxpool_output_size2 = int(num_kernels*(image_dims[1]/4) * (image_dims[2]/4)*2)\n",
        "\n",
        "    self.batchnorm1 = nn.BatchNorm2d(num_kernels)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(num_kernels*2)\n",
        "    self.batchnorm3 = nn.BatchNorm1d(fcl_size)\n",
        "    self.batchnorm4 = nn.BatchNorm1d(fcl_size2)\n",
        "    self.batchnorm5 = nn.BatchNorm1d(fcl_size3)\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Linear(1568, fcl_size)\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "    # fc2_size = fcl_size\n",
        "\n",
        "    self.fc2 = nn.Linear(fcl_size, fcl_size2)\n",
        "    self.fc4 = nn.Linear(fcl_size2, fcl_size3)\n",
        "    # self.fc7 = nn.Linear(fcl_size, fcl_size)\n",
        "\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "\n",
        "    fc3_size = len(classes)\n",
        "    # fc3_size = 6\n",
        "    self.fc3 = nn.Linear(fcl_size3, fc3_size)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.batchnorm1(x)\n",
        "\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.batchnorm2(x)\n",
        "\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    # x = self.pool1(x)\n",
        "    # x = self.activation_func(x)\n",
        "    # x = x.view(-1, self.maxpool_output_size1)\n",
        "    # x = self.pool2(x)\n",
        "    # x = self.activation_func(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = x.view(-1, self.maxpool_output_size2)\n",
        "\n",
        "    # print(x.size())\n",
        "\n",
        "    x = self.fc1(x)\n",
        "\n",
        "\n",
        "    # print(x.size())\n",
        "\n",
        "    # x = self.batchnorm3(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    # x = self.batchnorm4(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.fc4(x)\n",
        "\n",
        "    # x = self.batchnorm5(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "    # x = self.fc2(x)\n",
        "    # x = self.activation_func(x)\n",
        "    x = self.fc3(x)\n",
        "    x = torch.nn.functional.log_softmax(x)\n",
        "    # x = self.activation_func(x)\n",
        "\n",
        "    # x = self.activation_func7(x)\n",
        "    return x\n",
        "\n",
        "  def get_loss(self, learning_rate):\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    return loss, optimizer\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHaayoLKRwaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = MyCNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SbsGk9qjpmY",
        "colab_type": "code",
        "outputId": "4550a82a-cb90-4bd6-e267-34d90b882689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    fake_input = Variable(torch.zeros((1, image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "# \n",
        "visualize_network(net)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fcfbac73fd0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"463pt\" height=\"864pt\"\n viewBox=\"0.00 0.00 463.04 864.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.7135 .7135) rotate(0) translate(4 1207)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1207 645,-1207 645,4 -4,4\"/>\n<!-- 140531725879280 -->\n<g id=\"node1\" class=\"node\">\n<title>140531725879280</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"539,-21 414,-21 414,0 539,0 539,-21\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogSoftmaxBackward</text>\n</g>\n<!-- 140530134656784 -->\n<g id=\"node2\" class=\"node\">\n<title>140530134656784</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"528.5,-78 424.5,-78 424.5,-57 528.5,-57 528.5,-78\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140530134656784&#45;&gt;140531725879280 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140530134656784&#45;&gt;140531725879280</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M476.5,-56.7787C476.5,-49.6134 476.5,-39.9517 476.5,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"480.0001,-31.1732 476.5,-21.1732 473.0001,-31.1732 480.0001,-31.1732\"/>\n</g>\n<!-- 140530134655160 -->\n<g id=\"node3\" class=\"node\">\n<title>140530134655160</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"411.5,-148 357.5,-148 357.5,-114 411.5,-114 411.5,-148\"/>\n<text text-anchor=\"middle\" x=\"384.5\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.bias</text>\n<text text-anchor=\"middle\" x=\"384.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6)</text>\n</g>\n<!-- 140530134655160&#45;&gt;140530134656784 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140530134655160&#45;&gt;140530134656784</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M409.1543,-113.9832C422.6894,-104.641 439.3926,-93.1122 452.778,-83.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.7865,-86.7398 461.0283,-78.1788 450.8102,-80.9788 454.7865,-86.7398\"/>\n</g>\n<!-- 140530134658184 -->\n<g id=\"node4\" class=\"node\">\n<title>140530134658184</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"523.5,-141.5 429.5,-141.5 429.5,-120.5 523.5,-120.5 523.5,-141.5\"/>\n<text text-anchor=\"middle\" x=\"476.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140530134658184&#45;&gt;140530134656784 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140530134658184&#45;&gt;140530134656784</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M476.5,-120.2281C476.5,-111.5091 476.5,-98.9699 476.5,-88.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"480.0001,-88.1128 476.5,-78.1128 473.0001,-88.1129 480.0001,-88.1128\"/>\n</g>\n<!-- 140530134656336 -->\n<g id=\"node5\" class=\"node\">\n<title>140530134656336</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"527.5,-211.5 423.5,-211.5 423.5,-190.5 527.5,-190.5 527.5,-211.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-197.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140530134656336&#45;&gt;140530134658184 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140530134656336&#45;&gt;140530134658184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.6519,-190.3685C475.7972,-180.1925 476.0206,-164.5606 476.2016,-151.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.7034,-151.7806 476.3467,-141.7315 472.7041,-151.6805 479.7034,-151.7806\"/>\n</g>\n<!-- 140530134657176 -->\n<g id=\"node6\" class=\"node\">\n<title>140530134657176</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-288 356.5,-288 356.5,-254 410.5,-254 410.5,-288\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc4.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-261.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 140530134657176&#45;&gt;140530134656336 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140530134657176&#45;&gt;140530134656336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M406.2416,-253.6966C420.6068,-242.7666 439.0787,-228.7119 453.3325,-217.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.7491,-220.4258 461.5881,-211.5852 451.5104,-214.855 455.7491,-220.4258\"/>\n</g>\n<!-- 140530134657680 -->\n<g id=\"node7\" class=\"node\">\n<title>140530134657680</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"522.5,-281.5 428.5,-281.5 428.5,-260.5 522.5,-260.5 522.5,-281.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140530134657680&#45;&gt;140530134656336 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140530134657680&#45;&gt;140530134656336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-260.3685C475.5,-250.1925 475.5,-234.5606 475.5,-221.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-221.7315 475.5,-211.7315 472.0001,-221.7316 479.0001,-221.7315\"/>\n</g>\n<!-- 140530134311264 -->\n<g id=\"node8\" class=\"node\">\n<title>140530134311264</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"527.5,-351.5 423.5,-351.5 423.5,-330.5 527.5,-330.5 527.5,-351.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-337.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140530134311264&#45;&gt;140530134657680 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140530134311264&#45;&gt;140530134657680</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-330.3685C475.5,-320.1925 475.5,-304.5606 475.5,-291.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-291.7315 475.5,-281.7315 472.0001,-291.7316 479.0001,-291.7315\"/>\n</g>\n<!-- 140530134311544 -->\n<g id=\"node9\" class=\"node\">\n<title>140530134311544</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-428 356.5,-428 356.5,-394 410.5,-394 410.5,-428\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-414.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 140530134311544&#45;&gt;140530134311264 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140530134311544&#45;&gt;140530134311264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M406.2416,-393.6966C420.6068,-382.7666 439.0787,-368.7119 453.3325,-357.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.7491,-360.4258 461.5881,-351.5852 451.5104,-354.855 455.7491,-360.4258\"/>\n</g>\n<!-- 140530134311040 -->\n<g id=\"node10\" class=\"node\">\n<title>140530134311040</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"522.5,-421.5 428.5,-421.5 428.5,-400.5 522.5,-400.5 522.5,-421.5\"/>\n<text text-anchor=\"middle\" x=\"475.5\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140530134311040&#45;&gt;140530134311264 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140530134311040&#45;&gt;140530134311264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M475.5,-400.3685C475.5,-390.1925 475.5,-374.5606 475.5,-361.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.0001,-361.7315 475.5,-351.7315 472.0001,-361.7316 479.0001,-361.7315\"/>\n</g>\n<!-- 140530134311208 -->\n<g id=\"node11\" class=\"node\">\n<title>140530134311208</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"526.5,-491.5 422.5,-491.5 422.5,-470.5 526.5,-470.5 526.5,-491.5\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-477.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140530134311208&#45;&gt;140530134311040 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140530134311208&#45;&gt;140530134311040</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M474.6519,-470.3685C474.7972,-460.1925 475.0206,-444.5606 475.2016,-431.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"478.7034,-431.7806 475.3467,-421.7315 471.7041,-431.6805 478.7034,-431.7806\"/>\n</g>\n<!-- 140530134312328 -->\n<g id=\"node12\" class=\"node\">\n<title>140530134312328</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"410.5,-568 356.5,-568 356.5,-534 410.5,-534 410.5,-568\"/>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-554.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"383.5\" y=\"-541.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 140530134312328&#45;&gt;140530134311208 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140530134312328&#45;&gt;140530134311208</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M405.9944,-533.6966C420.2034,-522.7666 438.4745,-508.7119 452.5735,-497.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.947,-500.4565 460.7393,-491.5852 450.679,-494.9081 454.947,-500.4565\"/>\n</g>\n<!-- 140530134311712 -->\n<g id=\"node13\" class=\"node\">\n<title>140530134311712</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"520,-561.5 429,-561.5 429,-540.5 520,-540.5 520,-561.5\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-547.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 140530134311712&#45;&gt;140530134311208 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140530134311712&#45;&gt;140530134311208</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M474.5,-540.3685C474.5,-530.1925 474.5,-514.5606 474.5,-501.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"478.0001,-501.7315 474.5,-491.7315 471.0001,-501.7316 478.0001,-501.7315\"/>\n</g>\n<!-- 140530134311768 -->\n<g id=\"node14\" class=\"node\">\n<title>140530134311768</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"520.5,-631.5 426.5,-631.5 426.5,-610.5 520.5,-610.5 520.5,-631.5\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-617.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140530134311768&#45;&gt;140530134311712 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140530134311768&#45;&gt;140530134311712</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.6519,-610.3685C473.7972,-600.1925 474.0206,-584.5606 474.2016,-571.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.7034,-571.7806 474.3467,-561.7315 470.7041,-571.6805 477.7034,-571.7806\"/>\n</g>\n<!-- 140530134311992 -->\n<g id=\"node15\" class=\"node\">\n<title>140530134311992</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"563.5,-695 383.5,-695 383.5,-674 563.5,-674 563.5,-695\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-681.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140530134311992&#45;&gt;140530134311768 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140530134311992&#45;&gt;140530134311768</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-673.7281C473.5,-665.0091 473.5,-652.4699 473.5,-641.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-641.6128 473.5,-631.6128 470.0001,-641.6129 477.0001,-641.6128\"/>\n</g>\n<!-- 140530134312440 -->\n<g id=\"node16\" class=\"node\">\n<title>140530134312440</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"550,-752 397,-752 397,-731 550,-731 550,-752\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-738.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnBatchNormBackward</text>\n</g>\n<!-- 140530134312440&#45;&gt;140530134311992 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140530134312440&#45;&gt;140530134311992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-730.7787C473.5,-723.6134 473.5,-713.9517 473.5,-705.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-705.1732 473.5,-695.1732 470.0001,-705.1732 477.0001,-705.1732\"/>\n</g>\n<!-- 140530134312104 -->\n<g id=\"node17\" class=\"node\">\n<title>140530134312104</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"401.5,-815.5 307.5,-815.5 307.5,-794.5 401.5,-794.5 401.5,-815.5\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-801.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140530134312104&#45;&gt;140530134312440 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140530134312104&#45;&gt;140530134312440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M374.4179,-794.3715C393.553,-784.1608 422.6839,-768.6162 444.4131,-757.0212\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"446.3423,-759.9589 453.5171,-752.1631 443.0468,-753.7831 446.3423,-759.9589\"/>\n</g>\n<!-- 140530134312160 -->\n<g id=\"node18\" class=\"node\">\n<title>140530134312160</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"433,-879 276,-879 276,-858 433,-858 433,-879\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-865.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140530134312160&#45;&gt;140530134312104 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140530134312160&#45;&gt;140530134312104</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-857.7281C354.5,-849.0091 354.5,-836.4699 354.5,-825.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-825.6128 354.5,-815.6128 351.0001,-825.6129 358.0001,-825.6128\"/>\n</g>\n<!-- 140530134311320 -->\n<g id=\"node19\" class=\"node\">\n<title>140530134311320</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"295.5,-942.5 115.5,-942.5 115.5,-921.5 295.5,-921.5 295.5,-942.5\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-928.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140530134311320&#45;&gt;140530134312160 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140530134311320&#45;&gt;140530134312160</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M230.4392,-921.3715C255.0429,-910.8861 292.8449,-894.7758 320.2558,-883.094\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"321.6522,-886.3036 329.4794,-879.1631 318.9077,-879.864 321.6522,-886.3036\"/>\n</g>\n<!-- 140530134312272 -->\n<g id=\"node20\" class=\"node\">\n<title>140530134312272</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"282,-1006 129,-1006 129,-985 282,-985 282,-1006\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-992.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnBatchNormBackward</text>\n</g>\n<!-- 140530134312272&#45;&gt;140530134311320 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140530134312272&#45;&gt;140530134311320</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M205.5,-984.7281C205.5,-976.0091 205.5,-963.4699 205.5,-952.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0001,-952.6128 205.5,-942.6128 202.0001,-952.6129 209.0001,-952.6128\"/>\n</g>\n<!-- 140530133929152 -->\n<g id=\"node21\" class=\"node\">\n<title>140530133929152</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"133.5,-1069.5 39.5,-1069.5 39.5,-1048.5 133.5,-1048.5 133.5,-1069.5\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-1055.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140530133929152&#45;&gt;140530134312272 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140530133929152&#45;&gt;140530134312272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.4179,-1048.3715C125.553,-1038.1608 154.6839,-1022.6162 176.4131,-1011.0212\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"178.3423,-1013.9589 185.5171,-1006.1631 175.0468,-1007.7831 178.3423,-1013.9589\"/>\n</g>\n<!-- 140531461484840 -->\n<g id=\"node22\" class=\"node\">\n<title>140531461484840</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"165,-1133 8,-1133 8,-1112 165,-1112 165,-1133\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-1119.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140531461484840&#45;&gt;140530133929152 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140531461484840&#45;&gt;140530133929152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M86.5,-1111.7281C86.5,-1103.0091 86.5,-1090.4699 86.5,-1079.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"90.0001,-1079.6128 86.5,-1069.6128 83.0001,-1079.6129 90.0001,-1079.6128\"/>\n</g>\n<!-- 140530133884880 -->\n<g id=\"node23\" class=\"node\">\n<title>140530133884880</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"81,-1203 0,-1203 0,-1169 81,-1169 81,-1203\"/>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-1189.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-1176.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16, 1, 3, 3)</text>\n</g>\n<!-- 140530133884880&#45;&gt;140531461484840 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140530133884880&#45;&gt;140531461484840</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M52.8272,-1168.9832C58.9107,-1160.5853 66.2742,-1150.4204 72.5621,-1141.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5945,-1143.5204 78.6266,-1133.3687 69.9256,-1139.4138 75.5945,-1143.5204\"/>\n</g>\n<!-- 140530133920960 -->\n<g id=\"node24\" class=\"node\">\n<title>140530133920960</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"167.5,-1203 99.5,-1203 99.5,-1169 167.5,-1169 167.5,-1203\"/>\n<text text-anchor=\"middle\" x=\"133.5\" y=\"-1189.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"133.5\" y=\"-1176.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140530133920960&#45;&gt;140531461484840 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140530133920960&#45;&gt;140531461484840</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M120.9049,-1168.9832C114.6237,-1160.4969 107.0069,-1150.2062 100.5384,-1141.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.3071,-1139.3243 94.5445,-1133.3687 97.6806,-1143.4888 103.3071,-1139.3243\"/>\n</g>\n<!-- 140530133876128 -->\n<g id=\"node25\" class=\"node\">\n<title>140530133876128</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"259.5,-1076 151.5,-1076 151.5,-1042 259.5,-1042 259.5,-1076\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-1062.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm1.weight</text>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-1049.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140530133876128&#45;&gt;140530134312272 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140530133876128&#45;&gt;140530134312272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M205.5,-1041.9832C205.5,-1034.1157 205.5,-1024.6973 205.5,-1016.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0001,-1016.3686 205.5,-1006.3687 202.0001,-1016.3687 209.0001,-1016.3686\"/>\n</g>\n<!-- 140531461485792 -->\n<g id=\"node26\" class=\"node\">\n<title>140531461485792</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"373,-1076 278,-1076 278,-1042 373,-1042 373,-1076\"/>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-1062.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm1.bias</text>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-1049.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140531461485792&#45;&gt;140530134312272 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140531461485792&#45;&gt;140530134312272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M293.3422,-1041.9832C275.1833,-1032.3741 252.6526,-1020.4516 234.9561,-1011.0872\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"236.1563,-1007.7625 225.6804,-1006.1788 232.8822,-1013.9496 236.1563,-1007.7625\"/>\n</g>\n<!-- 140530134312720 -->\n<g id=\"node27\" class=\"node\">\n<title>140530134312720</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"395,-949 314,-949 314,-915 395,-915 395,-949\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-935.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.weight</text>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-922.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32, 16, 3, 3)</text>\n</g>\n<!-- 140530134312720&#45;&gt;140530134312160 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140530134312720&#45;&gt;140530134312160</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-914.9832C354.5,-907.1157 354.5,-897.6973 354.5,-889.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-889.3686 354.5,-879.3687 351.0001,-889.3687 358.0001,-889.3686\"/>\n</g>\n<!-- 140530134311376 -->\n<g id=\"node28\" class=\"node\">\n<title>140530134311376</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"481.5,-949 413.5,-949 413.5,-915 481.5,-915 481.5,-949\"/>\n<text text-anchor=\"middle\" x=\"447.5\" y=\"-935.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.bias</text>\n<text text-anchor=\"middle\" x=\"447.5\" y=\"-922.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140530134311376&#45;&gt;140530134312160 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140530134311376&#45;&gt;140530134312160</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M422.5777,-914.9832C408.8955,-905.641 392.0107,-894.1122 378.4799,-884.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.372,-881.9273 370.1398,-879.1788 376.4248,-887.7082 380.372,-881.9273\"/>\n</g>\n<!-- 140530134311824 -->\n<g id=\"node29\" class=\"node\">\n<title>140530134311824</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"527.5,-822 419.5,-822 419.5,-788 527.5,-788 527.5,-822\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-808.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm2.weight</text>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-795.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140530134311824&#45;&gt;140530134312440 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140530134311824&#45;&gt;140530134312440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-787.9832C473.5,-780.1157 473.5,-770.6973 473.5,-762.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-762.3686 473.5,-752.3687 470.0001,-762.3687 477.0001,-762.3686\"/>\n</g>\n<!-- 140530134312552 -->\n<g id=\"node30\" class=\"node\">\n<title>140530134312552</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"641,-822 546,-822 546,-788 641,-788 641,-822\"/>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-808.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm2.bias</text>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-795.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140530134312552&#45;&gt;140530134312440 -->\n<g id=\"edge29\" class=\"edge\">\n<title>140530134312552&#45;&gt;140530134312440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M561.3422,-787.9832C543.1833,-778.3741 520.6526,-766.4516 502.9561,-757.0872\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"504.1563,-753.7625 493.6804,-752.1788 500.8822,-759.9496 504.1563,-753.7625\"/>\n</g>\n<!-- 140530134312496 -->\n<g id=\"node31\" class=\"node\">\n<title>140530134312496</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"612,-561.5 539,-561.5 539,-540.5 612,-540.5 612,-561.5\"/>\n<text text-anchor=\"middle\" x=\"575.5\" y=\"-547.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140530134312496&#45;&gt;140530134311208 -->\n<g id=\"edge30\" class=\"edge\">\n<title>140530134312496&#45;&gt;140530134311208</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M560.1603,-540.3685C543.6024,-528.8927 517.033,-510.4783 497.8726,-497.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"499.8659,-494.3219 489.6532,-491.5022 495.8784,-500.0752 499.8659,-494.3219\"/>\n</g>\n<!-- 140531461484952 -->\n<g id=\"node32\" class=\"node\">\n<title>140531461484952</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"614,-638 539,-638 539,-604 614,-604 614,-638\"/>\n<text text-anchor=\"middle\" x=\"576.5\" y=\"-624.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"576.5\" y=\"-611.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256, 1568)</text>\n</g>\n<!-- 140531461484952&#45;&gt;140530134312496 -->\n<g id=\"edge31\" class=\"edge\">\n<title>140531461484952&#45;&gt;140530134312496</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M576.2528,-603.6966C576.1152,-594.0634 575.9429,-582.003 575.7979,-571.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"579.2967,-571.7402 575.6542,-561.7913 572.2975,-571.8403 579.2967,-571.7402\"/>\n</g>\n<!-- 140530134310984 -->\n<g id=\"node33\" class=\"node\">\n<title>140530134310984</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"615,-421.5 542,-421.5 542,-400.5 615,-400.5 615,-421.5\"/>\n<text text-anchor=\"middle\" x=\"578.5\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140530134310984&#45;&gt;140530134311264 -->\n<g id=\"edge32\" class=\"edge\">\n<title>140530134310984&#45;&gt;140530134311264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.8565,-400.3685C545.9707,-388.8927 518.8752,-370.4783 499.3354,-357.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"501.1913,-354.2284 490.9532,-351.5022 497.2567,-360.0179 501.1913,-354.2284\"/>\n</g>\n<!-- 140530133883536 -->\n<g id=\"node34\" class=\"node\">\n<title>140530133883536</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"614,-498 545,-498 545,-464 614,-464 614,-498\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-484.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128, 256)</text>\n</g>\n<!-- 140530133883536&#45;&gt;140530134310984 -->\n<g id=\"edge33\" class=\"edge\">\n<title>140530133883536&#45;&gt;140530134310984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.2528,-463.6966C579.1152,-454.0634 578.9429,-442.003 578.7979,-431.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"582.2967,-431.7402 578.6542,-421.7913 575.2975,-431.8403 582.2967,-431.7402\"/>\n</g>\n<!-- 140530134655104 -->\n<g id=\"node35\" class=\"node\">\n<title>140530134655104</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"615,-281.5 542,-281.5 542,-260.5 615,-260.5 615,-281.5\"/>\n<text text-anchor=\"middle\" x=\"578.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140530134655104&#45;&gt;140530134656336 -->\n<g id=\"edge34\" class=\"edge\">\n<title>140530134655104&#45;&gt;140530134656336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.8565,-260.3685C545.9707,-248.8927 518.8752,-230.4783 499.3354,-217.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"501.1913,-214.2284 490.9532,-211.5022 497.2567,-220.0179 501.1913,-214.2284\"/>\n</g>\n<!-- 140530134311600 -->\n<g id=\"node36\" class=\"node\">\n<title>140530134311600</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"613,-358 546,-358 546,-324 613,-324 613,-358\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc4.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-331.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 128)</text>\n</g>\n<!-- 140530134311600&#45;&gt;140530134655104 -->\n<g id=\"edge35\" class=\"edge\">\n<title>140530134311600&#45;&gt;140530134655104</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.2528,-323.6966C579.1152,-314.0634 578.9429,-302.003 578.7979,-291.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"582.2967,-291.7402 578.6542,-281.7913 575.2975,-291.8403 582.2967,-291.7402\"/>\n</g>\n<!-- 140530134656560 -->\n<g id=\"node37\" class=\"node\">\n<title>140530134656560</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"616,-141.5 543,-141.5 543,-120.5 616,-120.5 616,-141.5\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140530134656560&#45;&gt;140530134656784 -->\n<g id=\"edge36\" class=\"edge\">\n<title>140530134656560&#45;&gt;140530134656784</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M562.0275,-120.2281C545.6519,-110.1325 520.9682,-94.9149 502.3209,-83.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"504.0636,-80.3814 493.7145,-78.1128 500.3901,-86.3401 504.0636,-80.3814\"/>\n</g>\n<!-- 140530134656952 -->\n<g id=\"node38\" class=\"node\">\n<title>140530134656952</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"613,-218 546,-218 546,-184 613,-184 613,-218\"/>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.weight</text>\n<text text-anchor=\"middle\" x=\"579.5\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6, 64)</text>\n</g>\n<!-- 140530134656952&#45;&gt;140530134656560 -->\n<g id=\"edge37\" class=\"edge\">\n<title>140530134656952&#45;&gt;140530134656560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M579.5,-183.6966C579.5,-174.0634 579.5,-162.003 579.5,-151.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"583.0001,-151.7912 579.5,-141.7913 576.0001,-151.7913 583.0001,-151.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_9CTlb4Yx7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training parameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2\n",
        "n_epochs = 5\n",
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, sampler=train_sampler, num_workers = 2)\n",
        "test_loader2 = torch.utils.data.DataLoader(testdata, batch_size = 84, sampler=test_sampler, num_workers = 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, sampler=test_sampler, num_workers = 2)\n",
        "\n",
        "def train_model(net):\n",
        "    \"\"\" Train a the specified network.\n",
        "\n",
        "        Outputs a tuple with the following four elements\n",
        "        train_hist_x: the x-values (batch number) that the training set was \n",
        "            evaluated on.\n",
        "        train_loss_hist: the loss values for the training set corresponding to\n",
        "            the batch numbers returned in train_hist_x\n",
        "        test_hist_x: the x-values (batch number) that the test set was \n",
        "            evaluated on.\n",
        "        test_loss_hist: the loss values for the test set corresponding to\n",
        "            the batch numbers returned in test_hist_x\n",
        "    \"\"\" \n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 200\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    test_hist_x = []\n",
        "    test_loss_hist = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    # Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "            # Get inputs in right form\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "            \n",
        "            # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Compute the loss and find the loss with respect to each parameter of the model\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            \n",
        "            # Change each parameter with respect to the recently computed loss.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print every 20th batch of an epoch\n",
        "            if (i % print_every) == print_every-1:\n",
        "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "                # Reset running loss and time\n",
        "                train_loss_hist.append(running_loss / print_every)\n",
        "                train_hist_x.append(idx)\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            idx += 1\n",
        "\n",
        "        # At the end of the epoch, do a pass on the test set\n",
        "        total_test_loss = 0\n",
        "        for inputs, labels in test_loader:\n",
        "\n",
        "            # Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            test_outputs = net(inputs)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.data.item()\n",
        "        test_loss_hist.append(total_test_loss / len(test_loader))\n",
        "        test_hist_x.append(idx)\n",
        "        print(\"Validation loss = {:.2f}\".format(\n",
        "            total_test_loss / len(test_loader)))\n",
        "\n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "    return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJtMARZpp_z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67b5624a-7b08-44cf-d6ae-df9bab6d35a2"
      },
      "source": [
        "test_set[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,  22., 101.,  46.,  19.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,  48., 115., 168., 239., 255., 255., 252., 169.,  29.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 142., 222.,\n",
              "           241., 253., 255., 255., 255., 255., 255., 255., 255., 134.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  85., 144., 247., 199.,\n",
              "           136., 201., 255., 237., 204., 177., 196., 252., 242.,  40.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,  28., 242., 239., 215.,  50.,\n",
              "             0.,   3., 200., 191.,   0.,   0.,   0.,  80., 247., 205.,  14.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,  50., 249., 248.,  56.,   0.,   0.,\n",
              "             0.,   0., 162., 214.,   0.,   0.,   0.,   0.,  65., 253., 121.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,  45., 252., 218.,  51.,   0.,   0.,\n",
              "             0.,   0.,  58., 100.,   0.,   0.,   0.,   0.,   0., 174., 233.,\n",
              "             9.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,  61., 222., 250.,  55.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 126., 255.,\n",
              "            68.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 249., 170.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  71., 255., 184.,\n",
              "             8.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  12., 237., 252.,   9.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.,   0.,   8., 154., 251.,   7.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  13., 233., 247.,  87.,\n",
              "             0.,   0.,   0.,   0.,  24., 161., 219., 252., 255., 142.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26., 189., 255.,\n",
              "           185., 172.,  72.,   0., 184., 225., 162., 119.,  71.,   1.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2., 115.,\n",
              "           207., 235., 197.,   0., 193., 183.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0., 184., 193.,   0., 193., 183.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0., 188., 189.,   0., 193., 183.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0., 191., 185.,   0., 197., 182.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0., 195., 181.,   2., 240., 142.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0., 199., 177.,  44., 255.,  85.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0., 203., 174., 101., 255.,  28.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0., 223., 155., 156., 228.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0., 246., 130., 185., 191.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "            17., 255., 106., 203., 173.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "            41., 255.,  82., 221., 155.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "            66., 255.,  91., 243., 137.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "           105., 255., 255., 245.,  50.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "            12., 137.,  84.,  30.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "             0.,   0.,   0.,   0.,   0.,   0.]]]), 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGyZdjgNjf_h",
        "colab_type": "code",
        "outputId": "85950a1d-d674-47d5-9d36-2eab3da7b3d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 200\t train_loss: 0.77 took: 1.92s\n",
            "Epoch 1, Iteration 400\t train_loss: 0.52 took: 1.79s\n",
            "Epoch 1, Iteration 600\t train_loss: 0.45 took: 1.75s\n",
            "Epoch 1, Iteration 800\t train_loss: 0.43 took: 1.79s\n",
            "Epoch 1, Iteration 1000\t train_loss: 0.39 took: 1.78s\n",
            "Epoch 1, Iteration 1200\t train_loss: 0.36 took: 1.77s\n",
            "Epoch 1, Iteration 1400\t train_loss: 0.38 took: 1.78s\n",
            "Epoch 1, Iteration 1600\t train_loss: 0.37 took: 1.79s\n",
            "Epoch 1, Iteration 1800\t train_loss: 0.36 took: 1.74s\n",
            "Epoch 1, Iteration 2000\t train_loss: 0.36 took: 1.73s\n",
            "Epoch 1, Iteration 2200\t train_loss: 0.34 took: 1.74s\n",
            "Epoch 1, Iteration 2400\t train_loss: 0.35 took: 1.76s\n",
            "Epoch 1, Iteration 2600\t train_loss: 0.34 took: 1.72s\n",
            "Epoch 1, Iteration 2800\t train_loss: 0.36 took: 1.76s\n",
            "Epoch 1, Iteration 3000\t train_loss: 0.36 took: 1.72s\n",
            "Epoch 1, Iteration 3200\t train_loss: 0.35 took: 1.71s\n",
            "Epoch 1, Iteration 3400\t train_loss: 0.34 took: 1.75s\n",
            "Epoch 1, Iteration 3600\t train_loss: 0.34 took: 1.73s\n",
            "Epoch 1, Iteration 3800\t train_loss: 0.33 took: 1.75s\n",
            "Epoch 1, Iteration 4000\t train_loss: 0.34 took: 1.77s\n",
            "Epoch 1, Iteration 4200\t train_loss: 0.34 took: 1.75s\n",
            "Epoch 1, Iteration 4400\t train_loss: 0.34 took: 1.78s\n",
            "Epoch 1, Iteration 4600\t train_loss: 0.32 took: 1.80s\n",
            "Epoch 1, Iteration 4800\t train_loss: 0.31 took: 1.80s\n",
            "Epoch 1, Iteration 5000\t train_loss: 0.31 took: 1.79s\n",
            "Epoch 1, Iteration 5200\t train_loss: 0.34 took: 1.79s\n",
            "Epoch 1, Iteration 5400\t train_loss: 0.31 took: 1.78s\n",
            "Epoch 1, Iteration 5600\t train_loss: 0.31 took: 1.78s\n",
            "Epoch 1, Iteration 5800\t train_loss: 0.31 took: 1.77s\n",
            "Epoch 1, Iteration 6000\t train_loss: 0.32 took: 1.74s\n",
            "Epoch 1, Iteration 6200\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 6400\t train_loss: 0.32 took: 1.74s\n",
            "Epoch 1, Iteration 6600\t train_loss: 0.33 took: 1.76s\n",
            "Epoch 1, Iteration 6800\t train_loss: 0.33 took: 1.71s\n",
            "Epoch 1, Iteration 7000\t train_loss: 0.31 took: 1.72s\n",
            "Epoch 1, Iteration 7200\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 7400\t train_loss: 0.32 took: 1.77s\n",
            "Epoch 1, Iteration 7600\t train_loss: 0.31 took: 1.76s\n",
            "Epoch 1, Iteration 7800\t train_loss: 0.32 took: 1.75s\n",
            "Epoch 1, Iteration 8000\t train_loss: 0.31 took: 1.75s\n",
            "Epoch 1, Iteration 8200\t train_loss: 0.31 took: 1.72s\n",
            "Epoch 1, Iteration 8400\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 8600\t train_loss: 0.30 took: 1.73s\n",
            "Epoch 1, Iteration 8800\t train_loss: 0.32 took: 1.73s\n",
            "Epoch 1, Iteration 9000\t train_loss: 0.31 took: 1.73s\n",
            "Epoch 1, Iteration 9200\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 9400\t train_loss: 0.30 took: 1.76s\n",
            "Epoch 1, Iteration 9600\t train_loss: 0.30 took: 1.79s\n",
            "Epoch 1, Iteration 9800\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 1, Iteration 10000\t train_loss: 0.28 took: 1.72s\n",
            "Epoch 1, Iteration 10200\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 10400\t train_loss: 0.30 took: 1.73s\n",
            "Epoch 1, Iteration 10600\t train_loss: 0.30 took: 1.78s\n",
            "Epoch 1, Iteration 10800\t train_loss: 0.32 took: 1.73s\n",
            "Epoch 1, Iteration 11000\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 1, Iteration 11200\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 11400\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 1, Iteration 11600\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 11800\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 1, Iteration 12000\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 12200\t train_loss: 0.32 took: 1.77s\n",
            "Epoch 1, Iteration 12400\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 1, Iteration 12600\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 1, Iteration 12800\t train_loss: 0.30 took: 1.73s\n",
            "Epoch 1, Iteration 13000\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 13200\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 13400\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 1, Iteration 13600\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 13800\t train_loss: 0.30 took: 1.76s\n",
            "Epoch 1, Iteration 14000\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 1, Iteration 14200\t train_loss: 0.32 took: 1.74s\n",
            "Epoch 1, Iteration 14400\t train_loss: 0.31 took: 1.77s\n",
            "Epoch 1, Iteration 14600\t train_loss: 0.29 took: 1.78s\n",
            "Epoch 1, Iteration 14800\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 15000\t train_loss: 0.32 took: 1.75s\n",
            "Epoch 1, Iteration 15200\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 1, Iteration 15400\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 1, Iteration 15600\t train_loss: 0.30 took: 1.76s\n",
            "Epoch 1, Iteration 15800\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 16000\t train_loss: 0.31 took: 1.76s\n",
            "Epoch 1, Iteration 16200\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 1, Iteration 16400\t train_loss: 0.31 took: 1.73s\n",
            "Epoch 1, Iteration 16600\t train_loss: 0.31 took: 1.76s\n",
            "Epoch 1, Iteration 16800\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 1, Iteration 17000\t train_loss: 0.30 took: 1.79s\n",
            "Epoch 1, Iteration 17200\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 1, Iteration 17400\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 1, Iteration 17600\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 1, Iteration 17800\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 1, Iteration 18000\t train_loss: 0.28 took: 1.72s\n",
            "Epoch 1, Iteration 18200\t train_loss: 0.32 took: 1.75s\n",
            "Epoch 1, Iteration 18400\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 1, Iteration 18600\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 1, Iteration 18800\t train_loss: 0.30 took: 1.78s\n",
            "Epoch 1, Iteration 19000\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 1, Iteration 19200\t train_loss: 0.30 took: 1.76s\n",
            "Epoch 1, Iteration 19400\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 1, Iteration 19600\t train_loss: 0.28 took: 1.71s\n",
            "Epoch 1, Iteration 19800\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 1, Iteration 20000\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 1, Iteration 20200\t train_loss: 0.27 took: 1.73s\n",
            "Epoch 1, Iteration 20400\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 20600\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 1, Iteration 20800\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 1, Iteration 21000\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 1, Iteration 21200\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 1, Iteration 21400\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 21600\t train_loss: 0.31 took: 1.79s\n",
            "Epoch 1, Iteration 21800\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 1, Iteration 22000\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 1, Iteration 22200\t train_loss: 0.29 took: 1.79s\n",
            "Epoch 1, Iteration 22400\t train_loss: 0.27 took: 1.71s\n",
            "Epoch 1, Iteration 22600\t train_loss: 0.32 took: 1.76s\n",
            "Epoch 1, Iteration 22800\t train_loss: 0.30 took: 1.78s\n",
            "Epoch 1, Iteration 23000\t train_loss: 0.31 took: 1.74s\n",
            "Validation loss = 0.29\n",
            "Epoch 2, Iteration 200\t train_loss: 0.30 took: 1.88s\n",
            "Epoch 2, Iteration 400\t train_loss: 0.32 took: 1.78s\n",
            "Epoch 2, Iteration 600\t train_loss: 0.30 took: 1.78s\n",
            "Epoch 2, Iteration 800\t train_loss: 0.28 took: 1.82s\n",
            "Epoch 2, Iteration 1000\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 2, Iteration 1200\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 2, Iteration 1400\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 2, Iteration 1600\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 2, Iteration 1800\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 2, Iteration 2000\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 2, Iteration 2200\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 2, Iteration 2400\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 2600\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 2800\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 3000\t train_loss: 0.27 took: 1.73s\n",
            "Epoch 2, Iteration 3200\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 2, Iteration 3400\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 2, Iteration 3600\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 2, Iteration 3800\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 2, Iteration 4000\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 2, Iteration 4200\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 2, Iteration 4400\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 2, Iteration 4600\t train_loss: 0.31 took: 1.75s\n",
            "Epoch 2, Iteration 4800\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 2, Iteration 5000\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 2, Iteration 5200\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 2, Iteration 5400\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 2, Iteration 5600\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 2, Iteration 5800\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 2, Iteration 6000\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 2, Iteration 6200\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 2, Iteration 6400\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 2, Iteration 6600\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 2, Iteration 6800\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 2, Iteration 7000\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 2, Iteration 7200\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 2, Iteration 7400\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 2, Iteration 7600\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 7800\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 2, Iteration 8000\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 2, Iteration 8200\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 2, Iteration 8400\t train_loss: 0.30 took: 1.76s\n",
            "Epoch 2, Iteration 8600\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 2, Iteration 8800\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 2, Iteration 9000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 2, Iteration 9200\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 2, Iteration 9400\t train_loss: 0.31 took: 1.76s\n",
            "Epoch 2, Iteration 9600\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 2, Iteration 9800\t train_loss: 0.29 took: 1.71s\n",
            "Epoch 2, Iteration 10000\t train_loss: 0.30 took: 1.73s\n",
            "Epoch 2, Iteration 10200\t train_loss: 0.30 took: 1.80s\n",
            "Epoch 2, Iteration 10400\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 2, Iteration 10600\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 2, Iteration 10800\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 2, Iteration 11000\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 2, Iteration 11200\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 2, Iteration 11400\t train_loss: 0.29 took: 1.80s\n",
            "Epoch 2, Iteration 11600\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 2, Iteration 11800\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 2, Iteration 12000\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 2, Iteration 12200\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 2, Iteration 12400\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 2, Iteration 12600\t train_loss: 0.30 took: 1.78s\n",
            "Epoch 2, Iteration 12800\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 13000\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 2, Iteration 13200\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 2, Iteration 13400\t train_loss: 0.28 took: 1.72s\n",
            "Epoch 2, Iteration 13600\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 2, Iteration 13800\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 2, Iteration 14000\t train_loss: 0.28 took: 1.73s\n",
            "Epoch 2, Iteration 14200\t train_loss: 0.28 took: 1.73s\n",
            "Epoch 2, Iteration 14400\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 14600\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 2, Iteration 14800\t train_loss: 0.27 took: 1.72s\n",
            "Epoch 2, Iteration 15000\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 2, Iteration 15200\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 2, Iteration 15400\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 2, Iteration 15600\t train_loss: 0.26 took: 1.71s\n",
            "Epoch 2, Iteration 15800\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 2, Iteration 16000\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 2, Iteration 16200\t train_loss: 0.28 took: 1.80s\n",
            "Epoch 2, Iteration 16400\t train_loss: 0.29 took: 1.79s\n",
            "Epoch 2, Iteration 16600\t train_loss: 0.29 took: 1.82s\n",
            "Epoch 2, Iteration 16800\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 2, Iteration 17000\t train_loss: 0.28 took: 1.80s\n",
            "Epoch 2, Iteration 17200\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 17400\t train_loss: 0.28 took: 1.82s\n",
            "Epoch 2, Iteration 17600\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 2, Iteration 17800\t train_loss: 0.26 took: 1.80s\n",
            "Epoch 2, Iteration 18000\t train_loss: 0.28 took: 1.80s\n",
            "Epoch 2, Iteration 18200\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 2, Iteration 18400\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 2, Iteration 18600\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 2, Iteration 18800\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 2, Iteration 19000\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 2, Iteration 19200\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 2, Iteration 19400\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 2, Iteration 19600\t train_loss: 0.25 took: 1.74s\n",
            "Epoch 2, Iteration 19800\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 2, Iteration 20000\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 2, Iteration 20200\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 2, Iteration 20400\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 2, Iteration 20600\t train_loss: 0.27 took: 1.73s\n",
            "Epoch 2, Iteration 20800\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 2, Iteration 21000\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 2, Iteration 21200\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 2, Iteration 21400\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 21600\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 2, Iteration 21800\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 2, Iteration 22000\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 2, Iteration 22200\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 2, Iteration 22400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 2, Iteration 22600\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 2, Iteration 22800\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 2, Iteration 23000\t train_loss: 0.28 took: 1.74s\n",
            "Validation loss = 0.43\n",
            "Epoch 3, Iteration 200\t train_loss: 0.27 took: 1.86s\n",
            "Epoch 3, Iteration 400\t train_loss: 0.27 took: 1.81s\n",
            "Epoch 3, Iteration 600\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 3, Iteration 800\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 3, Iteration 1000\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 3, Iteration 1200\t train_loss: 0.24 took: 1.80s\n",
            "Epoch 3, Iteration 1400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 3, Iteration 1600\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 3, Iteration 1800\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 3, Iteration 2000\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 3, Iteration 2200\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 3, Iteration 2400\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 3, Iteration 2600\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 3, Iteration 2800\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 3, Iteration 3000\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 3, Iteration 3200\t train_loss: 0.25 took: 1.74s\n",
            "Epoch 3, Iteration 3400\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 3, Iteration 3600\t train_loss: 0.27 took: 1.80s\n",
            "Epoch 3, Iteration 3800\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 3, Iteration 4000\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 3, Iteration 4200\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 3, Iteration 4400\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 3, Iteration 4600\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 3, Iteration 4800\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 3, Iteration 5000\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 3, Iteration 5200\t train_loss: 0.27 took: 1.80s\n",
            "Epoch 3, Iteration 5400\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 3, Iteration 5600\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 3, Iteration 5800\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 3, Iteration 6000\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 3, Iteration 6200\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 3, Iteration 6400\t train_loss: 0.29 took: 1.78s\n",
            "Epoch 3, Iteration 6600\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 3, Iteration 6800\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 3, Iteration 7000\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 3, Iteration 7200\t train_loss: 0.25 took: 1.73s\n",
            "Epoch 3, Iteration 7400\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 3, Iteration 7600\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 3, Iteration 7800\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 3, Iteration 8000\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 3, Iteration 8200\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 3, Iteration 8400\t train_loss: 0.27 took: 1.71s\n",
            "Epoch 3, Iteration 8600\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 3, Iteration 8800\t train_loss: 0.28 took: 1.73s\n",
            "Epoch 3, Iteration 9000\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 3, Iteration 9200\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 3, Iteration 9400\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 3, Iteration 9600\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 3, Iteration 9800\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 3, Iteration 10000\t train_loss: 0.30 took: 1.76s\n",
            "Epoch 3, Iteration 10200\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 3, Iteration 10400\t train_loss: 0.30 took: 1.81s\n",
            "Epoch 3, Iteration 10600\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 3, Iteration 10800\t train_loss: 0.29 took: 1.78s\n",
            "Epoch 3, Iteration 11000\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 3, Iteration 11200\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 3, Iteration 11400\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 3, Iteration 11600\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 3, Iteration 11800\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 3, Iteration 12000\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 3, Iteration 12200\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 3, Iteration 12400\t train_loss: 0.30 took: 1.78s\n",
            "Epoch 3, Iteration 12600\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 3, Iteration 12800\t train_loss: 0.30 took: 1.79s\n",
            "Epoch 3, Iteration 13000\t train_loss: 0.27 took: 1.73s\n",
            "Epoch 3, Iteration 13200\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 3, Iteration 13400\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 3, Iteration 13600\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 3, Iteration 13800\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 3, Iteration 14000\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 3, Iteration 14200\t train_loss: 0.33 took: 1.76s\n",
            "Epoch 3, Iteration 14400\t train_loss: 0.31 took: 1.76s\n",
            "Epoch 3, Iteration 14600\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 3, Iteration 14800\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 3, Iteration 15000\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 3, Iteration 15200\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 3, Iteration 15400\t train_loss: 0.28 took: 1.80s\n",
            "Epoch 3, Iteration 15600\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 3, Iteration 15800\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 3, Iteration 16000\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 3, Iteration 16200\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 3, Iteration 16400\t train_loss: 0.26 took: 1.79s\n",
            "Epoch 3, Iteration 16600\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 3, Iteration 16800\t train_loss: 0.24 took: 1.78s\n",
            "Epoch 3, Iteration 17000\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 3, Iteration 17200\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 3, Iteration 17400\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 3, Iteration 17600\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 3, Iteration 17800\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 3, Iteration 18000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 3, Iteration 18200\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 3, Iteration 18400\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 3, Iteration 18600\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 3, Iteration 18800\t train_loss: 0.26 took: 1.79s\n",
            "Epoch 3, Iteration 19000\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 3, Iteration 19200\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 3, Iteration 19400\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 3, Iteration 19600\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 3, Iteration 19800\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 3, Iteration 20000\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 3, Iteration 20200\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 3, Iteration 20400\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 3, Iteration 20600\t train_loss: 0.25 took: 1.75s\n",
            "Epoch 3, Iteration 20800\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 3, Iteration 21000\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 3, Iteration 21200\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 3, Iteration 21400\t train_loss: 0.27 took: 1.73s\n",
            "Epoch 3, Iteration 21600\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 3, Iteration 21800\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 3, Iteration 22000\t train_loss: 0.26 took: 1.80s\n",
            "Epoch 3, Iteration 22200\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 3, Iteration 22400\t train_loss: 0.30 took: 1.76s\n",
            "Epoch 3, Iteration 22600\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 3, Iteration 22800\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 3, Iteration 23000\t train_loss: 0.27 took: 1.78s\n",
            "Validation loss = 0.34\n",
            "Epoch 4, Iteration 200\t train_loss: 0.27 took: 1.89s\n",
            "Epoch 4, Iteration 400\t train_loss: 0.25 took: 1.81s\n",
            "Epoch 4, Iteration 600\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 800\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 4, Iteration 1000\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 4, Iteration 1200\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 4, Iteration 1400\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 4, Iteration 1600\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 4, Iteration 1800\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 4, Iteration 2000\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 4, Iteration 2200\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 4, Iteration 2400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 4, Iteration 2600\t train_loss: 0.29 took: 1.78s\n",
            "Epoch 4, Iteration 2800\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 4, Iteration 3000\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 4, Iteration 3200\t train_loss: 0.30 took: 1.76s\n",
            "Epoch 4, Iteration 3400\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 4, Iteration 3600\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 4, Iteration 3800\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 4000\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 4, Iteration 4200\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 4, Iteration 4400\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 4, Iteration 4600\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 4, Iteration 4800\t train_loss: 0.27 took: 1.83s\n",
            "Epoch 4, Iteration 5000\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 4, Iteration 5200\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 4, Iteration 5400\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 4, Iteration 5600\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 4, Iteration 5800\t train_loss: 0.25 took: 1.79s\n",
            "Epoch 4, Iteration 6000\t train_loss: 0.28 took: 1.83s\n",
            "Epoch 4, Iteration 6200\t train_loss: 0.27 took: 1.80s\n",
            "Epoch 4, Iteration 6400\t train_loss: 0.26 took: 1.81s\n",
            "Epoch 4, Iteration 6600\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 4, Iteration 6800\t train_loss: 0.30 took: 1.82s\n",
            "Epoch 4, Iteration 7000\t train_loss: 0.31 took: 1.77s\n",
            "Epoch 4, Iteration 7200\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 4, Iteration 7400\t train_loss: 0.37 took: 1.74s\n",
            "Epoch 4, Iteration 7600\t train_loss: 0.32 took: 1.79s\n",
            "Epoch 4, Iteration 7800\t train_loss: 0.29 took: 1.78s\n",
            "Epoch 4, Iteration 8000\t train_loss: 0.30 took: 1.77s\n",
            "Epoch 4, Iteration 8200\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 4, Iteration 8400\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 4, Iteration 8600\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 4, Iteration 8800\t train_loss: 0.25 took: 1.74s\n",
            "Epoch 4, Iteration 9000\t train_loss: 0.25 took: 1.78s\n",
            "Epoch 4, Iteration 9200\t train_loss: 0.25 took: 1.75s\n",
            "Epoch 4, Iteration 9400\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 4, Iteration 9600\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 4, Iteration 9800\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 4, Iteration 10000\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 4, Iteration 10200\t train_loss: 0.30 took: 1.73s\n",
            "Epoch 4, Iteration 10400\t train_loss: 0.24 took: 1.77s\n",
            "Epoch 4, Iteration 10600\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 4, Iteration 10800\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 4, Iteration 11000\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 4, Iteration 11200\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 4, Iteration 11400\t train_loss: 0.27 took: 1.73s\n",
            "Epoch 4, Iteration 11600\t train_loss: 0.24 took: 1.75s\n",
            "Epoch 4, Iteration 11800\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 4, Iteration 12000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 12200\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 4, Iteration 12400\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 4, Iteration 12600\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 4, Iteration 12800\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 4, Iteration 13000\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 4, Iteration 13200\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 4, Iteration 13400\t train_loss: 0.24 took: 1.74s\n",
            "Epoch 4, Iteration 13600\t train_loss: 0.30 took: 1.74s\n",
            "Epoch 4, Iteration 13800\t train_loss: 0.27 took: 1.73s\n",
            "Epoch 4, Iteration 14000\t train_loss: 0.27 took: 1.80s\n",
            "Epoch 4, Iteration 14200\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 4, Iteration 14400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 4, Iteration 14600\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 14800\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 4, Iteration 15000\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 4, Iteration 15200\t train_loss: 0.30 took: 1.73s\n",
            "Epoch 4, Iteration 15400\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 15600\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 4, Iteration 15800\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 4, Iteration 16000\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 4, Iteration 16200\t train_loss: 0.27 took: 1.73s\n",
            "Epoch 4, Iteration 16400\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 4, Iteration 16600\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 4, Iteration 16800\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 4, Iteration 17000\t train_loss: 0.25 took: 1.72s\n",
            "Epoch 4, Iteration 17200\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 17400\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 4, Iteration 17600\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 17800\t train_loss: 0.25 took: 1.78s\n",
            "Epoch 4, Iteration 18000\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 4, Iteration 18200\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 4, Iteration 18400\t train_loss: 0.25 took: 1.79s\n",
            "Epoch 4, Iteration 18600\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 4, Iteration 18800\t train_loss: 0.24 took: 1.73s\n",
            "Epoch 4, Iteration 19000\t train_loss: 0.25 took: 1.73s\n",
            "Epoch 4, Iteration 19200\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 19400\t train_loss: 0.27 took: 1.72s\n",
            "Epoch 4, Iteration 19600\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 4, Iteration 19800\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 4, Iteration 20000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 20200\t train_loss: 0.26 took: 1.79s\n",
            "Epoch 4, Iteration 20400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 4, Iteration 20600\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 4, Iteration 20800\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 4, Iteration 21000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 4, Iteration 21200\t train_loss: 0.24 took: 1.77s\n",
            "Epoch 4, Iteration 21400\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 4, Iteration 21600\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 4, Iteration 21800\t train_loss: 0.29 took: 1.72s\n",
            "Epoch 4, Iteration 22000\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 4, Iteration 22200\t train_loss: 0.29 took: 1.79s\n",
            "Epoch 4, Iteration 22400\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 4, Iteration 22600\t train_loss: 0.32 took: 1.77s\n",
            "Epoch 4, Iteration 22800\t train_loss: 0.32 took: 1.77s\n",
            "Epoch 4, Iteration 23000\t train_loss: 0.32 took: 1.77s\n",
            "Validation loss = 0.32\n",
            "Epoch 5, Iteration 200\t train_loss: 0.27 took: 1.89s\n",
            "Epoch 5, Iteration 400\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 5, Iteration 600\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 5, Iteration 800\t train_loss: 0.29 took: 1.77s\n",
            "Epoch 5, Iteration 1000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 5, Iteration 1200\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 1400\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 5, Iteration 1600\t train_loss: 0.25 took: 1.75s\n",
            "Epoch 5, Iteration 1800\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 5, Iteration 2000\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 5, Iteration 2200\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 5, Iteration 2400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 5, Iteration 2600\t train_loss: 0.28 took: 1.75s\n",
            "Epoch 5, Iteration 2800\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 3000\t train_loss: 0.28 took: 1.79s\n",
            "Epoch 5, Iteration 3200\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 5, Iteration 3400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 5, Iteration 3600\t train_loss: 0.25 took: 1.74s\n",
            "Epoch 5, Iteration 3800\t train_loss: 0.23 took: 1.75s\n",
            "Epoch 5, Iteration 4000\t train_loss: 0.24 took: 1.75s\n",
            "Epoch 5, Iteration 4200\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 5, Iteration 4400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 5, Iteration 4600\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 5, Iteration 4800\t train_loss: 0.26 took: 1.79s\n",
            "Epoch 5, Iteration 5000\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 5, Iteration 5200\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 5, Iteration 5400\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 5, Iteration 5600\t train_loss: 0.23 took: 1.74s\n",
            "Epoch 5, Iteration 5800\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 6000\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 5, Iteration 6200\t train_loss: 0.29 took: 1.78s\n",
            "Epoch 5, Iteration 6400\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 5, Iteration 6600\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 6800\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 5, Iteration 7000\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 5, Iteration 7200\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 5, Iteration 7400\t train_loss: 0.28 took: 1.74s\n",
            "Epoch 5, Iteration 7600\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 5, Iteration 7800\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 5, Iteration 8000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 5, Iteration 8200\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 5, Iteration 8400\t train_loss: 0.32 took: 1.74s\n",
            "Epoch 5, Iteration 8600\t train_loss: 0.29 took: 1.75s\n",
            "Epoch 5, Iteration 8800\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 5, Iteration 9000\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 5, Iteration 9200\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 5, Iteration 9400\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 5, Iteration 9600\t train_loss: 0.28 took: 1.72s\n",
            "Epoch 5, Iteration 9800\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 5, Iteration 10000\t train_loss: 0.25 took: 1.74s\n",
            "Epoch 5, Iteration 10200\t train_loss: 0.25 took: 1.72s\n",
            "Epoch 5, Iteration 10400\t train_loss: 0.28 took: 1.73s\n",
            "Epoch 5, Iteration 10600\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 5, Iteration 10800\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 5, Iteration 11000\t train_loss: 0.26 took: 1.80s\n",
            "Epoch 5, Iteration 11200\t train_loss: 0.26 took: 1.80s\n",
            "Epoch 5, Iteration 11400\t train_loss: 0.29 took: 1.74s\n",
            "Epoch 5, Iteration 11600\t train_loss: 0.29 took: 1.76s\n",
            "Epoch 5, Iteration 11800\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 12000\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 5, Iteration 12200\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 5, Iteration 12400\t train_loss: 0.24 took: 1.78s\n",
            "Epoch 5, Iteration 12600\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 12800\t train_loss: 0.28 took: 1.78s\n",
            "Epoch 5, Iteration 13000\t train_loss: 0.25 took: 1.75s\n",
            "Epoch 5, Iteration 13200\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 13400\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 5, Iteration 13600\t train_loss: 0.26 took: 1.80s\n",
            "Epoch 5, Iteration 13800\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 5, Iteration 14000\t train_loss: 0.25 took: 1.79s\n",
            "Epoch 5, Iteration 14200\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 5, Iteration 14400\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 5, Iteration 14600\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 5, Iteration 14800\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 5, Iteration 15000\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 5, Iteration 15200\t train_loss: 0.24 took: 1.76s\n",
            "Epoch 5, Iteration 15400\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 5, Iteration 15600\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 5, Iteration 15800\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 5, Iteration 16000\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 5, Iteration 16200\t train_loss: 0.28 took: 1.84s\n",
            "Epoch 5, Iteration 16400\t train_loss: 0.26 took: 1.81s\n",
            "Epoch 5, Iteration 16600\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 5, Iteration 16800\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 5, Iteration 17000\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 5, Iteration 17200\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 5, Iteration 17400\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 5, Iteration 17600\t train_loss: 0.31 took: 1.78s\n",
            "Epoch 5, Iteration 17800\t train_loss: 0.32 took: 1.79s\n",
            "Epoch 5, Iteration 18000\t train_loss: 0.29 took: 1.78s\n",
            "Epoch 5, Iteration 18200\t train_loss: 0.29 took: 1.87s\n",
            "Epoch 5, Iteration 18400\t train_loss: 0.27 took: 1.85s\n",
            "Epoch 5, Iteration 18600\t train_loss: 0.28 took: 1.83s\n",
            "Epoch 5, Iteration 18800\t train_loss: 0.26 took: 1.80s\n",
            "Epoch 5, Iteration 19000\t train_loss: 0.27 took: 1.82s\n",
            "Epoch 5, Iteration 19200\t train_loss: 0.27 took: 1.77s\n",
            "Epoch 5, Iteration 19400\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 5, Iteration 19600\t train_loss: 0.26 took: 1.81s\n",
            "Epoch 5, Iteration 19800\t train_loss: 0.26 took: 1.79s\n",
            "Epoch 5, Iteration 20000\t train_loss: 0.27 took: 1.76s\n",
            "Epoch 5, Iteration 20200\t train_loss: 0.29 took: 1.79s\n",
            "Epoch 5, Iteration 20400\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 20600\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 5, Iteration 20800\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 5, Iteration 21000\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 5, Iteration 21200\t train_loss: 0.25 took: 1.75s\n",
            "Epoch 5, Iteration 21400\t train_loss: 0.25 took: 1.78s\n",
            "Epoch 5, Iteration 21600\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 5, Iteration 21800\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 5, Iteration 22000\t train_loss: 0.25 took: 1.75s\n",
            "Epoch 5, Iteration 22200\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 5, Iteration 22400\t train_loss: 0.28 took: 1.77s\n",
            "Epoch 5, Iteration 22600\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 5, Iteration 22800\t train_loss: 0.25 took: 1.79s\n",
            "Epoch 5, Iteration 23000\t train_loss: 0.27 took: 1.79s\n",
            "Validation loss = 0.28\n",
            "Training finished, took 1019.98s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNSsFzVji6W",
        "colab_type": "code",
        "outputId": "73d95626-e1af-4cec-db47-5e57b1f99b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "# plt.xlim(20000,120000)\n",
        "# plt.ylim(0,.4)\n",
        "plt.show()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEKCAYAAAA8QgPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXeYFFXWh9/bPXmYBAxxCEOOQ04C\nAoKKopgVw65pdc1pV8U1frru6uoaUFzXNa6JRQygoigIYkRAyTkzxCFMzj33+6OquqvDBIZuJvR5\nn2ee6a6qrrpV1X1/de4JV2mtEQRBEIRjxVHXDRAEQRAaJiIggiAIQq0QAREEQRBqhQiIIAiCUCtE\nQARBEIRaIQIiCIIg1AoREEEQBKFWiIAIgiAItUIERBAEQagVEXXdgGOlefPmumPHjnXdDEEQhAbF\n8uXLD2mtU4O5zwYnIB07dmTZsmV13QxBEIQGhVJqZ7D3KUNYgiAIQq0QAREEQRBqhQiIIAiCUCsa\nnA9EEIQTT1lZGZmZmRQXF9d1U4RqiImJIS0tjcjIyJAfSwREEIRqyczMJCEhgY4dO6KUquvmCJWg\ntebw4cNkZmaSnp4e8uPJEJYgCNVSXFxMs2bNRDzqOUopmjVrdsIsRREQQRBqhIhHw+BE3qewEZCl\nO47wz682UlpeUddNEQRBaBSEjYD8uvMoL3yzhfIKERBBaGhkZ2fz0ksv1eqzZ555JtnZ2TXe/pFH\nHuHpp5+u1bHCjbAREMuqq9B12w5BEI6dqgSkvLy8ys/OnTuX5OTkUDQr7AkbAXGYCqK1KIggNDSm\nTp3K1q1b6d+/P3fffTeLFi1i9OjRTJ48mV69egFw7rnnMmjQIHr37s0rr7zi/mzHjh05dOgQO3bs\noGfPnlx33XX07t2b0047jaKioiqPu2LFCoYPH05GRgbnnXceR48eBWDatGn06tWLjIwMpkyZAsC3\n335L//796d+/PwMGDCAvLy9EV6P+EHZhvGKBCMLx8X+frmXd3tyg7rNXm0QePrt3peufeOIJ1qxZ\nw4oVKwBYtGgRv/76K2vWrHGHq77++us0bdqUoqIihgwZwgUXXECzZs289rN582bef/99/vOf/3Dx\nxRfz4YcfcsUVV1R63N///ve88MILjBkzhoceeoj/+7//47nnnuOJJ55g+/btREdHu4fHnn76aaZP\nn87IkSPJz88nJibmeC9LvSfsLBBEQAShUTB06FCvXIdp06bRr18/hg8fzu7du9m8ebPfZ9LT0+nf\nvz8AgwYNYseOHZXuPycnh+zsbMaMGQPAlVdeyeLFiwHIyMjg8ssv55133iEiwngOHzlyJHfddRfT\npk0jOzvbvbwx0/jP0MTjAxEFEYTjoSpL4UQSHx/vfr1o0SLmz5/PTz/9RFxcHGPHjg2YCxEdHe1+\n7XQ6qx3CqozPP/+cxYsX8+mnn/L444+zevVqpk6dyqRJk5g7dy4jR45k3rx59OjRo1b7byiEnQUi\n8iEIDY+EhIQqfQo5OTmkpKQQFxfHhg0b+Pnnn4/7mElJSaSkpPDdd98B8PbbbzNmzBgqKirYvXs3\n48aN48knnyQnJ4f8/Hy2bt1K3759uffeexkyZAgbNmw47jbUd8QCEQSh3tOsWTNGjhxJnz59OOOM\nM5g0aZLX+okTJ/Lyyy/Ts2dPunfvzvDhw4Ny3LfeeosbbriBwsJCOnXqxBtvvIHL5eKKK64gJycH\nrTW33XYbycnJPPjggyxcuBCHw0Hv3r0544wzgtKG+oxqaFFJgwcP1rWZUOrtn3fy4CdrWHr/BFIT\noqv/gCAIbtavX0/Pnj3ruhlCDQl0v5RSy7XWg4N5nLAZwrKS+xuaYAqCINRXwkZAxAciCIIQXMJG\nQMQHIgiCEFzCRkAcVhqI6IcgCEJQCBsBUaYXRCwQQRCE4BA+AiIWiCAIQlAJIwGxiinWcUMEQTgh\nNGnSBIC9e/dy4YUXBtxm7NixVJcW8Nxzz1FYWOh+f6zl4SujMZSNDxsBcftAJA5LEMKKNm3aMGvW\nrFp/3ldApDy8h7AREJkPRBAaLlOnTmX69Onu99bTe35+PuPHj2fgwIH07duX2bNn+312x44d9OnT\nB4CioiKmTJlCz549Oe+887xqYd14440MHjyY3r178/DDDwNGgca9e/cybtw4xo0bB3jKwwM888wz\n9OnThz59+vDcc8+5jxcuZePDppSJzAciCEHii6mwf3Vw99mqL5zxRKWrL7nkEu644w5uvvlmAGbO\nnMm8efOIiYnh448/JjExkUOHDjF8+HAmT55c6bzg//rXv4iLi2P9+vWsWrWKgQMHutc9/vjjNG3a\nFJfLxfjx41m1ahW33XYbzzzzDAsXLqR58+Ze+1q+fDlvvPEGS5YsQWvNsGHDGDNmDCkpKWFTNj5s\nLBALsUAEoeExYMAADh48yN69e1m5ciUpKSm0a9cOrTV/+ctfyMjIYMKECezZs4cDBw5Uup/Fixe7\nO/KMjAwyMjLc62bOnMnAgQMZMGAAa9euZd26dVW26fvvv+e8884jPj6eJk2acP7557sLL4ZL2fiw\ns0AkF10QjpMqLIVQctFFFzFr1iz279/PJZdcAsC7775LVlYWy5cvJzIyko4dOwYs414d27dv5+mn\nn2bp0qWkpKRw1VVX1Wo/FuFSNj5sLBDxgQhCw+aSSy5hxowZzJo1i4suuggwnt5btGhBZGQkCxcu\nZOfOnVXu4+STT+a9994DYM2aNaxatQqA3Nxc4uPjSUpK4sCBA3zxxRfuz1RWSn706NF88sknFBYW\nUlBQwMcff8zo0aOP+bwactn4kFogSqmJwPOAE3hVa/2Ez/pngXHm2zighdY6JOENDgnjFYQGTe/e\nvcnLy6Nt27a0bt0agMsvv5yzzz6bvn37Mnjw4GqfxG+88UauvvpqevbsSc+ePRk0aBAA/fr1Y8CA\nAfTo0YN27doxcuRI92euv/56Jk6cSJs2bVi4cKF7+cCBA7nqqqsYOnQoAH/4wx8YMGBAlcNVldFQ\ny8aHrJy7UsoJbAJOBTKBpcClWuuAA4tKqVuBAVrra6rab23LuX+xeh83vvsrX9w+mp6tE4/584IQ\nzkg594ZFYyjnPhTYorXeprUuBWYA51Sx/aXA+6FqjGSiC4IgBJdQCkhbYLftfaa5zA+lVAcgHfim\nkvXXK6WWKaWWZWVl1aoxVlif1MISBEEIDvXFiT4FmKW1dgVaqbV+RWs9WGs9ODU1tVYHCBwVLghC\nTZEcqobBibxPoRSQPUA72/s0c1kgphDC4SsQJ7ogHA8xMTEcPnxYRKSeo7Xm8OHDJyy5MJRRWEuB\nrkqpdAzhmAJc5ruRUqoHkAL8FMK2yIRSgnAcpKWlkZmZSW2HkIUTR0xMDGlpaSfkWCETEK11uVLq\nFmAeRhjv61rrtUqpR4FlWus55qZTgBk6xI82MqWtINSeyMhI0tPT67oZQj0jpHkgWuu5wFyfZQ/5\nvH8klG1wIxaIIAhCUKkvTvSQIz4QQRCE4BI2AuKuhCUKIgiCEBTCRkDEByIIghBcwkZA3FFYUk1R\nEAQhKISdgIh8CIIgBIfwERCklIkgCEIwCRsBcch8UoIgCEElbATEU0yxjhsiCILQSAgbAXG4fSCi\nIIIgCMEgbAREprQVBEEILmEkIFYmuiiIIAhCMAgfATH/i34IgiAEh7AREE8muiiIIAhCMAgbAfFk\notdtOwRBEBoLYSMgUgtLEAQhuISNgFhIJrogCEJwCBsBkflABEEQgkvYCIi7mKIoiCAIQlAIGwER\nH4ggCEJwCRsBUTInuiAIQlAJGwFx18IS/RAEQQgKYSMgyHwggiAIQSVsBMQ9H4ggCIIQFMJGQDzz\ngYgFIgiCEAzCRkDEByIIghBcwkZAPHOi13FDBEEQGgnhIyCSSCgIghBUQiogSqmJSqmNSqktSqmp\nlWxzsVJqnVJqrVLqvdC1xfgv+iEIghAcIkK1Y6WUE5gOnApkAkuVUnO01uts23QF7gNGaq2PKqVa\nhKo9Mh+IIAhCcAmlBTIU2KK13qa1LgVmAOf4bHMdMF1rfRRAa30wVI2ROdEFQRCCSygFpC2w2/Y+\n01xmpxvQTSn1g1LqZ6XUxFA1RqrxCoIgBJeQDWEdw/G7AmOBNGCxUqqv1jrbvpFS6nrgeoD27dvX\n6kBWHqHkgQiCIASHUFoge4B2tvdp5jI7mcAcrXWZ1no7sAlDULzQWr+itR6stR6cmppaq8YoqcYr\nCIIQVEIpIEuBrkqpdKVUFDAFmOOzzScY1gdKqeYYQ1rbQtEYCeMVBEEILiETEK11OXALMA9YD8zU\nWq9VSj2qlJpsbjYPOKyUWgcsBO7WWh8ORXvEByIIghBcQuoD0VrPBeb6LHvI9loDd5l/IUV8IIIg\nCMElbDLRxQIRBEEILmEjIMiMhIIgCEElbARE5gMRBEEILmEjIDIfiCAIQnAJGwGR+UAEQRCCS9gI\niMwHIgiCEFzCR0AsC0Ry0QVBEIJC+AmI6IcgCEJQCB8BwcoDEQURBEEIBmEjIOJEFwRBCC5hIyCe\nMN46boggCEIjIWwExCFOdEEQhKASNgIiFoggCEJwCRsBATMSS5wggiAIQSG8BASxQARBEIJFWAmI\nQynxgQiCIASJsBIQpcQCEQRBCBZhJiBKXCCCIAhBIrwEBMlEFwRBCBZhJSCGD0QQBEEIBmElIEpB\nhThBBEEQgkJYCYhYIIIgCMEjrATEyAMRCREEQQgGNRIQpVRnpVS0+XqsUuo2pVRyaJsWfJSSRHRB\nEIRgUVML5EPApZTqArwCtAPeC1mrQkSE00F5RUVdN0MQBKFRUFMBqdBalwPnAS9ore8GWoeuWaEh\nwqEod4kJIgiCEAxqKiBlSqlLgSuBz8xlkaFpUuiIdDooEwERBEEICjUVkKuBEcDjWuvtSql04O3Q\nNSs0RDiVDGEJgiAEiRoJiNZ6ndb6Nq31+0qpFCBBa/1kdZ9TSk1USm1USm1RSk0NsP4qpVSWUmqF\n+feHWpxDjZEhLEEQhOARUZONlFKLgMnm9suBg0qpH7TWd1XxGScwHTgVyASWKqXmaK3X+Wz6P631\nLbVp/LFiDGGJBSIIghAMajqElaS1zgXOB/6rtR4GTKjmM0OBLVrrbVrrUmAGcE7tm3r8GENYYoEI\ngiAEg5oKSIRSqjVwMR4nenW0BXbb3meay3y5QCm1Sik1SynVLtCOlFLXK6WWKaWWZWVl1fDw/kQ4\nxAIRBEEIFjUVkEeBecBWrfVSpVQnYHMQjv8p0FFrnQF8DbwVaCOt9Sta68Fa68Gpqam1PlikU3wg\ngiAIwaJGPhCt9QfAB7b324ALqvnYHoyEQ4s0c5l9v4dtb18F/lGT9tSWCIckEgqCIASLmpYySVNK\nfayUOmj+faiUSqvmY0uBrkqpdKVUFDAFmOOzX3sy4mRg/bE0/liJcCrJAxEEQQgSNR3CegOj829j\n/n1qLqsUM3P9Foyhr/XATK31WqXUo0qpyeZmtyml1iqlVgK3AVcd+ynUnEgpZSIIghA0ajSEBaRq\nre2C8aZS6o7qPqS1ngvM9Vn2kO31fcB9NWzDcSN5IIIgCMGjphbIYaXUFUopp/l3BXC42k/VMyQP\nRBAEIXjUVECuwQjh3Q/sAy4kxMNNoUDyQARBEIJHTUuZ7NRaT9Zap2qtW2itz6X6KKx6R4TDIUNY\ngiAIQeJ4ZiSstIxJfSXSqWQISxAEIUgcj4CooLXiBCFDWIIgCMHjeASkwfXEUspEEAQheFQZxquU\nyiOwUCggNiQtCiFSykQQBCF4VCkgWuuEE9WQE4HMiS4IghA8jmcIq8ER6TBKmWgtVoggCMLxElYC\nEuE0Tlcc6YIgCMdPmAmIETgmfhBBEITjJ6wEJNJhnG6Z+EEEQRCOm7ASELFABEEQgkdYCUhUhGmB\nSC6IIAjCcRNWApIYEwlAblFZHbdEEASh4RNWApIcZwjI0UIREEEQhOMlrAQkJS4KgOzC0jpuiSAI\nQsMnrAQkKdawQLLFAmm8FOeCRNkJwgkhrAQkJd60QIrEAmmUrP0Ynu4K/z4ZtswHqTggCCElrAQk\nPspJhEOJBdLY0Bq++yd8cBW06AklufDOBfDfybD3t7punSA0WsJKQJRSJMdFiRO9MeEqgzm3wIJH\noc8FcPWXcMtSmPgE7F8Dr4yFWdfAkW113VJBaHSElYAAJMREkF9SXtfNEIJBUTa8cz789g6cfDec\n/ypExkBENAy/EW5fAaP/DBvmwotDYe49kJ9V160WhEZD2AlIXJSTolIRkAbP0R3w2mmw8yc4919w\nygPg8Pk6xyTB+Afhtt9gwOWw9FWY1h++/QeU5NdJswWhMRGWAlJQ4qrrZgjHw+6l8J/xkL8ffvcx\n9L+s6u0TW8PZz8NNP0OnsbDwcZg2AJa+ZgyBCYJQK8JQQCIoLBMBabCs/RjeOguim8AfFkD66Jp/\nNrUbTHkXrv0amnWGz++C6cNg7ScSsSUItSDsBCQ+2kmh+EAaHlrDd88YkVat+xni0bxr7fbVbihc\n/QVcOgOckfDBlfDqBNjxQ1CbLAiNnbATkNjICApLxQJpULjKYM6tsOD/jEir38+B+ObHt0+loPsZ\ncMMPMPlFyN0Lb54J714MB9YFp92C0MgJqYAopSYqpTYqpbYopaZWsd0FSimtlBocyvaAaYGIE73h\nUJRt5HT89rZ3pFWwcEbAwN/BrcthwiOw62f410nwyU2Qkxm84whCIyRkAqKUcgLTgTOAXsClSqle\nAbZLAG4HloSqLXbioiIoEAukYeCOtPoRznkpcKRVsIiKg1F3GqG/I26G1R/AtIHw1YNQdDQ0xxSE\nBk4oLZChwBat9TatdSkwAzgnwHaPAU8CxSFsi5u4KCel5RWUy5wg9RuvSKuPjDDcE0FcUzj9ccMi\n6XM+/PgCPN8Pfngeyk7IV1QQGgyhFJC2wG7b+0xzmRul1ECgndb68xC2w4u4KCeARGLVZ9Z+4om0\nunY+pJ984tuQ3B7Oexlu+A7ShsLXD8ELg+C3d6FCvjuCAHXoRFdKOYBngD/VYNvrlVLLlFLLsrKO\nL5M4LioCgELJBal/aA3fP2tERVmRVqnd6rZNrfrCFbPgyk+hSSrMvgleHgWb5knorxD2hFJA9gDt\nbO/TzGUWCUAfYJFSagcwHJgTyJGutX5Faz1Yaz04NTX1uBoVH21aIOJIr19YkVbzH4He5wcn0iqY\npJ8M1y2EC9+AsiJ472J48yzIXFbXLROEOiOUArIU6KqUSldKRQFTgDnWSq11jta6uda6o9a6I/Az\nMFlrHdJfpNsCEUd6/cEeaTX6z3DBa8GNtAoWShl+kZt/gTOfhqwN8Op4mPl7OLSlrlsnCCeckAmI\n1rocuAWYB6wHZmqt1yqlHlVKTQ7VcavD8oEUSDJh/eDoDnj9dNj5gxFpNf7B0EVaBYuIKBh6nRGx\nNWYqbJ4P04fCZ3dB3oG6bp0gnDAiQrlzrfVcYK7Psocq2XZsKNtiIU70ekTmMnh/CrhKjZpWdeEs\nPx6iE2DcfTD4Glj8D1j+JqycASfdAifdaqwXhEZMPX/UCz7x0eJErxes/QTenASRcXUXaRUsElrC\npH8aQ1tdT4Vvn4Tn+8OSV6BcZr8UGi9hJyCxkeJEr1PskVatMuC6b+o+0ipYNOsMF78Ff/jGmBnx\ni7uNoa01H8o87UKjJOwExLJA7p61ih2HCuq4NWGGqww+vc0TaXXlp/Ur0ipYpA0yzu3yWYaFNesa\n+M842PZtXbdMEIJK2AmI5QMBmPbN5jpsSZhRlA3vXgi//rd+R1oFC6WM4awbvoNzX4aCQ8Yc7W+f\nD/tX13XrBCEohJ2AREd4TnnzAZmV7oRwdKcRabXjezhnesOItAoWDif0v9QojXLqY7BnObw8Gj66\n3rgugtCACZNfsQellPv14fySOmxJmJC5zMiVyNsHV3wEA66o6xbVDZExMPI2I/R35G1GEMGLg2He\n/VB4pK5b52bJtsNMWyCWuVAzwk5A7OzPLaZMiiqGjnWzvSOtOo2p6xbVPbEpcOqjcNuvkHEx/PyS\nEbH13TNQWljXreOSV37mma831XUzhAZCWAtIhYb9OUaF1S/X7CenSObHDgpaw/fPGRnajS3SKlgk\npRnDeTf8AB1OMibLemGQ4SNySYSg0DAISwG5aWxnJvRsCcDuI4Us3XGEG95ZzjNfbazjljUC3JFW\nD5uRVvWsplV9o2UvuGwGXDUXktoa9cBeHgkb5tZpsUaZ7kCoCWEpIPdM7MGTF/QFYNWeHOavM8pP\nlFVIddXjojjHFmn1JzPSKrauW9Uw6DgSrv0aLn4bKsphxqXwxhmw64TMs+ZHSbkIiFA9YSkgAM2a\nRNO+aRy/7TrKnuwiAErlR1N7ju40Zg90R1o9FD6RVsFCKeg1GW76Gc56Fo5sg9dPgxmXQ9aJ9UuI\ngAg1Iax/4d1aJrDzcCF5xcaY8yGJyqodmcsl0iqYOCON+lq3/QbjHjASEF8aBnNug9x9J6QJJeVS\n6keonrAWkGbxURwtLCWv2HCeZ+WJgBwz62bDm2eakVZfS6RVMImKhzF3G6G/Q6+HFe/BtAGw4FFj\nuDCEiDUu1ISwFpCU+CgO5Jbw665sQCyQY0JrY55wK9LqDwsgtXtdt6pxEt8czngSblkKPSbBd/80\nQn9/egnKQ/OdlSEsoSaEtYA0jY/0ep+VV0KZqwJXhWbj/rw6alUDwFUGn95uzBPe+zwj0qrJ8c0U\nKdSApulw4Wtw/bfQOgPm3WckI66aCWXFQT1USZkIiFA9IZ0PpL6TEhflfh0T6aC4rIL9OcVMX7iF\nGUt3890945i3dj9Xj0zH6VBV7CmMKM6BmVfCtoVGpNW4B8RZfqJp0x9+Pxu2LDDCpT+6DtQfoWkn\nowpwak/jf4ue0KyL4VOpIREORXmFFh+IUCPCWkCsyrwATeOi2JtTTObRImYs3Q3A379Yz9zV+0mK\njeSiwe0q2034cHSnMRf44S0w+UUY+Lu6blF402U8dBoHm7+Cvb/CwXVwcANs+By0aUE4Ig0RadET\nWvSCFj2M/ykdjTpdPjhNAREfiFATwlpAImxWRZIpIM/ayjjsyTaGBWT+dIxIq/cvMSZIuuJD6DS2\nrlskgGH9dZ9o/FmUFcPhzXBwvUdU9iyHtR95tomIgebdPJZKi16Q2oNIh6YE8YEINSOsBWR8z5Zc\nOrQd7/+ym4m9W5GVV8wvOzyF7WTedJN1s43qsU1awlWfi7O8vhMZA636Gn92SvLh0EZDUA6ug6wN\nRt7Oqv+5N1niiGFzVFua/9wfjgz0iEtCKyNPRRBshLWAOB2Kv5+fwTUj0+mU2oTWyTHcM2uVe31+\ncZgLiNbw4zTDWZ42BKa8L87yhkx0E2g7yPizU5QNWRvh4Dpmf/4VHfROeuxbBDs+9GwTk+S2UryG\nwqRMTVgT1gJi0bVlAgAZaUley30tkOkLt/D+L7v47p5xXmXhGyWuMpj7Z1j+phFpde6/pCxJYyU2\nGdoPg/bDeHZeO7KKSvjHhAwu7hVrDINlbfAMha39GJa/4flsfKq/qKT2MPYpNHpEQGx0at4EgM6p\n8WzNKiDfnDfdZdbIemqeUWxxVWYO/dod/w+kqNRFXkkZLRLq2cx89kirUXfBKWE0AVQImLd2P+v3\n5XLHhPpfkdjyC5a4KgzrIn208WehNeQfMAVlvedvxbtQapugLaGNzb9iRoaldjesIKHRIAJiIyrC\nwZd3jCYlLophf1vgLoZaVGY40VsmRnMgt4Ql2w9XKyBr9+bwy/YjXD0yvdJtrnhtCct3HmXHE5OC\ndg7HTfYuePdiwwkrkVZB4Y9vLwdoEAJihauXlFUSOKKU4Q9JaAWdT/Es1xpydnv7Vw6ug6WvQrkt\nRyW5g7eotOhpOPNrOL1xXnEZe7KLSG0STbMm0bU9TSFIiID40KNVIlprHMqYLwQMSwGgsMT4fzC3\n+uzfSdO+B+CqkzqilOLHrYdoFh9N91YJ7m2W7zwKwCuLt/K3uRvY/PgZRDqP/0k/t7iMdXtzGd6p\n2bF9MHM5vD/FyG6WSKugo7Wu90OfbgvkWKOwlILk9sZft9M8yytccHSHORRmWSwbjByWCnP+HeWo\ncQ7L7TNW8M2GgwBs+usZREWIZVyXiIAEQClFfFQEeaYPpLDUxbIdR9zvdxwuYM2eHFolxdC8STTb\nsvKJjnTSNtnfR1BU5iIuKoLL/mOU5Q5kbfxt7gbA8Lkk25Iba8sd5o9sxUOn1nx/62bDR380nORX\nfmqMZwtBxfou1GccpsAVBSt03eGEZp2Nv55neZa7yoxqw75DYdXksBTuy8FBEhU4KCgpJyri+H8v\nQu2p39/mOqRJjEdAvlq3n9d/2O5eN3/9QeavN56CVj1yGqf881sgsDjkF5fXuNMoKHWRHOe/vLS8\ngj4Pz+OJC/py/sC0avdjlWHJLSpn6oermZTRmrP7tQm8sTvS6mHKWg+k/OL3iE1pVaP2CsdGTlFZ\nvRcQlzluG/LcJ2ek4RNJ7W4EaVi4c1g2eMRl76/uHJYZQHF0JFt0W6I+nwMdBkHrftCqj1F88jg5\n7dlvuXhwO/4wutNx7ysQ27LyOeWf3zL75pFB8aPWNfX721yHtEmOZZ853W3m0SL38rbJse75QwB2\nHfafx9qq7guQV1JOag1nlisoKeeVxVtpmxzHmX1buYc7sgtLKXVV8PcvNlQpINmFpSTGRBITaZj1\nWfklfLl2P1+u3R9YQOyRVr3Opc+v59Lj3S3MvqVmArLlYD4TnvmW+XeNoUsLcY5WR05RGa2T6m8k\nm6tCU2z6Pooq84GEmspyWEoLIGsjH3zxFUd2rKKH2k2P7fNh3QxzA2X4UlpnGILSup9R5PMYo8E2\nHcjnr5+vD5mAzFyWCcCC9QdEQBozXVKbuH0Udvq3T/YSkMyjHgGZuXQ3Y3ukMvTxBe5lecXlFNTw\naS6/pNw9nAXw1Z0n061lAqXm9KJVleM6lF/C4L/O584J3YiJNEpUbDpQRUFIn0irsrH3U/LrPFZm\n1rxM+OwVewD4bNXeE+YgXr7zKLGRTnq1STwhxwsmOYVl1W9Uh1z1xi8cMP17xXUlIJURFQ9tB7Ik\nycmsckNcPp4yggEpJbBvpecGsS/sAAAgAElEQVRv50+w+gPP51I6egSldT9o1a/SXCZ9AqYQtn6T\nLZPqWeRlLQmpgCilJgLPA07gVa31Ez7rbwBuBlxAPnC91npdKNtUUyp7or7qpI58vsozqc8N7/zq\nfn3Ph6v44vbRXtvnF5d7WSR2oiMcXs5K37yTjfvz6NYywT0erahcQaxS9HNW7iEp1nA83vfR6sAb\nB4i02n/E35KqjnIzyiAYjv+acsG/fgQCDxfWV6yAjJyi+i0g320+5H5dWFo/k2jtNbqKyiogsbXx\nZy/lUnDIW1T2rTR8fBaJbb2tlNb9ILGN+0EtlBw2f6dB8zHVMSETEKWUE5gOnApkAkuVUnN8BOI9\nrfXL5vaTgWeAiX47qwNO692Sn7YdZkSnZvRum8jDs9ey+WA+Qzo25e1rh9K7TRIDH/va73O5Pp1E\nfkmZe8ZDXyp8nnju/3iN3/q84jL3eHRVFoi1jatCExvlXyQPjKfKjz+bw5Qt96BskVbLdhzhcEFp\n5TuvBJdtDvl9OUW0Soyp91FGoabMVcGvO48yzBYBFxvppKDURXY9FxA7u44UUVLuIjoi8Heptny9\n7gAtEqJrPXxTZuvkK7Xs45sbhSa7jPcsK8qG/ath/yqPqGz60uOwj2uOs2UGd0c0YU1FOhwxC04G\n+fvsMH/EjaW+XigtkKHAFq31NgCl1AzgHMAtIFrrXNv28UDobcga0qFZPK9fNcT9fs4to9xPKKO7\nGibw7JtHcs70HwBIjosku7CMS1752Ws/uQEskN1HCsktLqPM5X26u3ysgNtnrADgveuGAVTZOVvC\n5dKamAA/+vyScr6b/Trnrr2f/NjmJFxrRFq5KjQXvvyTe7tj+b1YP+an5m3kqXkbeeOqIYzr0aLm\nO2jAlLsqmLtmP2O7p5IY4wk1ffzz9bz54w7m3XGyO2Q7NiqCglKX38NFfaKiwvu7uH5fLnf+bwUv\nXe4pe2I5gP93/XCaJ0Szbm+un28tt7iMez5YxV/P60NzM09jybbDdG+VQHJcFNf9dxlQewuyzFVB\nbKSTojLXsVlJscn+SZGlBXBgrSkoK2DPCq53fktkhAumPQ/RSTafSn/jdbMuAasY1xTrOhfUUwvv\nWAmlgLQFdtveZwLDfDdSSt0M3AVEAaf4rq8vxEY5icX7i9OvXTLdWjZh04F8pk7swVTbkNGj5/Tm\nodlryS8uJ7fI82UZ/rcF7M89tsl/LHN3T3YRr363jYuHtPPqtMAzPOJyafdTDsD4Hi1YsOEAru+f\nZ+K6x1ihO/Nj72ncbIbp+v4Io2sQV6+1ZtuhAi8LBAh4XkcLSolwKhJiaj4nRWX4dnJ1yb8Xb+Op\neRsZ0y2Vt64Z6l7+2y7Db/bnD1Yy68YRREc43dc02ENY1pj9sVh97y3ZxbBOTemc6j1EezDAdM5f\nrtnv/X6t8X7++gP85zsjKtFXQGYu3c2Xa/fTNiWWB8/qRZmrgkte+ZmB7ZP56KaRNW5nZZS6NMlx\nkRTluCgoOc6n+Kh4aDfU+AOycooY8/d5dFO7+ezCBI+l8st/wGVen8g4w8Fv96uk9qjxnCvWkHXh\n8ba9nlDnWTha6+la687AvcADgbZRSl2vlFqmlFqWlZV1YhtYDQ+e1QuHMir7LvzzWPfyMd1SiY9y\n8voP273Gln072ZpELx2xDS/99fP1ZDzyFQfzvPeTaw6T2edyuGNCVy4a0JLHI14n6btH2ZY6gSml\nD3BUpbg/5zsWW1kf/fr329ltWkiPzFnL+H9+y/ZDBV7bBBrXHfDY14x5apHf8i/X7GPic4uPSRRC\n8dSWW1zGR79mBlxXUaHpOPVzXv1um9+6pWbV5qU7jng5X62hidV7cty+MstSq0xAHp69hrtmrjim\ndpe5Kki/by7PL9hc7bZZeSVsOZiPq0Lzl49Xc+6LP/htc7jAX0Ds8+UA7DOnN4i1hSL73j/LH2Z9\nB60Oc+3eXIJBWXmF28cXbD9NaXkFpUSyRneCQVfBWc/Cdd/AX/bAjT/CuS/DwCtBOY356WffDC+P\ngr+1gVfGGrN0LnvdKJ1fyQyR1nVpLBZIKAVkD2CfhSnNXFYZM4BzA63QWr+itR6stR6cmlq/qsGO\n7prKtr9PIjUhmvTmnjj05NgoXr9qCFrjlUNi5+RuqQzukBJwnR3fjhpgp0/4sHsIywzFHJrelDtG\ntWTYzzdyecQC9vW9kW8znqSEKG/HvU+nX1pe4TXODHAgt5hHP1vHDe8sJ7e4jLd+2gnABp9pfysL\n/TwSwL8y9aPVbNifx4gnFlBehfMyp6iM5TuP8t3mLNbsOfZOyFWh+XrdgUojbB76ZA13zVzJmj3+\n0WeF5vn89fP1fuvc1QlKXWyz3R/72LZ1yOoE5K2fdvLRr1X9NPzJNiO6nptfvYA8O38T17611N3h\n5gWYpsBuJVs08RWQHDP60HYti31mLrQExDpnK5pL4y02vt+xmlLmqiDRLSDBfYq3t8kehaYdEbyz\nvQn5PS+CM56Aa76AqbvhluVwwWsw7AaIToS1n8Bnd8J/TjFE5V8j4eMb4eeXjeiwkvxGZ4GEcghr\nKdBVKZWOIRxTgMvsGyilumqtrV/AJKD6X0MDIT7aybBOzbj79O7c8b/AT5dORY3yAl5atNVv2bq9\nuXRJbUJKvJGJa3VOOUVl5JeU0zG2EF6fSHLWJu4pu46Jve+k2Ozwdxwu4JftR0iOiww481x+cbl7\nv2A8wYLxFPnSwq1+yy18Qz/tnbZvGY/2TePILszhQG4J2UVlRDodRDiU31PvDW8v56dth/3aWO6q\nIMLsrH7ZfoRebRL9OjyAl7/dylPzNvLq7wczoVdLv/VWSPbRQn+Rq2w+mD3ZRV7nOv6f37rH9O3L\nrfqT1jWevWIvD57Vi+ZNovnrZ+vo1iqBi6uY6XLd3lyem7+JaZcOcIdmW+QU1TzoIbuwlH3ZxVV2\nuIHEzfd6WtZzri0opKjUO7veKi3ia4GAt9gczCsJWLmhKlZn5rBs51HGdk8lKsIR9Kd4e1vzisvd\n1/ybDQd54JM1bM3K5+GzexsbOBzQvIvx1/dCY5nWRoSjPfpry3xY+Z65V8W7tGZlZEcKD/WGbfmG\nXyW2+ofI+krIBERrXa6UugWYhxHG+7rWeq1S6lFgmdZ6DnCLUmoCUAYcBa4MVXtONFbn1qGZd2r5\nBzeM4IVvtrB4UxZOhyKjXVKgj1fLw3PW8sI3m1n2wKmA0UmAMYS1dm8uaT2bQ3IP9g1/kJkzNSOL\ny91PPd9tPuQeVvvghhF++95xuMBLQKyESjA65LgoJw6lyPfpYK3O87yXfuB3wztwWm9PQmJOURnJ\ncVFUVGienb+JclsAQVGpi3HPLyIxNpL3rxtOSbnLXWJ/dQDLAKCgxEVSnIOZS3dzz4eruHdiD24c\n29lvu3Xm0EllnY1VPHC/eY5aaz7+bQ+tEmPcNZfsfLBsN3ebc8Z0So1nW5ZhfViCZrfCrLBre3jo\n1A9X8eqVQ3j1e8MqnVxZhQDgkU/X8sv2I/y68ygndfGedyPbllPiqtDu8whEcVkFpa4KP8G3E8jB\n3yTGu3uwBMguNoWlLuwV1yKdtmq+eBdltAvY/pziYxKQ4jIXZ7/4vXkMB/FRzuN6ii8uc6E17ojF\n4jKX18OU/UHAGrotc1WwbMcRFm/K4q7TAkyqphSkdDD+ek32LM/b7xaUrQu/YpBjE2k5P8J//2Os\nT+5g86mYzvomDSMYJaR5IFrrucBcn2UP2V7fHsrj1wfaN/UIyIbHJhIT6eSv5/Th5KcWcuGgNPqn\nVR3O+Nwl/Su1YA7ll/LHt5cxpGNTZi7LJCHaU37F4YyAi94gIrcYWMDqzBwOBOhA7E/ZbZNjyS0u\n470luxjQPoUyVwWRToef3ya9eTzlLs3GA/5DWGWuCn7blc1vu7JZfHdT97rthwoY0D6KjQfyeOGb\nLV6fO1xQSl5JOXkl5Zz81ELAE6UTFeGAAP1efmk589cf4J4Pjc68qBKByDUj4AJZJ+Cp/WSJ5KKN\nWdw1c2XAbQFe+94zHNm3bRLFpS725hRzpKCUFokxXp1kSbkLrbVXtF1+SbmXZbbbFnlXUeEdANG8\niSHigRzcdgHJLyl3+wXsFJW6eOzzde5z211Frk9ugFylCB9RsjrYbJu15mt1Wg8GZT4WiMLbR7Y/\n59gCSc576Uf360inIi4qgsJSF+dMNx5WLhxUfYkfO0Men09xmYvNj5/Jwdxihv5tAef094h5YamL\ndXtziYpQHMo3zrdpfLQ7YvH2Cd2qFG0vrOrF3U7nxq/7UlpeQQq5PDS4nPNa2XJW1s+xfaa1t6O+\ndT8jf6WehcnXuRO9sdHbJ0O6qe1J3jKJ2zeLY9vfzmRin9akxEfx5tWecOFbT+nC1DM8hQxP7111\nWZF5aw+4x+jH2kJorSEZK/rp1e+38+nKvX6ftw8PtU6KoVPzeA7klbA6M4eu93/BT1sPszoz2+sz\n6c3j/Z5OlYKi0gqvnJcr3/jF/frXXcY+jgbwhyzdfsRv2acr9/Lekl1EVZKkOPXDVTz5pSdrv7Lq\nsdbT8tSPVns9OX+4PJNtWfn8uNU4f2t8Pys/8FP6Y5+to6JCe3XccVERPDzZGNI4mFfCAR+hLSx1\n+SWntUmO9WqrXXDybSJYUu4ixSyEue1QAfPW7udQfgkHzWPYh9wqCw+euWw37y3Zxfp9hhW2+2jl\nAmK/NsPSmzK+Rws/n5bVbl8LxI7lR7D+W5/RPtvWJBIxv6Scxz5bR1Gpy30OYIhUXJSTIwUlrNyd\nzZ8/qFzwKyOvuNwt7LNXGL+L+esOuNcXlbk4c9p3THhmsTtJN8d2zQtLyzmUX8JT8zb4RSNWhtZG\ngEvP1okcJZHMlOEw6k646E30rb/St/hVLil5EE7/G6SfDEd3wuKnYMZl8GxveKozrJxR/YFOIFLK\nJMh8fNNIyis8HYRSiteuHOw3lGV/0hzbvQUpcZEcLSxjfM+W9G+XzBNfGJ1jTKSD9/4wjMteXVLt\nsWMiHPz3mqH8/vVfOGw+NcVEGr6F8kq+5P/+1hNhFBcdQUWFkbz48W+GU/f9X3bx6Spv4clIS/Ib\nM2+TFEtxmcsr58Xu/F+6/QjXjkr3Gg6LjzIS7GYs3eXXrlvf/63Kc7WG4B6Y1JPpC7dQWOri3lmr\nGNm1udewkNXhZ+WVMG3BZh48qxd5xWX8yafT2ZtdzJaD+ZVmCL/2/XZKyl1eHV9spNOd63DWC9/z\n7995TxVbWOpyBxBcPqw97y7ZRWGJy90hAV7DgLlFZSTGRKK1pvsDX7qXT/OJtNrxxCSv659TVEYg\nT4rvPbLXdKtq2zJXBTFRTq9r4arQ7vf2bX1FxhLMUh8neml5Bb/s8Dwo2MW2sLScXg/N47FzevO7\nER3dy19etJXXvt/uN9RV6qogLjrCL28KjKHED3/N5LwBaTUu9W6VLGoSE+EOKtl52PPdte7Xfq82\nu3jgkzV8ve4Ao7qkMqJz9VMnWNfkrIzW7DpcwD+/3sR5A9uSlhJHTlEZecSxRPeEEbYcmdJCM1dl\nhWGlJLev0TmdKMQCCTJREQ6/iqvje7akS4uESj7h+RwYnaodpRQndWnuZV778tSFGVw5ogN3ntqN\nNuaPzfrSK6X8HLAQON+jSbSThJgI8orL2XzQGJ76fPU+fAOYJmW0Id7nHFPiIykqcwXMuu/bNokv\n1+4n45F5Xh13i0SjHtDWrAK/6YRryum9WxEXFUFBSTn/W7ab297/zatTtoc7v/b9ds564TtW+9T7\nGtg+mW83ZTHhmW95b4m/mFm887P3uphIB6m2SY2+We/tMykqdTHu6UUAdG3RhMEdUvhy7X5GPbnQ\nvc3nqz1lcWav2MuaPTkBI6J8sVtCgYafwN/vE2gIq7jMxXPzN3nNcVNqJuttzSrgsv/8TEm5ixF/\nX+C+rjlF3k70D5btZs2eHErLK5i13AiJLis3vjR2a+vBTzyVFuwPEtaxX/EJl67svErLK4iPcroF\nxPK7gHGP7/1wNR//Fjg025eKCs2WLGMmxQO2a/CzzTK3vtP2Ybf8knJ3WRL7UNaC9QeYvtAzRLth\nfy53zPiNMleF+1pERzjcQnWP6U+zi3uJPbItKg7aDYGh18E5L0KHk2p0XicKEZB6ghX+WFldKd/J\noW4Z1wWAjs3iuGhwO/7vnD60SY51P61F2PYT4fQfN22b4u/AdDocpoCUscf8QrsqNKkJnk7y5SsG\n0jY5lrhob1GKjTSeWAP96E8zo59yfcTFvt9erY+9OOKw9Ka0axpHXJTTy0/w09bD7M0uYveRQorL\nvIeQ1uzJ9bLmmsVHkWHzQ/n6dQC+vvPkgMePjXTSylYU71szMMKioLTcfXylVEA/hV2wnpq3kbNe\n+J5b3v/Vbztf7HkbP2w55C7hX1Gh3a99ncz2TsoSgw+WZ/Lc/M1G8p/53RnTLZU480Hmx62H2XIw\n3+v65voMYd09axVnvfA93R74glWmOOcWl3H+Sz+wfIf/8GSkU3n5USyLPdJn2mTL5xIZ4b88Lsrp\nHoKyV16wLNN7P1zNj1sPUR2HCkq8rA2LQNfKLnoFJZ57a7X/3lmruPatZe6pr3MKy5j43Hd8smIv\nF/zrR/d1sz+8WSMD9uN1f+BLVuw2hnwLS8vpOPVzXvxmM8t2HKnxcNmJQgSknvD7ER0AvKKf7Fw6\ntD3b/nYmzZtEcfv4ru7OyHfCqNgoJ4+d05v3/uBJ+ve1FgCax/tPB5pfXEZCTCR5xeXszfF8oe2d\n+8D2RshhrI9VExPpJL+k3D1xFkCrxBjuP7Mn7ZoGmOQEbwHp2Lxmczkk2Jzh7183HDCG3vbZ2nvd\nf5dx0hPfsHCjfxSVL0lxkbSupjJqy6QYurX0T/iMjXISFeFg29/OJCE6gv25xSTafEP2IaDMo4UB\nBSQQ9sTTQBSXuTiYW0KCeazpC7dy3ks/oLXmb3PXc/pzi9l1uNAvSs7uA1lldlCRNsHr1y6JX+4f\nz12ndve6v74WqN2vEygBEYwcoV93ZTPNJ2ACDL+g9VRfVOritGcXA8aT/K3v/8Zc0ypzhwIHGCaz\nW/nRkU601uw4VOAVafbklxu9OtxpCzb7fSdWZ+b4lRQCb2vDem0X0ckv/sA60y9TWOIip7CM/y3z\nFN4oKXfx3592uN+vyszh201GErS9vtgv24/w9s872XXEW8TOnf4Dr3+/3f0w8PRXm7jw5Z/86ufV\nNSIg9YTrRndiy+NnuDuZJy/oy6Pn9PbaxuFQLHvgVO48tZvbqugUoOP93YiO7jBYwP00+Y8LM7h6\nZEcAEmM9P8ApQ9rx4mUDeOjs3iTEGNEtxWUVdEo19m1/Ykr06QSbxUdx49jOxEY6/UJu37tuGNed\n3InJ/drw4mUD3MutQIM2to67VaLnte8wnh27wFp+pDhzuMWXeWv3+y3zJTk2kta2MfbrRqcz3qee\nV3xUBHNuGcWYbt5JrNFmJ+uwhWPbBd3uNG6ZGON37exM6Omfo1IZhwtKOZhX4lXFoLDURX5JOa+Z\nSasH8oq9fC2AlzW2xAxcsFtMrZNiaZEQg9PhPexZWTFQCDwfTnV0aBrvjv5bszfHq/rBpyv3ctO7\nhgVmFZ/0TeQsLa8g3mYBR0c4eOfnnYx9epGXBblydzb/WmQIWEWF5pmvN3H1G0v5ZoPHWW5ZTD1a\neQ8x262N6gqNFpSWM+Cxr7yWHc4v5YhPbpGVSOnrm3nwkzVsPej//X30s3X8b6lHlJJiI09o5eua\nUL9aE8YopbyGnS4Z0p7f2xyKvljhkgNrkMludbqdU+NpYyYupiZ4Ouw/ndadszLakN483qtm1Zl9\nWgMwrFMz/n5+X9JSYt0di/Wbv31CV+6d2MPdmdppZvoHHA7FWRltOG9AWzo2i3NHYtnn9Ghu8yVU\nVTcrIcbfmrIP0V0wMI20lFiinA5+2OKdgDiwvX/IdFJspFeo9f2TevHqlYN5xeYQtzpUK6zVsoLs\nGfT9zGGwmEgnyx6YQLumseQVl5EQE8HQjk256qSOXpUKfLl4cM3DUD/5bQ8H84rp1NzbKjpSUOq2\nFg7llfgNGVoMbJ/srnNld0TbLTH7dQ40HYEVHbfzGKcB+N/1w2nfLM4tIPZ8IPvw509bDwfMwwFD\nQJJiPUJd5qrgt93ZAbddtvMoecVlzF/vEY1r3lzmfr14s2EVDEtv6vU5X+utKgpLXX4lgA7ll/hF\nHFp+q+gIB5/fNspr3W+7/eceAtxDWeAd0VlfEAFpoPxuRAceO7cPlw6tPirjnxf149Kh7chIS3aH\nkdutCvvQiv3p/7Jh7fnmT2O4+qSOXDq0Pd/f66l16VvIr0+bRK8nq3l3nOw3ZPPsJf1ZdPc4dz5K\nnzYex3nzBM+Pw9e/YichJoKB7ZPpbysHbsXpA1w7Kp3v7z2F2yd09fvs//7onzQZG+Wkg88Qm1L+\nGfHgeVpPTTTEzh6NZFXezS4spXmTaEZ1ac53mw+RV1zO8M7NiHA6mJRhCHLn1Hg/Kyc1IbrGQ1xP\nzdvIgdwSWiVF848LM/jLmUbY9xRbJehD+SUBO/6OzeI4K6MNGw/k8ezXm7xyclrYrED7sGUgC8QK\nevjaFvqaHFd9+1snxdLEzFd6YcFmHpztcazbr+eiAMOPJ5mRTqWuCv54ciceP68Pk/u14WBeScBy\nP2A8jNz9wSquf3u5e5n9Ov9mhpeP7V77xL0N+/xL7GTllXC4oNTLSrRCxKMiHPRu4x00sulAfsB9\nbz7oWZ5Sg+t7ohEBaaDERDr53fAONUpmatc0jr+fn0Gk0+HOvLULSFSAIarHzjWc8p1Sm3iFHFtY\nT7rWmj+O6cymv57B57eN4u7Tu7s71ECkmQ58u98jUFbyuWbk2Rl9WjGpr9H5JsRE8tFNI/nkZk9l\n14O28Mok80fWOdX/ad9u/j8wqSdgOC8DdXyB5lSxLB1LcOyWktVRWGPwd07o5k5StDqs5k2iWfCn\nMcy5ZRSv2aYKAOO6L/rzWK+CnD/fN547fITQ7otpnRTLxYPbMci0Qu3DLh8szwzY8fdrl0x/0xKz\nF2J87pL+7mAHazsLKxTZ7hf545jOfn4wXyH25cJBabRrGkt8tJO84nL++fUmttg6SPsQ278Xb6NN\nUgw9bUJmVaA+rVcrUuKjuHxYB/c9sYTAl5gIh9tXAYZQByrb4pu/BYEF0T7UamHVhrOzL6eYw/ml\nXtbtf83tfK9bVdh9OE0D+C3rGhGQMOPCQWncOLYzt47vyme3juKfF/XzWn9671bMuWUkvxveocr9\nuAXER1t6t0niZjNCrDLev244b1871KtDT4iJZP5dJzP/rjFuU39011SmXzaQRyb3dg93+ZaxB+8x\nasuJHWg7OxP7tOLGsZ15+Oxebiuqu81vFOhHbiX2XTKkHc9P6c+VIzzXyCqPbnVoLRJjOLW30SFH\n2YbYOqc2CWjdJMVGkhIfRXrzePeTdouEaL+om9k3e4Y+RpnlTXwDKcAY27c7lPuZFsPJXVP9xvsB\nzh3Q1svvER8dwUNn9QI8ORv3TvSU74iPcvpNcFZdIMS9E3uglKJJdM2epO+Y0M1rKO0vZ/Zk+QMT\n3BYXeII67Fw3Ot2rhL49Uq5Lqn8wxDd/GkNqQjTXjExnxvXD3cvtodJD05ty3oC2LLp7bJVtvnJE\nB1olxrB4UxZHCkpp5jPs1Cw+yj3sPPvmkTxoXmOA56f058GzevHw2b0IRNN4sUCEOiY6wsm9E3vQ\nJDqCPm2TuMCnBITTobzCWitjfE/D5O9Xg219aZMc656U67lL+vPZrUan2KVFAl1aNHE/8Y7tnsqk\njNa0TIxxPzV2bOb/lPvOtcMY3CGF28Z3dZcsqcphba2/d2IPBnUwxr43PDaRT2/1dM6WgNits6ln\n9ODu07tzaq9WnNO/rZfPKibSyaPn9GbWDZ44fWtcvbLQS3tyqb2jfP2qISz681gcDuWXyR4b5eS+\nM3owLL2pu8NOsQmIUoFDoq8Zlc7Wv53JBYPSiIuKYESnZl4RbYG4ZlQ6UREO95CjPfIpPjrCPe5v\nWcHVhWJb59ikiiFKi+mXDfSa9+aiQWm0bxZHsybRXtf98mHteeHSAV6f7ZuWzMqHT6Nv2ySOFpa6\nreRplw7wcr6DWX0htQlKKR46uxfDzQKoYAiexe+Gd+DZS/oTE+nkuUv6u61oX1xaM65HKl+tO8D+\n3GKaJ0Tz5R2eSaxuGtfFLdT92iVz7ah0d5BLRprxvrKhwEBVresayUQXasVpvVu5a3sdD+cOaOu3\n7P3rhrPrSIHbCQ+erPYOAZ5yR3Vtzqiu3sUGq/MnNPEJbfY9j+hI/8TOhJjIKq0r36CHy4d1IMrp\n4LyB/ucI8O3d4+g49XPjeLbQzphIp1scbh7XhbbJsTw0e617/R/HdOaPYzyFI61zjYty8sXto2mT\nHEvX+7/wOtaYbqlew53vXz8crTXp93mVqvMjMSbCXQLH7puKj3a6Q0pbJESzL6c4YEDHgj+NYeGG\ng6SlxNX4uzK+Rwv3A4oVLRhXSWSeUoquPiHWUU4HMZFO0lJi2XwwHwVM7N2Kyf3aeEXmvXjZAPfQ\nqJ2bx3Vx3+dvNx3k521HvAIMzh3QlneX7AyY2d+zdSKH8jwd/ZUjOnpZQD0DWH8zrh/O2z/tdA93\n+X53x3ZPpUL7f7/qAyIgQq05XvGojBGdm/mVhrhzQjcO5ZcwrnvN5oOpzgIJ5NexY1ky1Q3lVYXT\noZhSgyCHqkiMieT3IzoysktznJUU0nM6FEvvn0BynCfM06GMycEsR3OgyLaazGTYIiHGHaRg78Tj\noyPcw5gPndWLCKeDIR2b+n2+c2oTv9kPs8z92asZW1x/cif+cmZP93vLAokNkMtk0cbHf9bSDHJI\niY8iu9CITBuSblhp9qTD7MKyaq/BK78fzKG8Ejr5nEMgq3LubaPp2TqBt37cARjXyxKPUV2as/NI\nAUPS/a9RRloyT13ksQzRPVIAAAy+SURBVOSb+fg62qXE8di5fapsZ10hAiI0CHq1SeTjY5gS1T48\ns+jPY90O7ecu6e/Ogajy8zGRrHv09GNyeIYS307YF3tSprX95oP5NIuPqjIsulViTJXhoT1bJ7qd\n0ApPZxsXFYE2g7lbJsUE9EVUxuXD2vPT1kNMGdKeP32w0u2fWZmZ4+V0No7jH/Thi93f9dN9p7jn\n2EmJi+RQfilK4fZFREV4zmFogM480L4D+dMsAXnvD8P4Ys1+nA7l9tNZgRx2aXrz6iE4lKr2wQWg\nR2tvK6VFQv1znluIgAiNEvsP1e7cPXdA24DDZoHwrWnWkDinfxue/moTEY6q3Zw/3XdKlU/h90/q\nydq9OWzYn0frZO9kT+shPFAH+8ENIyot2d4yMYYPbjiJLWa9tQsGpfHmDzsA73wg8ETOVZd/fU7/\nNvRLS/aaoM3yDWmNO2zWspruP7Mn3VpWXZ+uKqwSJPHREX7WQbKZo+KwXdeIY0gAtA9nQuCyQ/WF\nhvsLEYQaMCBA8mB94ux+barMvK8tN43tQvdWiZzSo+r8huqGcJrGR/HlHSebSZEeoYhwOtw+EHv5\nlkl9W+N0qIDDWb50aZHAkr+Mp0VCNNMWGPkoqQne1pBVKLGqaY8Bnp8ywG+ZPbjACmiw2hwoTPtY\nsCyQQHXm3MOnxzF1x7IHJlDu0uzJLqxRUEtdIQIiNFrWPzox4A+8PuEbQRQsHA7FqQGm8K0tgYbB\nrKd5+7rplw88pv22NPMqrKRHXwvE6fCeY/1YSLGFvVrVGCyrqbJ5ZmrK5cM78OAna0hL9o8KTA4w\nhHWsWNehVTV12uoaERCh0XK8T5lC1Xxwwwg+/m0PMZHHnw1glTr3H8IyLZBaVKENlB9TWf7SsfK7\n4R0qDbCwwpVrEqTQ0BEBEQShxrx+1WD2ZBu+jSEdm9ZoqKomXHVSR978cYdfkqUVnl0ba8oawrJP\nZ2yV4HGEsHNvGhdFj1YJ3HVqt5Ado74gAiIIQo05pUfwhsXsPDK5d8AM7B6tEtnxxKQAn6ieZuac\n8lOGeOZrtHwg1cQWHBcRTgdf3hF4DpnGhgiIIAj1gmAP+STGRLLsgQleznQrwqm+lUVvqIiACILQ\naPH1qdx3Zg+S4yM5vXerOmpR40IERBCEsCE5Lor7zuhZ/YZCjRA7ThAEQagVIiCCIAhCrRABEQRB\nEGqFCIggCIJQK0RABEEQhFoRUgFRSk1USm1USm1RSk0NsP4updQ6pdQqpdQCpVTtJ18QBEEQTigh\nExCllBOYDpwB9AIuVUr5ppr+BgzWWmcAs4B/hKo9giAIQnAJpQUyFNiitd6mtS4FZgDn2DfQWi/U\nWheab38G0hAEQRAaBKFMJGwL7La9zwSGVbH9tcAXgVYopa4Hrjff5iulNtayTc2BQ7X8bH2ksZ0P\nyDk1BBrb+UB4nFPQXQT1IhNdKXUFMBgYE2i91voV4JUgHGeZ1nrw8e6nvtDYzgfknBoCje18QM6p\ntoRSQPYA7Wzv08xlXiilJgD3A2O01iUhbI8gCIIQRELpA1kKdFVKpSulooApwBz7BkqpAcC/gcla\n64MhbIsgCIIQZEImIFrrcuAWYB6wHpiptV6rlHpUKTXZ3OwpoAnwgVJqhVJqTiW7CxbHPQxWz2hs\n5wNyTg2BxnY+IOdUK5Q1Q5cgCIIgHAuSiS4IgiDUirAQkOoy4usapVQ7pdRCMyt/rVLqdnN5U6XU\n10qpzeb/FHO5UkpNM89nlVJqoG1fV5rbb1ZKXWlbPkgptdr8zDQV7OnfAp+XUyn1m1LqM/N9ulJq\nidmG/5m+MZRS0eb7Leb6jrZ93Gcu36iUOt22/ITfU6VUslJqllJqg1JqvVJqREO+R0qpO83v2xql\n1PtKqZiGdo+UUq8rpQ4qpdbYloX8nlR2jBCe01Pm926VUupjpVSybd0xXf/a3ONK0Vo36j/ACWwF\nOgFRwEqgV123y6eNrYGB5usEYBNG9v4/gKnm8qnAk+brMzFyZhQwHFhiLm8KbDP/p5ivU8x1v5jb\nKvOzZ5yA87oLeA/4zHw/E5hivn4ZuNF8fRPwsvl6CvA/83Uv835FA+nmfXTW1T0F3gL+YL6OApIb\n6j3CyNPaDsTa7s1VDe0eAScDA4E1tmUhvyeVHSOE53QaEGG+ftJ2Tsd8/Y/1HlfZ1lD/6Or6DxgB\nzLO9vw+4r67bVU2bZwOnAhuB1uay1sBG8/W/gUtt2280118K/Nu2/N/mstbABttyr+1CdA5pwALg\nFOAz8wd4yPYjcN8XjECLEebrCHM75XuvrO3q4p4CSRgdrvJZ3iDvEZ5E36bmNf8MOL0h3iOgI96d\nbcjvSWXHCNU5+aw7D3g30HWt7vrX5ndYVTvDYQgrUEZ82zpqS7WYZuMAYAnQUmu9z1y1H2hpvq7s\nnKpanhlgeSh5DrgHqDDfNwOytRGd59sGd7vN9Tnm9sd6nqEkHcgC3lDGsNyrSql4Gug90lrvAZ4G\ndgH7MK75chr2PbI4EfeksmOcCK7BU7XjWM+pNr/DSgkHAWkwKKWaAB8Cd2itc+3rtPFY0CBC5pRS\nZwEHtdbL67otQSQCY1jhX1rrAUABxtCFmwZ2j1IwatOlA22AeGBinTYqBJyIe3Ii77tS6n6gHHj3\nRByvOsJBQGqUEV/XKKUiMcTjXa31R+biA0qp1ub61oCVbFnZOVW1PC3A8lAxEpislNqBUUTzFOB5\nIFkpZVU/sLfB3W5zfRJwmGM/z1CSCWRqrZeY72dhCEpDvUcTgO1a6yytdRnwEcZ9a8j3yOJE3JPK\njhEylFJXAWcBl5uiBcd+Toc59ntcOaEYk6xPfxhPjtswnrQsZ1Lvum6XTxsV8F/gOZ/lT+HtqPuH\n+XoS3s7AX8zlTTHG6VPMv+1AU3OdrzPwzBN0bmPxONE/wNt5d5P5+ma8nXczzde98XYQbsNwDtbJ\nPQW+A7qbrx8x70+DvEcYhU3XAnHm8d4Cbm2I9wh/H0jI70llxwjhOU0E1gGpPtsd8/U/1ntcZTtD\n/aOrD38Y0RebMKIS7q/r9gRo3ygME3gVsML8OxNj/HEBsBmYb/tSK4y5VrYCqzHmVLH2dQ2wxfy7\n2rZ8MLDG/MyLVOMcC+K5jcUjIJ3MH+QW80scbS6PMd9vMdd3sn3+frPNG7FFJdXFPQX6A8vM+/SJ\n2dk02HsE/B+wwTzm22Yn1KDuEfA+hg+nDMNKvPZE3JPKjhHCc9qC4Z+w+oeXa3v9a3OPK/uTTHRB\nEAShVoSDD0QQBEEIASIggiAIQq0QAREEQRBqhQiIIAiCUCtEQARBEIRaIQIiNHiUUi5lTEi2Uin1\nq1LqpGq2T1ZK3VSD/S5SStXZPNlKqR1KqeZ1dXxBqA4REKExUKS17q+17odRMO7v1WyfjFF5tNFi\nyzQWhJAhAiI0NhKBo2DUFlNKLTCtktVKqXPMbZ4AOptWy1Pmtvea26xUSj1h299FSqlflFKblFKj\nfQ+m1P+3d/8uVUZxHMffn3CpIShwiKCGGkoMDKMfFJHQUrSUQ7QUbU1tObRUBA0tTo0RJdIUhUFJ\nLVKKYQaS/QEG0RAYVKTeTL8N59x8uFwRn2sNl88L5Dk8Pvec4+Dzvedc7ufRsbxSqT4npL/wzIi/\nKwhJ+yQN5fZ1SfclvZb0UdIZSbfz+IM51qaqJ58fk7Qzv75V0iNJb/PP4UK/fZJGSF8MNPun/C7F\nmsF6SROkb9JuIWVvAcwBpyPie76Rv5E0QIqeaI+IDgBJJ0jBggciYkbS5kLfLRGxX9JJ4BopQ6rW\nXlKkxGdghJQpNbzCnHcAXaTnOYwC3RHRI+kxKXLjSb7uW0TskXSelHB8ipQr1hsRw5K2kWK4d+fr\n24AjETG7wvhmDXMBsWYwWygGh4AHktpJ0RW3JB0lxcpvpX7s9nHgXkTMAETE18LvqsGW70j5RPWM\nRcSnPP5Evm6lAvI8IuYlTZKyiwbz+cmacR4Wjr2F+bZp6YGFG3OSM8CAi4f9Ly4g1lQiYjSvNlpJ\nWUCtQGe+WU+RVimrUcnHBZb/f6kU2sXrfrO0TVw7biXPd1HSfCxlCi3WjBN12uuAgxExV+wwF5Sf\ny/4lZmvMn4FYU5G0i/SOfpoUR/0lF48uYHu+7Afp0cFVL4GLkjbkPopbWI2YAjpzu7tkH2cLx9Hc\nfkFKzgVAUkfJvs0a4hWINYPqZyCQtq0uRMSCpH7gad4mGiclzxIR05JGJH0gbSVdyTfhcUm/gGfA\n1TWY1w3grqSbwFDJPjZJek9asZzL5y4Dd/L5FuAVcKnBuZqtmtN4zcysFG9hmZlZKS4gZmZWiguI\nmZmV4gJiZmaluICYmVkpLiBmZlaKC4iZmZXiAmJmZqX8AW8nC/L6blQpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ13LfcF3Ji2",
        "colab_type": "code",
        "outputId": "7d0a60c0-3140-429e-f20c-1b99df8a77c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "def get_accuracy(net, loader):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        # Get inputs in right form\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "        n_total += labels.shape[0]\n",
        "    return n_correct/n_total\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader2))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.9228456724460979\n",
            "Test accuracy is 0.9047619047619048\n",
            "Test accuracy is 0.7142857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMZxrWj47rA",
        "colab_type": "code",
        "outputId": "bb94b939-b7fa-41d2-89da-7633447ed213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        }
      },
      "source": [
        "def examine_label(idx):\n",
        "    image, label = test_set[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    confidence = class_scores.cpu().detach().numpy()\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "    print(prediction)\n",
        "    print(label)\n",
        "    print(max(confidence[0])/sum(confidence[0]))\n",
        "    print(confidence)\n",
        "\n",
        "def examine_labelours(idx):\n",
        "    image, label = test_set2[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    confidence = class_scores.cpu().detach().numpy()\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "    print(prediction)\n",
        "    print(label)\n",
        "    print(max(confidence[0])/sum(confidence[0]))\n",
        "    print(confidence)\n",
        "\n",
        "examine_label(7)\n",
        "examine_labelours(7)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "2.4063626635186095e-07\n",
            "[[-2.2888184e-05 -1.2093451e+01 -2.1438517e+01 -3.0718925e+01\n",
            "  -1.9910927e+01 -1.0953428e+01]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n",
            "0.00805506057359772\n",
            "[[-6.905057   -0.18458366 -4.942439   -3.9888139  -2.0073504  -4.886998  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSeuwv8R5NGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "44453f05-ba7a-41ca-954f-649993d646b2"
      },
      "source": [
        "gdown.download('https://l.facebook.com/l.php?u=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1K7Erwhj8naBng_RoUWnI2hJmfwzgzMKQ%3Fusp%3Dsharing%26fbclid%3DIwAR1PKsukqEVTM0NUn0KB_YmqAhgO1Sr0DnAb_xGUKSHRC4OemawFx-5gU_Q&h=AT3lp_oIwm9ztQaGRUMXdkq6oUG-Y4Gcyz1KvOqwX7WXRGPWY0qcpMlA3gHDjBsPe80nkLJo3VcTJiB-fPM4VoftAwq9P7yT5PvXs-Azjya2RsqiNJpnwKl30cL0jixwxVGLfrFryiI', 'testimages', False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://l.facebook.com/l.php?u=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1K7Erwhj8naBng_RoUWnI2hJmfwzgzMKQ%3Fusp%3Dsharing%26fbclid%3DIwAR1PKsukqEVTM0NUn0KB_YmqAhgO1Sr0DnAb_xGUKSHRC4OemawFx-5gU_Q&h=AT3lp_oIwm9ztQaGRUMXdkq6oUG-Y4Gcyz1KvOqwX7WXRGPWY0qcpMlA3gHDjBsPe80nkLJo3VcTJiB-fPM4VoftAwq9P7yT5PvXs-Azjya2RsqiNJpnwKl30cL0jixwxVGLfrFryiI\n",
            "To: /content/testimages\n",
            "82.2kB [00:00, 11.1MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'testimages'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OcGb6BBsLao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "image_list = []\n",
        "for filename in glob.glob('*.jpg'):\n",
        "    im=Image.open(filename)\n",
        "    image_list.append(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj7mzywDsvGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b14b68bb-30f5-4f91-ea59-ec67a23af770"
      },
      "source": [
        "print(image_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBAC79668>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3508DA0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3567320>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FD0085DFC88>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB352E828>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3510780>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB351D8D0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3469D68>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3469C18>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A128>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A208>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A2B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A400>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A710>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A6A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A908>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A080>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB340A7B8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B4B3C8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B4BA90>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8BA6898>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B5B7B8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B5BDD8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B438D0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B43E48>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B606A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B60DA0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBBC760B8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34C6DD8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34C6C88>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3456B38>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34563C8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3461FD0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34610B8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34696D8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3542D30>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34FB240>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34F23C8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34F22B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34F24E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34E6AC8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB34E65C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB348D518>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB348D4A8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3497C50>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3497CF8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB353F710>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB353F748>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3558A58>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3558390>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B5D198>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B5D860>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B5DF60>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B66668>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B66A58>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3567550>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3567E80>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB357DE48>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3577EF0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3577320>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3577A90>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB352E390>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3510E10>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3510278>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3510A20>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB71BAC50>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB71BA198>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8BE4BA8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8B684A8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3508C50>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB3508A20>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB351D5C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB351D438>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FD0085DF8D0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8C13EF0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFB8C13860>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FD024CD22E8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBAC64EB8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBAC64710>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBAC73DD8>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBAC73518>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBACC09B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBACC0550>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=32x32 at 0x7FCFBAC796D8>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd95vXAlufFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6b886ce-5b00-4caf-b3b6-d0850582e80b"
      },
      "source": [
        "imageArray = []\n",
        "for image in image_list:\n",
        "    print(np.invert(np.array(image)))\n",
        "    # imageArray = np.concatenate(imageArray, np.array(image))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 6]\n",
            " [0 0 0 ... 3 1 2]\n",
            " [0 0 0 ... 0 2 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 6]\n",
            " [0 3 1 ... 3 0 1]\n",
            " [0 2 0 ... 2 0 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [3 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 3 0]\n",
            " [0 0 0 ... 0 3 0]\n",
            " [0 0 0 ... 0 2 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [3 3 0 ... 0 0 0]\n",
            " [0 0 3 ... 0 0 0]\n",
            " [3 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " ...\n",
            " [ 1  0 15 ...  0  0  0]\n",
            " [ 0 18  0 ...  7  0  3]\n",
            " [ 3  0  5 ...  0  0  0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 3 0 ... 0 4 3]\n",
            " [3 0 3 ... 1 0 0]\n",
            " [0 3 1 ... 7 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 1 2 0]\n",
            " [0 0 0 ... 0 0 2]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [2 0 0 ... 0 0 2]\n",
            " [0 4 5 ... 7 0 5]\n",
            " [2 0 3 ... 0 3 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 5 0 ... 2 0 4]\n",
            " [0 0 7 ... 0 2 2]\n",
            " [1 3 2 ... 0 0 0]]\n",
            "[[2 0 0 ... 0 0 5]\n",
            " [0 0 0 ... 1 0 2]\n",
            " [0 0 4 ... 4 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[3 0 2 ... 0 1 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [1 8 2 ... 0 2 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 4]\n",
            " [0 0 0 ... 0 6 0]\n",
            " [0 0 0 ... 4 0 0]]\n",
            "[[4 2 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 6 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 0 0 ... 2 1 0]\n",
            " [0 4 5 ... 0 0 0]\n",
            " [0 0 0 ... 1 1 1]\n",
            " ...\n",
            " [1 0 0 ... 1 0 5]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 3 3 ... 0 0 0]\n",
            " [0 0 4 ... 0 0 0]\n",
            " ...\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 3 2 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]]\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [3 0 7 ... 0 0 0]\n",
            " [0 0 4 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 5]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 2 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 2 0 1]\n",
            " [0 0 0 ... 3 0 1]]\n",
            "[[ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 1  0 72 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 3 0 ... 0 1 0]\n",
            " [3 0 1 ... 5 0 0]\n",
            " [3 0 0 ... 1 0 6]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[5 0 0 ... 0 0 0]\n",
            " [2 3 5 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 3 0 ... 4 0 1]\n",
            " [0 1 0 ... 5 1 0]\n",
            " [0 0 0 ... 1 0 3]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 3 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 2 1 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 4 ... 0 0 0]\n",
            " [2 0 0 ... 0 0 0]\n",
            " [0 5 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 2 5]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [2 2 0 ... 0 0 0]\n",
            " [0 1 4 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[ 0  0  0 ... 42  3  0]\n",
            " [ 0  0  0 ...  0  0  1]\n",
            " [ 0  0  0 ...  2  3  0]\n",
            " ...\n",
            " [ 0  4  3 ...  0  0  0]\n",
            " [ 4  0  0 ...  0  0  0]\n",
            " [ 0  1  4 ...  0  0  0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 3 ... 0 0 0]\n",
            " [2 0 0 ... 0 0 0]\n",
            " [0 6 0 ... 0 0 0]]\n",
            "[[0 2 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 4]\n",
            " [0 0 0 ... 0 1 0]\n",
            " ...\n",
            " [0 1 0 ... 0 2 2]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 3 ... 3 3 0]]\n",
            "[[0 1 1 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [2 2 1 ... 2 0 0]\n",
            " [0 0 0 ... 2 2 5]\n",
            " [3 0 3 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 3 5]\n",
            " [0 3 0 ... 0 3 0]\n",
            " [0 0 0 ... 2 0 3]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 4 3 4]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 4 0 0]]\n",
            "[[ 0  7  0 ...  0  0  0]\n",
            " [ 0  0 10 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 5 1 0]\n",
            " [0 0 0 ... 1 0 1]\n",
            " [0 0 0 ... 0 1 5]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " ...\n",
            " [  0 152 204 ...   0   0   0]\n",
            " [  5  38  25 ...   5   0   0]\n",
            " [  1   0   4 ...   1   2   0]]\n",
            "[[0 0 1 ... 1 0 1]\n",
            " [6 2 0 ... 0 4 0]\n",
            " [0 0 6 ... 0 3 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [3 0 2 ... 0 0 0]\n",
            " [0 0 9 ... 0 0 0]\n",
            " [0 5 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 2]\n",
            " [0 0 0 ... 4 0 0]\n",
            " [0 0 0 ... 0 5 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 4]\n",
            " [0 0 1 ... 2 0 0]\n",
            " [1 1 1 ... 4 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 5]\n",
            " [0 0 0 ... 7 0 0]\n",
            " [0 0 0 ... 0 1 4]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 4 2 ... 0 0 0]\n",
            " [2 0 0 ... 0 0 0]\n",
            " [0 3 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 2 4 ... 0 0 0]\n",
            " [4 2 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 2 ... 0 0 0]\n",
            " [5 2 4 ... 0 0 0]\n",
            " [2 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 2 1 0]\n",
            " [0 0 0 ... 0 3 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 4 2 ... 0 0 0]\n",
            " [0 1 3 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 4 ... 0 0 0]\n",
            " [0 3 0 ... 0 0 0]\n",
            " [1 0 5 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 4 0 4]\n",
            " [0 0 0 ... 0 1 0]]\n",
            "[[0 0 0 ... 8 1 0]\n",
            " [0 0 0 ... 0 0 2]\n",
            " [0 0 0 ... 0 1 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[ 1  2  0 ...  0  0  0]\n",
            " [ 0 10  0 ...  0  0  0]\n",
            " [ 5  0  1 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "[[7 0 0 ... 0 0 0]\n",
            " [0 4 0 ... 0 0 0]\n",
            " [4 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 4]\n",
            " [0 0 0 ... 6 1 0]\n",
            " [0 0 0 ... 0 0 3]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 7 0 ... 2 0 1]\n",
            " [0 6 1 ... 0 3 1]\n",
            " [0 1 0 ... 7 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[  0   4   0 ...  27 213 164]\n",
            " [  3   0   3 ... 148 162 135]\n",
            " [  0   1   0 ... 197 117 106]\n",
            " ...\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  3   0   0 ...   0   0   0]\n",
            " [  0   1   1 ...   0   0   0]]\n",
            "[[0 2 1 ... 0 0 0]\n",
            " [2 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[2 0 4 ... 3 0 0]\n",
            " [0 0 0 ... 0 4 0]\n",
            " [0 1 3 ... 0 5 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 7 ... 0 0 0]\n",
            " [4 0 0 ... 0 0 0]\n",
            " [0 6 0 ... 0 0 0]]\n",
            "[[3 1 0 ... 0 6 0]\n",
            " [0 0 0 ... 0 3 1]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 3 ... 0 0 0]\n",
            " [2 0 0 ... 0 0 0]\n",
            " [0 5 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 4 2 2]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " ...\n",
            " [  0   5   0 ...   0   0   4]\n",
            " [  5   1   0 ...  11   1   0]\n",
            " [  0   0 134 ...   0   0   0]]\n",
            "[[0 0 0 ... 2 0 1]\n",
            " [0 0 0 ... 0 2 2]\n",
            " [0 0 0 ... 2 4 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 3 0 1]\n",
            " [0 0 0 ... 7 0 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 4 ... 0 0 0]\n",
            " [0 4 8 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 2 3 ... 0 0 0]\n",
            " [0 3 2 ... 0 0 0]\n",
            " ...\n",
            " [1 0 8 ... 0 0 0]\n",
            " [0 4 0 ... 0 0 0]\n",
            " [3 0 1 ... 0 0 0]]\n",
            "[[3 0 0 ... 1 3 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [6 2 0 ... 0 3 5]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[  0   2   0 ... 164 115   0]\n",
            " [  0   1  65 ...  28 200  88]\n",
            " [  1   0 154 ...   3  33 189]\n",
            " ...\n",
            " [  0   3   6 ...   6   0   3]\n",
            " [  0   1   3 ...   0   2   0]\n",
            " [  0   0   0 ...   1   4   1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [3 0 3 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]]\n",
            "[[0 1 0 ... 0 0 0]\n",
            " [8 0 0 ... 0 0 0]\n",
            " [0 0 5 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [3 2 0 ... 0 0 0]\n",
            " [0 0 4 ... 0 0 0]\n",
            " [3 0 2 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq_5m4isr3Ai",
        "colab_type": "text"
      },
      "source": [
        "√"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "909JOya1axX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}